{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# types to int\n",
    "temp_df = pd.read_csv('E:/ML/DS_fake_news/fake_news_cleaned.csv', usecols=['type'], nrows=2000)\n",
    "\n",
    "# remove the 'type' unknown\n",
    "temp_df = temp_df[temp_df['type'] != 'unknown']\n",
    "\n",
    "temp_df['type'] = temp_df['type'].astype('category')\n",
    "\n",
    "cat_dict = {k: v for v, k in enumerate(temp_df['type'].cat.categories)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 0,\n",
       " 'clickbait': 1,\n",
       " 'conspiracy': 2,\n",
       " 'fake': 3,\n",
       " 'hate': 4,\n",
       " 'junksci': 5,\n",
       " 'political': 6,\n",
       " 'reliable': 7,\n",
       " 'rumor': 8,\n",
       " 'satire': 9,\n",
       " 'unreliable': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(chunk_size=2000):\n",
    "    # read the data in chunks\n",
    "    reader = pd.read_csv('E:/ML/DS_fake_news/fake_news_cleaned.csv', chunksize=chunk_size)\n",
    "    for chunk in reader:\n",
    "        # remove rows with class 'unknown'\n",
    "        chunk = chunk[chunk['type'] != 'unknown']\n",
    "        # remove rows with class nan\n",
    "        chunk = chunk[chunk['type'].notna()]\n",
    "        # convert the 'type' column to int\n",
    "        chunk['type'] = chunk['type'].map(cat_dict)\n",
    "\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Replace the classification layer with a new, randomly initialized linear layer\n",
    "classifier = torch.nn.Linear(model.config.hidden_size, 11)\n",
    "model.classifier = classifier\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the requires_grad attribute of all other layers in the model to False\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# Set the requires_grad attribute of the new linear layer to True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "# Define the optimizer and loss function for training the linear layer\n",
    "optimizer = torch.optim.SGD(model.classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.texts = data['content']\n",
    "        self.labels = data['type']\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text and label for this index\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        # Tokenize the text using the provided tokenizer\n",
    "        tokens = self.tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        \n",
    "        # Return the input and label tensors for this index\n",
    "        return tokens['input_ids'].squeeze(), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Load the pre-trained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "data = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch number: 1, loss: 2.27750825881958\n",
      "epoch: 0, batch number: 2, loss: 2.0710036754608154\n",
      "epoch: 0, batch number: 3, loss: 2.7545487880706787\n",
      "epoch: 0, batch number: 4, loss: 2.1205174922943115\n",
      "epoch: 0, batch number: 5, loss: 2.4327151775360107\n",
      "epoch: 0, batch number: 6, loss: 2.3628571033477783\n",
      "epoch: 0, batch number: 7, loss: 2.404097080230713\n",
      "epoch: 0, batch number: 8, loss: 2.0378687381744385\n",
      "epoch: 0, batch number: 9, loss: 2.2624127864837646\n",
      "epoch: 0, batch number: 10, loss: 2.4795167446136475\n",
      "epoch: 0, batch number: 11, loss: 2.245877265930176\n",
      "epoch: 0, batch number: 12, loss: 2.4042019844055176\n",
      "epoch: 0, batch number: 13, loss: 2.4266252517700195\n",
      "epoch: 0, batch number: 14, loss: 2.1465466022491455\n",
      "epoch: 0, batch number: 15, loss: 2.507112503051758\n",
      "epoch: 0, batch number: 16, loss: 2.1862270832061768\n",
      "epoch: 0, batch number: 17, loss: 2.505695343017578\n",
      "epoch: 0, batch number: 18, loss: 2.5045485496520996\n",
      "epoch: 0, batch number: 19, loss: 2.3588404655456543\n",
      "epoch: 0, batch number: 20, loss: 2.4631574153900146\n",
      "epoch: 0, batch number: 21, loss: 2.221924066543579\n",
      "epoch: 0, batch number: 22, loss: 2.3364086151123047\n",
      "epoch: 0, batch number: 23, loss: 2.383615016937256\n",
      "epoch: 0, batch number: 24, loss: 2.3237295150756836\n",
      "epoch: 0, batch number: 25, loss: 1.9413485527038574\n",
      "epoch: 0, batch number: 26, loss: 2.3446168899536133\n",
      "epoch: 0, batch number: 27, loss: 2.1505556106567383\n",
      "epoch: 0, batch number: 28, loss: 2.1725287437438965\n",
      "epoch: 0, batch number: 29, loss: 2.110985517501831\n",
      "epoch: 0, batch number: 30, loss: 2.2810778617858887\n",
      "epoch: 0, batch number: 31, loss: 2.1826398372650146\n",
      "epoch: 0, batch number: 32, loss: 2.276618719100952\n",
      "epoch: 0, batch number: 33, loss: 2.339138984680176\n",
      "epoch: 0, batch number: 34, loss: 1.8768097162246704\n",
      "epoch: 0, batch number: 35, loss: 2.2138631343841553\n",
      "epoch: 0, batch number: 36, loss: 2.2067599296569824\n",
      "epoch: 0, batch number: 37, loss: 2.216190814971924\n",
      "epoch: 0, batch number: 38, loss: 2.1423943042755127\n",
      "epoch: 0, batch number: 39, loss: 2.159031629562378\n",
      "epoch: 0, batch number: 40, loss: 2.3423705101013184\n",
      "epoch: 0, batch number: 41, loss: 2.1823713779449463\n",
      "epoch: 0, batch number: 42, loss: 2.0512406826019287\n",
      "epoch: 0, batch number: 43, loss: 2.161710023880005\n",
      "epoch: 0, batch number: 44, loss: 2.0552051067352295\n",
      "epoch: 0, batch number: 45, loss: 2.2731714248657227\n",
      "epoch: 0, batch number: 46, loss: 2.2914910316467285\n",
      "epoch: 0, batch number: 47, loss: 2.2545666694641113\n",
      "epoch: 0, batch number: 48, loss: 2.2313430309295654\n",
      "epoch: 0, batch number: 49, loss: 2.13456392288208\n",
      "epoch: 0, batch number: 50, loss: 2.181577444076538\n",
      "epoch: 0, batch number: 51, loss: 1.9766554832458496\n",
      "epoch: 0, batch number: 52, loss: 2.273108720779419\n",
      "epoch: 0, batch number: 53, loss: 2.3530633449554443\n",
      "epoch: 0, batch number: 54, loss: 2.2655200958251953\n",
      "epoch: 0, batch number: 55, loss: 1.9199414253234863\n",
      "epoch: 0, batch number: 56, loss: 2.2033002376556396\n",
      "epoch: 0, batch number: 57, loss: 2.314967155456543\n",
      "epoch: 0, batch number: 58, loss: 2.134505271911621\n",
      "epoch: 0, batch number: 59, loss: 2.1493818759918213\n",
      "epoch: 0, batch number: 60, loss: 2.167872905731201\n",
      "epoch: 0, batch number: 61, loss: 2.1442346572875977\n",
      "epoch: 0, batch number: 62, loss: 2.3671274185180664\n",
      "epoch: 0, batch number: 63, loss: 2.3051037788391113\n",
      "epoch: 1, batch number: 64, loss: 1.826337456703186\n",
      "epoch: 1, batch number: 65, loss: 2.0479331016540527\n",
      "epoch: 1, batch number: 66, loss: 1.8727686405181885\n",
      "epoch: 1, batch number: 67, loss: 1.8284332752227783\n",
      "epoch: 1, batch number: 68, loss: 1.8362116813659668\n",
      "epoch: 1, batch number: 69, loss: 2.0158064365386963\n",
      "epoch: 1, batch number: 70, loss: 2.0252442359924316\n",
      "epoch: 1, batch number: 71, loss: 1.987145185470581\n",
      "epoch: 1, batch number: 72, loss: 1.886903166770935\n",
      "epoch: 1, batch number: 73, loss: 1.8480342626571655\n",
      "epoch: 1, batch number: 74, loss: 1.8349709510803223\n",
      "epoch: 1, batch number: 75, loss: 1.9275753498077393\n",
      "epoch: 1, batch number: 76, loss: 1.9135082960128784\n",
      "epoch: 1, batch number: 77, loss: 2.0357871055603027\n",
      "epoch: 1, batch number: 78, loss: 1.8340226411819458\n",
      "epoch: 1, batch number: 79, loss: 1.6568546295166016\n",
      "epoch: 1, batch number: 80, loss: 1.7341288328170776\n",
      "epoch: 1, batch number: 81, loss: 1.869596242904663\n",
      "epoch: 1, batch number: 82, loss: 1.9623444080352783\n",
      "epoch: 1, batch number: 83, loss: 1.88914155960083\n",
      "epoch: 1, batch number: 84, loss: 1.895891785621643\n",
      "epoch: 1, batch number: 85, loss: 1.8220371007919312\n",
      "epoch: 1, batch number: 86, loss: 1.6141865253448486\n",
      "epoch: 1, batch number: 87, loss: 1.5658278465270996\n",
      "epoch: 1, batch number: 88, loss: 1.8424453735351562\n",
      "epoch: 1, batch number: 89, loss: 1.8054134845733643\n",
      "epoch: 1, batch number: 90, loss: 1.7234359979629517\n",
      "epoch: 1, batch number: 91, loss: 1.6620819568634033\n",
      "epoch: 1, batch number: 92, loss: 1.9652585983276367\n",
      "epoch: 1, batch number: 93, loss: 1.9443199634552002\n",
      "epoch: 1, batch number: 94, loss: 1.684346318244934\n",
      "epoch: 1, batch number: 95, loss: 1.7311254739761353\n",
      "epoch: 1, batch number: 96, loss: 1.9867463111877441\n",
      "epoch: 1, batch number: 97, loss: 1.6049246788024902\n",
      "epoch: 1, batch number: 98, loss: 1.729266881942749\n",
      "epoch: 1, batch number: 99, loss: 1.5293527841567993\n",
      "epoch: 1, batch number: 100, loss: 1.7044144868850708\n",
      "epoch: 1, batch number: 101, loss: 1.8604532480239868\n",
      "epoch: 1, batch number: 102, loss: 1.451003074645996\n",
      "epoch: 1, batch number: 103, loss: 1.8866904973983765\n",
      "epoch: 1, batch number: 104, loss: 1.8307685852050781\n",
      "epoch: 1, batch number: 105, loss: 1.6450591087341309\n",
      "epoch: 1, batch number: 106, loss: 1.533199667930603\n",
      "epoch: 1, batch number: 107, loss: 1.8217389583587646\n",
      "epoch: 1, batch number: 108, loss: 1.4993643760681152\n",
      "epoch: 1, batch number: 109, loss: 1.649247646331787\n",
      "epoch: 1, batch number: 110, loss: 1.6606553792953491\n",
      "epoch: 1, batch number: 111, loss: 1.798820972442627\n",
      "epoch: 1, batch number: 112, loss: 2.107255697250366\n",
      "epoch: 1, batch number: 113, loss: 1.5218585729599\n",
      "epoch: 1, batch number: 114, loss: 1.548107385635376\n",
      "epoch: 1, batch number: 115, loss: 1.7056655883789062\n",
      "epoch: 1, batch number: 116, loss: 1.5369833707809448\n",
      "epoch: 1, batch number: 117, loss: 1.4464545249938965\n",
      "epoch: 1, batch number: 118, loss: 2.061657667160034\n",
      "epoch: 1, batch number: 119, loss: 1.4324679374694824\n",
      "epoch: 1, batch number: 120, loss: 1.673048496246338\n",
      "epoch: 1, batch number: 121, loss: 1.7286045551300049\n",
      "epoch: 1, batch number: 122, loss: 1.7663229703903198\n",
      "epoch: 1, batch number: 123, loss: 1.6889829635620117\n",
      "epoch: 1, batch number: 124, loss: 1.3240171670913696\n",
      "epoch: 1, batch number: 125, loss: 1.4476745128631592\n",
      "epoch: 1, batch number: 126, loss: 1.6657085418701172\n",
      "epoch: 2, batch number: 127, loss: 1.313659429550171\n",
      "epoch: 2, batch number: 128, loss: 1.6135997772216797\n",
      "epoch: 2, batch number: 129, loss: 1.3505460023880005\n",
      "epoch: 2, batch number: 130, loss: 1.4174975156784058\n",
      "epoch: 2, batch number: 131, loss: 1.5494825839996338\n",
      "epoch: 2, batch number: 132, loss: 1.5818753242492676\n",
      "epoch: 2, batch number: 133, loss: 1.6938990354537964\n",
      "epoch: 2, batch number: 134, loss: 1.319993257522583\n",
      "epoch: 2, batch number: 135, loss: 1.5604108572006226\n",
      "epoch: 2, batch number: 136, loss: 1.6308274269104004\n",
      "epoch: 2, batch number: 137, loss: 1.395396113395691\n",
      "epoch: 2, batch number: 138, loss: 1.2853320837020874\n",
      "epoch: 2, batch number: 139, loss: 1.333715796470642\n",
      "epoch: 2, batch number: 140, loss: 1.1084065437316895\n",
      "epoch: 2, batch number: 141, loss: 1.48035728931427\n",
      "epoch: 2, batch number: 142, loss: 1.3954048156738281\n",
      "epoch: 2, batch number: 143, loss: 1.443441390991211\n",
      "epoch: 2, batch number: 144, loss: 1.4912627935409546\n",
      "epoch: 2, batch number: 145, loss: 1.1918816566467285\n",
      "epoch: 2, batch number: 146, loss: 1.2505388259887695\n",
      "epoch: 2, batch number: 147, loss: 1.364403486251831\n",
      "epoch: 2, batch number: 148, loss: 1.4399616718292236\n",
      "epoch: 2, batch number: 149, loss: 1.2818028926849365\n",
      "epoch: 2, batch number: 150, loss: 1.3629335165023804\n",
      "epoch: 2, batch number: 151, loss: 1.1233595609664917\n",
      "epoch: 2, batch number: 152, loss: 1.2985059022903442\n",
      "epoch: 2, batch number: 153, loss: 1.3012293577194214\n",
      "epoch: 2, batch number: 154, loss: 1.5117216110229492\n",
      "epoch: 2, batch number: 155, loss: 1.449932336807251\n",
      "epoch: 2, batch number: 156, loss: 1.346225619316101\n",
      "epoch: 2, batch number: 157, loss: 1.379504680633545\n",
      "epoch: 2, batch number: 158, loss: 1.3194223642349243\n",
      "epoch: 2, batch number: 159, loss: 1.2869348526000977\n",
      "epoch: 2, batch number: 160, loss: 1.3317375183105469\n",
      "epoch: 2, batch number: 161, loss: 1.1362155675888062\n",
      "epoch: 2, batch number: 162, loss: 1.3626322746276855\n",
      "epoch: 2, batch number: 163, loss: 1.4848684072494507\n",
      "epoch: 2, batch number: 164, loss: 1.3838557004928589\n",
      "epoch: 2, batch number: 165, loss: 1.6914864778518677\n",
      "epoch: 2, batch number: 166, loss: 1.0168812274932861\n",
      "epoch: 2, batch number: 167, loss: 1.1416622400283813\n",
      "epoch: 2, batch number: 168, loss: 1.4406068325042725\n",
      "epoch: 2, batch number: 169, loss: 1.2352510690689087\n",
      "epoch: 2, batch number: 170, loss: 1.2432740926742554\n",
      "epoch: 2, batch number: 171, loss: 1.4048789739608765\n",
      "epoch: 2, batch number: 172, loss: 1.2803250551223755\n",
      "epoch: 2, batch number: 173, loss: 1.11191725730896\n",
      "epoch: 2, batch number: 174, loss: 1.3310139179229736\n",
      "epoch: 2, batch number: 175, loss: 1.5707310438156128\n",
      "epoch: 2, batch number: 176, loss: 1.190234899520874\n",
      "epoch: 2, batch number: 177, loss: 1.0621196031570435\n",
      "epoch: 2, batch number: 178, loss: 1.4103816747665405\n",
      "epoch: 2, batch number: 179, loss: 1.278761863708496\n",
      "epoch: 2, batch number: 180, loss: 1.440700888633728\n",
      "epoch: 2, batch number: 181, loss: 1.1683204174041748\n",
      "epoch: 2, batch number: 182, loss: 1.4519829750061035\n",
      "epoch: 2, batch number: 183, loss: 1.154016137123108\n",
      "epoch: 2, batch number: 184, loss: 1.8425040245056152\n",
      "epoch: 2, batch number: 185, loss: 1.3412666320800781\n",
      "epoch: 2, batch number: 186, loss: 1.3022634983062744\n",
      "epoch: 2, batch number: 187, loss: 1.2378748655319214\n",
      "epoch: 2, batch number: 188, loss: 1.1350021362304688\n",
      "epoch: 3, batch number: 189, loss: 1.2825336456298828\n",
      "epoch: 3, batch number: 190, loss: 1.250472903251648\n",
      "epoch: 3, batch number: 191, loss: 1.4882508516311646\n",
      "epoch: 3, batch number: 192, loss: 1.232903003692627\n",
      "epoch: 3, batch number: 193, loss: 1.0445773601531982\n",
      "epoch: 3, batch number: 194, loss: 1.2418965101242065\n",
      "epoch: 3, batch number: 195, loss: 1.1028070449829102\n",
      "epoch: 3, batch number: 196, loss: 1.039435863494873\n",
      "epoch: 3, batch number: 197, loss: 1.308746576309204\n",
      "epoch: 3, batch number: 198, loss: 1.1495356559753418\n",
      "epoch: 3, batch number: 199, loss: 1.4130098819732666\n",
      "epoch: 3, batch number: 200, loss: 1.3290224075317383\n",
      "epoch: 3, batch number: 201, loss: 1.3091176748275757\n",
      "epoch: 3, batch number: 202, loss: 1.4857434034347534\n",
      "epoch: 3, batch number: 203, loss: 0.9204893708229065\n",
      "epoch: 3, batch number: 204, loss: 0.9862701892852783\n",
      "epoch: 3, batch number: 205, loss: 1.2107075452804565\n",
      "epoch: 3, batch number: 206, loss: 1.2288882732391357\n",
      "epoch: 3, batch number: 207, loss: 1.0563714504241943\n",
      "epoch: 3, batch number: 208, loss: 0.8382556438446045\n",
      "epoch: 3, batch number: 209, loss: 1.2194474935531616\n",
      "epoch: 3, batch number: 210, loss: 1.0937504768371582\n",
      "epoch: 3, batch number: 211, loss: 1.054223656654358\n",
      "epoch: 3, batch number: 212, loss: 1.0113710165023804\n",
      "epoch: 3, batch number: 213, loss: 1.107427716255188\n",
      "epoch: 3, batch number: 214, loss: 1.195184350013733\n",
      "epoch: 3, batch number: 215, loss: 1.5845330953598022\n",
      "epoch: 3, batch number: 216, loss: 1.4220447540283203\n",
      "epoch: 3, batch number: 217, loss: 1.030576229095459\n",
      "epoch: 3, batch number: 218, loss: 1.1942205429077148\n",
      "epoch: 3, batch number: 219, loss: 1.3098556995391846\n",
      "epoch: 3, batch number: 220, loss: 1.1331747770309448\n",
      "epoch: 3, batch number: 221, loss: 1.2568440437316895\n",
      "epoch: 3, batch number: 222, loss: 1.1992027759552002\n",
      "epoch: 3, batch number: 223, loss: 1.472549557685852\n",
      "epoch: 3, batch number: 224, loss: 1.030996561050415\n",
      "epoch: 3, batch number: 225, loss: 1.3739757537841797\n",
      "epoch: 3, batch number: 226, loss: 1.0973800420761108\n",
      "epoch: 3, batch number: 227, loss: 1.0865442752838135\n",
      "epoch: 3, batch number: 228, loss: 1.176440715789795\n",
      "epoch: 3, batch number: 229, loss: 1.213915467262268\n",
      "epoch: 3, batch number: 230, loss: 1.1150457859039307\n",
      "epoch: 3, batch number: 231, loss: 1.1617424488067627\n",
      "epoch: 3, batch number: 232, loss: 0.9967012405395508\n",
      "epoch: 3, batch number: 233, loss: 0.9516807198524475\n",
      "epoch: 3, batch number: 234, loss: 1.1977510452270508\n",
      "epoch: 3, batch number: 235, loss: 1.2290782928466797\n",
      "epoch: 3, batch number: 236, loss: 0.979400098323822\n",
      "epoch: 3, batch number: 237, loss: 1.2526763677597046\n",
      "epoch: 3, batch number: 238, loss: 1.0561463832855225\n",
      "epoch: 3, batch number: 239, loss: 1.1207035779953003\n",
      "epoch: 3, batch number: 240, loss: 1.525202989578247\n",
      "epoch: 3, batch number: 241, loss: 1.3642007112503052\n",
      "epoch: 3, batch number: 242, loss: 1.3488426208496094\n",
      "epoch: 3, batch number: 243, loss: 1.11958909034729\n",
      "epoch: 3, batch number: 244, loss: 0.9444124698638916\n",
      "epoch: 3, batch number: 245, loss: 1.2225289344787598\n",
      "epoch: 3, batch number: 246, loss: 1.1517629623413086\n",
      "epoch: 3, batch number: 247, loss: 1.404436707496643\n",
      "epoch: 3, batch number: 248, loss: 1.3155977725982666\n",
      "epoch: 3, batch number: 249, loss: 0.9382784366607666\n",
      "epoch: 4, batch number: 250, loss: 1.546754240989685\n",
      "epoch: 4, batch number: 251, loss: 1.4572882652282715\n",
      "epoch: 4, batch number: 252, loss: 1.578274130821228\n",
      "epoch: 4, batch number: 253, loss: 1.3347382545471191\n",
      "epoch: 4, batch number: 254, loss: 1.6289035081863403\n",
      "epoch: 4, batch number: 255, loss: 1.5703880786895752\n",
      "epoch: 4, batch number: 256, loss: 1.1069642305374146\n",
      "epoch: 4, batch number: 257, loss: 1.4782394170761108\n",
      "epoch: 4, batch number: 258, loss: 1.4110865592956543\n",
      "epoch: 4, batch number: 259, loss: 1.6182949542999268\n",
      "epoch: 4, batch number: 260, loss: 1.518754243850708\n",
      "epoch: 4, batch number: 261, loss: 1.5320357084274292\n",
      "epoch: 4, batch number: 262, loss: 1.4041063785552979\n",
      "epoch: 4, batch number: 263, loss: 1.4907584190368652\n",
      "epoch: 4, batch number: 264, loss: 1.3566513061523438\n",
      "epoch: 4, batch number: 265, loss: 1.6476423740386963\n",
      "epoch: 4, batch number: 266, loss: 1.6896363496780396\n",
      "epoch: 4, batch number: 267, loss: 2.1376876831054688\n",
      "epoch: 4, batch number: 268, loss: 1.5096187591552734\n",
      "epoch: 4, batch number: 269, loss: 1.2607197761535645\n",
      "epoch: 4, batch number: 270, loss: 1.357911229133606\n",
      "epoch: 4, batch number: 271, loss: 1.6719993352890015\n",
      "epoch: 4, batch number: 272, loss: 1.2795870304107666\n",
      "epoch: 4, batch number: 273, loss: 1.3795956373214722\n",
      "epoch: 4, batch number: 274, loss: 1.5303688049316406\n",
      "epoch: 4, batch number: 275, loss: 1.483522891998291\n",
      "epoch: 4, batch number: 276, loss: 1.2642009258270264\n",
      "epoch: 4, batch number: 277, loss: 1.53170907497406\n",
      "epoch: 4, batch number: 278, loss: 1.6156140565872192\n",
      "epoch: 4, batch number: 279, loss: 1.3743647336959839\n",
      "epoch: 4, batch number: 280, loss: 1.6292545795440674\n",
      "epoch: 4, batch number: 281, loss: 1.4814389944076538\n",
      "epoch: 4, batch number: 282, loss: 1.4593526124954224\n",
      "epoch: 4, batch number: 283, loss: 1.2358938455581665\n",
      "epoch: 4, batch number: 284, loss: 1.2842166423797607\n",
      "epoch: 4, batch number: 285, loss: 1.4294383525848389\n",
      "epoch: 4, batch number: 286, loss: 1.274850606918335\n",
      "epoch: 4, batch number: 287, loss: 1.316195011138916\n",
      "epoch: 4, batch number: 288, loss: 1.7090976238250732\n",
      "epoch: 4, batch number: 289, loss: 1.182572841644287\n",
      "epoch: 4, batch number: 290, loss: 1.0809816122055054\n",
      "epoch: 4, batch number: 291, loss: 1.326212763786316\n",
      "epoch: 4, batch number: 292, loss: 1.339128017425537\n",
      "epoch: 4, batch number: 293, loss: 1.218561053276062\n",
      "epoch: 4, batch number: 294, loss: 1.5233607292175293\n",
      "epoch: 4, batch number: 295, loss: 1.4895896911621094\n",
      "epoch: 4, batch number: 296, loss: 1.4221456050872803\n",
      "epoch: 4, batch number: 297, loss: 1.264387845993042\n",
      "epoch: 4, batch number: 298, loss: 1.4074081182479858\n",
      "epoch: 4, batch number: 299, loss: 1.3661476373672485\n",
      "epoch: 4, batch number: 300, loss: 1.2593191862106323\n",
      "epoch: 4, batch number: 301, loss: 1.5248826742172241\n",
      "epoch: 4, batch number: 302, loss: 1.3284858465194702\n",
      "epoch: 4, batch number: 303, loss: 1.2208856344223022\n",
      "epoch: 4, batch number: 304, loss: 1.2900471687316895\n",
      "epoch: 4, batch number: 305, loss: 1.2444013357162476\n",
      "epoch: 4, batch number: 306, loss: 1.2117000818252563\n",
      "epoch: 4, batch number: 307, loss: 1.302518606185913\n",
      "epoch: 4, batch number: 308, loss: 1.2914375066757202\n",
      "epoch: 4, batch number: 309, loss: 1.2169830799102783\n",
      "epoch: 4, batch number: 310, loss: 1.3876831531524658\n",
      "epoch: 4, batch number: 311, loss: 1.0956943035125732\n",
      "epoch: 4, batch number: 312, loss: 1.1199285984039307\n",
      "epoch: 5, batch number: 313, loss: 1.2230228185653687\n",
      "epoch: 5, batch number: 314, loss: 1.365187168121338\n",
      "epoch: 5, batch number: 315, loss: 1.4905836582183838\n",
      "epoch: 5, batch number: 316, loss: 1.1094969511032104\n",
      "epoch: 5, batch number: 317, loss: 1.3133445978164673\n",
      "epoch: 5, batch number: 318, loss: 1.2849977016448975\n",
      "epoch: 5, batch number: 319, loss: 1.1135085821151733\n",
      "epoch: 5, batch number: 320, loss: 1.3453165292739868\n",
      "epoch: 5, batch number: 321, loss: 1.1918808221817017\n",
      "epoch: 5, batch number: 322, loss: 1.1989123821258545\n",
      "epoch: 5, batch number: 323, loss: 1.2769027948379517\n",
      "epoch: 5, batch number: 324, loss: 1.1065495014190674\n",
      "epoch: 5, batch number: 325, loss: 1.181408166885376\n",
      "epoch: 5, batch number: 326, loss: 1.311193823814392\n",
      "epoch: 5, batch number: 327, loss: 1.19402015209198\n",
      "epoch: 5, batch number: 328, loss: 1.131600022315979\n",
      "epoch: 5, batch number: 329, loss: 1.103803277015686\n",
      "epoch: 5, batch number: 330, loss: 1.143458604812622\n",
      "epoch: 5, batch number: 331, loss: 1.1991174221038818\n",
      "epoch: 5, batch number: 332, loss: 1.1157575845718384\n",
      "epoch: 5, batch number: 333, loss: 1.3004188537597656\n",
      "epoch: 5, batch number: 334, loss: 1.157837152481079\n",
      "epoch: 5, batch number: 335, loss: 1.1865679025650024\n",
      "epoch: 5, batch number: 336, loss: 1.0704833269119263\n",
      "epoch: 5, batch number: 337, loss: 1.1421579122543335\n",
      "epoch: 5, batch number: 338, loss: 1.1197879314422607\n",
      "epoch: 5, batch number: 339, loss: 1.1580344438552856\n",
      "epoch: 5, batch number: 340, loss: 1.1127476692199707\n",
      "epoch: 5, batch number: 341, loss: 1.1644172668457031\n",
      "epoch: 5, batch number: 342, loss: 1.1718510389328003\n",
      "epoch: 5, batch number: 343, loss: 1.05607008934021\n",
      "epoch: 5, batch number: 344, loss: 1.0738632678985596\n",
      "epoch: 5, batch number: 345, loss: 1.1046189069747925\n",
      "epoch: 5, batch number: 346, loss: 1.1663316488265991\n",
      "epoch: 5, batch number: 347, loss: 1.1682360172271729\n",
      "epoch: 5, batch number: 348, loss: 1.1480004787445068\n",
      "epoch: 5, batch number: 349, loss: 1.1341336965560913\n",
      "epoch: 5, batch number: 350, loss: 1.019984483718872\n",
      "epoch: 5, batch number: 351, loss: 1.0315523147583008\n",
      "epoch: 5, batch number: 352, loss: 1.193000316619873\n",
      "epoch: 5, batch number: 353, loss: 1.1068456172943115\n",
      "epoch: 5, batch number: 354, loss: 1.0035642385482788\n",
      "epoch: 5, batch number: 355, loss: 1.143420934677124\n",
      "epoch: 5, batch number: 356, loss: 1.1694852113723755\n",
      "epoch: 5, batch number: 357, loss: 1.025414228439331\n",
      "epoch: 5, batch number: 358, loss: 1.000040888786316\n",
      "epoch: 5, batch number: 359, loss: 1.0307066440582275\n",
      "epoch: 5, batch number: 360, loss: 1.0656654834747314\n",
      "epoch: 5, batch number: 361, loss: 1.1302900314331055\n",
      "epoch: 5, batch number: 362, loss: 0.9668906927108765\n",
      "epoch: 5, batch number: 363, loss: 1.0813429355621338\n",
      "epoch: 5, batch number: 364, loss: 1.0877876281738281\n",
      "epoch: 5, batch number: 365, loss: 1.0023213624954224\n",
      "epoch: 5, batch number: 366, loss: 1.0216701030731201\n",
      "epoch: 5, batch number: 367, loss: 1.3823561668395996\n",
      "epoch: 5, batch number: 368, loss: 1.054051399230957\n",
      "epoch: 5, batch number: 369, loss: 1.1098132133483887\n",
      "epoch: 5, batch number: 370, loss: 1.0360971689224243\n",
      "epoch: 5, batch number: 371, loss: 0.9994046092033386\n",
      "epoch: 5, batch number: 372, loss: 1.070244312286377\n",
      "epoch: 5, batch number: 373, loss: 1.080695390701294\n",
      "epoch: 5, batch number: 374, loss: 1.1482629776000977\n",
      "epoch: 5, batch number: 375, loss: 1.0911704301834106\n",
      "epoch: 6, batch number: 376, loss: 0.9087300896644592\n",
      "epoch: 6, batch number: 377, loss: 0.856254518032074\n",
      "epoch: 6, batch number: 378, loss: 0.8124316334724426\n",
      "epoch: 6, batch number: 379, loss: 0.9554817080497742\n",
      "epoch: 6, batch number: 380, loss: 0.8568212985992432\n",
      "epoch: 6, batch number: 381, loss: 0.9554136395454407\n",
      "epoch: 6, batch number: 382, loss: 0.8170249462127686\n",
      "epoch: 6, batch number: 383, loss: 0.8693128824234009\n",
      "epoch: 6, batch number: 384, loss: 0.854645311832428\n",
      "epoch: 6, batch number: 385, loss: 0.8408526182174683\n",
      "epoch: 6, batch number: 386, loss: 0.8639563918113708\n",
      "epoch: 6, batch number: 387, loss: 0.8587416410446167\n",
      "epoch: 6, batch number: 388, loss: 0.9962494373321533\n",
      "epoch: 6, batch number: 389, loss: 0.8347998261451721\n",
      "epoch: 6, batch number: 390, loss: 0.7704768776893616\n",
      "epoch: 6, batch number: 391, loss: 0.7339457869529724\n",
      "epoch: 6, batch number: 392, loss: 0.7912948131561279\n",
      "epoch: 6, batch number: 393, loss: 0.755530059337616\n",
      "epoch: 6, batch number: 394, loss: 0.6655228137969971\n",
      "epoch: 6, batch number: 395, loss: 0.742790162563324\n",
      "epoch: 6, batch number: 396, loss: 0.7750505208969116\n",
      "epoch: 6, batch number: 397, loss: 0.7588717341423035\n",
      "epoch: 6, batch number: 398, loss: 0.6608939170837402\n",
      "epoch: 6, batch number: 399, loss: 0.7919864058494568\n",
      "epoch: 6, batch number: 400, loss: 0.7093986868858337\n",
      "epoch: 6, batch number: 401, loss: 0.7084579467773438\n",
      "epoch: 6, batch number: 402, loss: 0.8332951664924622\n",
      "epoch: 6, batch number: 403, loss: 0.7508619427680969\n",
      "epoch: 6, batch number: 404, loss: 0.7491983771324158\n",
      "epoch: 6, batch number: 405, loss: 0.7025558948516846\n",
      "epoch: 6, batch number: 406, loss: 0.7253424525260925\n",
      "epoch: 6, batch number: 407, loss: 0.7587890625\n",
      "epoch: 6, batch number: 408, loss: 0.6536456942558289\n",
      "epoch: 6, batch number: 409, loss: 0.8523860573768616\n",
      "epoch: 6, batch number: 410, loss: 0.8293873071670532\n",
      "epoch: 6, batch number: 411, loss: 0.6952852010726929\n",
      "epoch: 6, batch number: 412, loss: 0.7073691487312317\n",
      "epoch: 6, batch number: 413, loss: 0.754788875579834\n",
      "epoch: 6, batch number: 414, loss: 0.7722991704940796\n",
      "epoch: 6, batch number: 415, loss: 0.6793895959854126\n",
      "epoch: 6, batch number: 416, loss: 0.8924590349197388\n",
      "epoch: 6, batch number: 417, loss: 0.7234777212142944\n",
      "epoch: 6, batch number: 418, loss: 0.8310285210609436\n",
      "epoch: 6, batch number: 419, loss: 0.775522768497467\n",
      "epoch: 6, batch number: 420, loss: 0.8942238688468933\n",
      "epoch: 6, batch number: 421, loss: 0.6821357607841492\n",
      "epoch: 6, batch number: 422, loss: 0.7705575227737427\n",
      "epoch: 6, batch number: 423, loss: 0.7484882473945618\n",
      "epoch: 6, batch number: 424, loss: 0.6696354746818542\n",
      "epoch: 6, batch number: 425, loss: 0.683819055557251\n",
      "epoch: 6, batch number: 426, loss: 0.8633815050125122\n",
      "epoch: 6, batch number: 427, loss: 0.739015519618988\n",
      "epoch: 6, batch number: 428, loss: 0.808414101600647\n",
      "epoch: 6, batch number: 429, loss: 0.7148645520210266\n",
      "epoch: 6, batch number: 430, loss: 0.8383021950721741\n",
      "epoch: 6, batch number: 431, loss: 0.6608127355575562\n",
      "epoch: 6, batch number: 432, loss: 0.7192143797874451\n",
      "epoch: 6, batch number: 433, loss: 0.804394543170929\n",
      "epoch: 6, batch number: 434, loss: 0.6504350304603577\n",
      "epoch: 6, batch number: 435, loss: 0.6486281752586365\n",
      "epoch: 6, batch number: 436, loss: 0.6883504986763\n",
      "epoch: 6, batch number: 437, loss: 1.3042539358139038\n",
      "epoch: 7, batch number: 438, loss: 1.380197525024414\n",
      "epoch: 7, batch number: 439, loss: 1.5839300155639648\n",
      "epoch: 7, batch number: 440, loss: 1.0259461402893066\n",
      "epoch: 7, batch number: 441, loss: 1.3833681344985962\n",
      "epoch: 7, batch number: 442, loss: 1.2137736082077026\n",
      "epoch: 7, batch number: 443, loss: 1.262173056602478\n",
      "epoch: 7, batch number: 444, loss: 1.1888347864151\n",
      "epoch: 7, batch number: 445, loss: 1.396760106086731\n",
      "epoch: 7, batch number: 446, loss: 1.2562122344970703\n",
      "epoch: 7, batch number: 447, loss: 1.1911089420318604\n",
      "epoch: 7, batch number: 448, loss: 1.0867644548416138\n",
      "epoch: 7, batch number: 449, loss: 1.1147725582122803\n",
      "epoch: 7, batch number: 450, loss: 1.2392269372940063\n",
      "epoch: 7, batch number: 451, loss: 0.6423249840736389\n",
      "epoch: 7, batch number: 452, loss: 0.8807798027992249\n",
      "epoch: 7, batch number: 453, loss: 1.1711821556091309\n",
      "epoch: 7, batch number: 454, loss: 2.040811777114868\n",
      "epoch: 7, batch number: 455, loss: 1.0149492025375366\n",
      "epoch: 7, batch number: 456, loss: 1.5191835165023804\n",
      "epoch: 7, batch number: 457, loss: 1.136332392692566\n",
      "epoch: 7, batch number: 458, loss: 1.210776925086975\n",
      "epoch: 7, batch number: 459, loss: 1.4079861640930176\n",
      "epoch: 7, batch number: 460, loss: 1.168363094329834\n",
      "epoch: 7, batch number: 461, loss: 1.1800612211227417\n",
      "epoch: 7, batch number: 462, loss: 0.9901077747344971\n",
      "epoch: 7, batch number: 463, loss: 0.9629645347595215\n",
      "epoch: 7, batch number: 464, loss: 0.8898688554763794\n",
      "epoch: 7, batch number: 465, loss: 0.9817731976509094\n",
      "epoch: 7, batch number: 466, loss: 0.8390703201293945\n",
      "epoch: 7, batch number: 467, loss: 1.5003761053085327\n",
      "epoch: 7, batch number: 468, loss: 1.0211515426635742\n",
      "epoch: 7, batch number: 469, loss: 1.2116128206253052\n",
      "epoch: 7, batch number: 470, loss: 0.8739094734191895\n",
      "epoch: 7, batch number: 471, loss: 0.8953570127487183\n",
      "epoch: 7, batch number: 472, loss: 1.4353392124176025\n",
      "epoch: 7, batch number: 473, loss: 1.0036251544952393\n",
      "epoch: 7, batch number: 474, loss: 1.0842009782791138\n",
      "epoch: 7, batch number: 475, loss: 0.8916809558868408\n",
      "epoch: 7, batch number: 476, loss: 0.8448864221572876\n",
      "epoch: 7, batch number: 477, loss: 1.1787859201431274\n",
      "epoch: 7, batch number: 478, loss: 1.4023813009262085\n",
      "epoch: 7, batch number: 479, loss: 1.2953482866287231\n",
      "epoch: 7, batch number: 480, loss: 1.1612929105758667\n",
      "epoch: 7, batch number: 481, loss: 1.2334092855453491\n",
      "epoch: 7, batch number: 482, loss: 1.1116427183151245\n",
      "epoch: 7, batch number: 483, loss: 0.8299282193183899\n",
      "epoch: 7, batch number: 484, loss: 0.976294994354248\n",
      "epoch: 7, batch number: 485, loss: 0.8718546628952026\n",
      "epoch: 7, batch number: 486, loss: 1.6163535118103027\n",
      "epoch: 7, batch number: 487, loss: 1.5911074876785278\n",
      "epoch: 7, batch number: 488, loss: 1.0737847089767456\n",
      "epoch: 7, batch number: 489, loss: 1.9201725721359253\n",
      "epoch: 7, batch number: 490, loss: 0.9597287774085999\n",
      "epoch: 7, batch number: 491, loss: 0.9386722445487976\n",
      "epoch: 7, batch number: 492, loss: 1.2415440082550049\n",
      "epoch: 7, batch number: 493, loss: 0.8214461803436279\n",
      "epoch: 7, batch number: 494, loss: 0.9830954074859619\n",
      "epoch: 7, batch number: 495, loss: 1.4992254972457886\n",
      "epoch: 7, batch number: 496, loss: 1.1562310457229614\n",
      "epoch: 7, batch number: 497, loss: 1.32448148727417\n",
      "epoch: 7, batch number: 498, loss: 1.6447542905807495\n",
      "epoch: 8, batch number: 499, loss: 0.4145641326904297\n",
      "epoch: 8, batch number: 500, loss: 0.4735952913761139\n",
      "epoch: 8, batch number: 501, loss: 0.36668452620506287\n",
      "epoch: 8, batch number: 502, loss: 0.6661021709442139\n",
      "epoch: 8, batch number: 503, loss: 0.34908008575439453\n",
      "epoch: 8, batch number: 504, loss: 0.5859540700912476\n",
      "epoch: 8, batch number: 505, loss: 0.810115396976471\n",
      "epoch: 8, batch number: 506, loss: 0.34154632687568665\n",
      "epoch: 8, batch number: 507, loss: 0.4663618206977844\n",
      "epoch: 8, batch number: 508, loss: 0.4536523222923279\n",
      "epoch: 8, batch number: 509, loss: 0.6741418838500977\n",
      "epoch: 8, batch number: 510, loss: 0.29324397444725037\n",
      "epoch: 8, batch number: 511, loss: 0.43254196643829346\n",
      "epoch: 8, batch number: 512, loss: 0.29046615958213806\n",
      "epoch: 8, batch number: 513, loss: 0.766599178314209\n",
      "epoch: 8, batch number: 514, loss: 0.7120312452316284\n",
      "epoch: 8, batch number: 515, loss: 0.35576409101486206\n",
      "epoch: 8, batch number: 516, loss: 0.25077909231185913\n",
      "epoch: 8, batch number: 517, loss: 0.45682820677757263\n",
      "epoch: 8, batch number: 518, loss: 0.3777020275592804\n",
      "epoch: 8, batch number: 519, loss: 0.2944459617137909\n",
      "epoch: 8, batch number: 520, loss: 0.4821772873401642\n",
      "epoch: 8, batch number: 521, loss: 0.2031131088733673\n",
      "epoch: 8, batch number: 522, loss: 0.3251637816429138\n",
      "epoch: 8, batch number: 523, loss: 0.4992343485355377\n",
      "epoch: 8, batch number: 524, loss: 0.25399333238601685\n",
      "epoch: 8, batch number: 525, loss: 0.41735395789146423\n",
      "epoch: 8, batch number: 526, loss: 0.35943272709846497\n",
      "epoch: 8, batch number: 527, loss: 0.5454252362251282\n",
      "epoch: 8, batch number: 528, loss: 0.4125305116176605\n",
      "epoch: 8, batch number: 529, loss: 0.27978137135505676\n",
      "epoch: 8, batch number: 530, loss: 0.41850194334983826\n",
      "epoch: 8, batch number: 531, loss: 0.12692199647426605\n",
      "epoch: 8, batch number: 532, loss: 0.5518622398376465\n",
      "epoch: 8, batch number: 533, loss: 0.4754078984260559\n",
      "epoch: 8, batch number: 534, loss: 0.2773301601409912\n",
      "epoch: 8, batch number: 535, loss: 0.4421105682849884\n",
      "epoch: 8, batch number: 536, loss: 0.21753980219364166\n",
      "epoch: 8, batch number: 537, loss: 0.22460351884365082\n",
      "epoch: 8, batch number: 538, loss: 0.12350237369537354\n",
      "epoch: 8, batch number: 539, loss: 0.21413131058216095\n",
      "epoch: 8, batch number: 540, loss: 0.13161692023277283\n",
      "epoch: 8, batch number: 541, loss: 0.4422982335090637\n",
      "epoch: 8, batch number: 542, loss: 0.43624332547187805\n",
      "epoch: 8, batch number: 543, loss: 0.1059977114200592\n",
      "epoch: 8, batch number: 544, loss: 0.25063544511795044\n",
      "epoch: 8, batch number: 545, loss: 0.3823765814304352\n",
      "epoch: 8, batch number: 546, loss: 0.28207236528396606\n",
      "epoch: 8, batch number: 547, loss: 0.40130099654197693\n",
      "epoch: 8, batch number: 548, loss: 0.4862538278102875\n",
      "epoch: 8, batch number: 549, loss: 0.2423790991306305\n",
      "epoch: 8, batch number: 550, loss: 0.08445306867361069\n",
      "epoch: 9, batch number: 551, loss: 0.31723612546920776\n",
      "epoch: 9, batch number: 552, loss: 0.32276827096939087\n",
      "epoch: 9, batch number: 553, loss: 0.39585745334625244\n",
      "epoch: 9, batch number: 554, loss: 0.3404799699783325\n",
      "epoch: 9, batch number: 555, loss: 0.1679050624370575\n",
      "epoch: 9, batch number: 556, loss: 0.3384864926338196\n",
      "epoch: 9, batch number: 557, loss: 0.4478318989276886\n",
      "epoch: 9, batch number: 558, loss: 0.5836894512176514\n",
      "epoch: 9, batch number: 559, loss: 0.5837212204933167\n",
      "epoch: 9, batch number: 560, loss: 0.47766631841659546\n",
      "epoch: 9, batch number: 561, loss: 0.47413119673728943\n",
      "epoch: 9, batch number: 562, loss: 0.6237931251525879\n",
      "epoch: 9, batch number: 563, loss: 0.2860490679740906\n",
      "epoch: 9, batch number: 564, loss: 0.3116270899772644\n",
      "epoch: 9, batch number: 565, loss: 0.3426234722137451\n",
      "epoch: 9, batch number: 566, loss: 0.22680026292800903\n",
      "epoch: 9, batch number: 567, loss: 0.45536261796951294\n",
      "epoch: 9, batch number: 568, loss: 0.6022909283638\n",
      "epoch: 9, batch number: 569, loss: 0.40559861063957214\n",
      "epoch: 9, batch number: 570, loss: 0.5283601880073547\n",
      "epoch: 9, batch number: 571, loss: 0.19449137151241302\n",
      "epoch: 9, batch number: 572, loss: 0.191301167011261\n",
      "epoch: 9, batch number: 573, loss: 0.3547198474407196\n",
      "epoch: 9, batch number: 574, loss: 0.3954410254955292\n",
      "epoch: 9, batch number: 575, loss: 0.5040827393531799\n",
      "epoch: 9, batch number: 576, loss: 0.5737664699554443\n",
      "epoch: 9, batch number: 577, loss: 0.08999227732419968\n",
      "epoch: 9, batch number: 578, loss: 0.18331600725650787\n",
      "epoch: 9, batch number: 579, loss: 0.2902180254459381\n",
      "epoch: 9, batch number: 580, loss: 0.3440311849117279\n",
      "epoch: 9, batch number: 581, loss: 0.6049703359603882\n",
      "epoch: 9, batch number: 582, loss: 0.29658615589141846\n",
      "epoch: 9, batch number: 583, loss: 0.3261040449142456\n",
      "epoch: 9, batch number: 584, loss: 0.23557905852794647\n",
      "epoch: 9, batch number: 585, loss: 0.4455673098564148\n",
      "epoch: 9, batch number: 586, loss: 0.20358875393867493\n",
      "epoch: 9, batch number: 587, loss: 0.5092831254005432\n",
      "epoch: 9, batch number: 588, loss: 0.7398389577865601\n",
      "epoch: 9, batch number: 589, loss: 0.7978940010070801\n",
      "epoch: 9, batch number: 590, loss: 0.33172303438186646\n",
      "epoch: 9, batch number: 591, loss: 0.30622145533561707\n",
      "epoch: 9, batch number: 592, loss: 0.18452246487140656\n",
      "epoch: 9, batch number: 593, loss: 0.6793177127838135\n",
      "epoch: 9, batch number: 594, loss: 0.21722547709941864\n",
      "epoch: 9, batch number: 595, loss: 0.2509411573410034\n",
      "epoch: 9, batch number: 596, loss: 0.34695613384246826\n",
      "epoch: 9, batch number: 597, loss: 0.43129345774650574\n",
      "epoch: 9, batch number: 598, loss: 0.20258177816867828\n",
      "epoch: 9, batch number: 599, loss: 0.2940238416194916\n",
      "epoch: 9, batch number: 600, loss: 0.3292251527309418\n",
      "epoch: 9, batch number: 601, loss: 0.5525821447372437\n",
      "epoch: 9, batch number: 602, loss: 0.3130718171596527\n",
      "epoch: 9, batch number: 603, loss: 0.4841017425060272\n",
      "epoch: 9, batch number: 604, loss: 0.5524035692214966\n",
      "epoch: 9, batch number: 605, loss: 0.3808513581752777\n",
      "epoch: 9, batch number: 606, loss: 0.3383219540119171\n",
      "epoch: 9, batch number: 607, loss: 0.17375732958316803\n",
      "epoch: 9, batch number: 608, loss: 0.8107186555862427\n",
      "epoch: 9, batch number: 609, loss: 0.30545732378959656\n",
      "epoch: 9, batch number: 610, loss: 0.32022321224212646\n",
      "epoch: 9, batch number: 611, loss: 0.4680156111717224\n",
      "epoch: 9, batch number: 612, loss: 0.07190993428230286\n",
      "epoch: 10, batch number: 613, loss: 0.7252011895179749\n",
      "epoch: 10, batch number: 614, loss: 0.845004141330719\n",
      "epoch: 10, batch number: 615, loss: 0.2482137829065323\n",
      "epoch: 10, batch number: 616, loss: 0.55708247423172\n",
      "epoch: 10, batch number: 617, loss: 0.6610756516456604\n",
      "epoch: 10, batch number: 618, loss: 0.4649445116519928\n",
      "epoch: 10, batch number: 619, loss: 0.4349626898765564\n",
      "epoch: 10, batch number: 620, loss: 1.0273188352584839\n",
      "epoch: 10, batch number: 621, loss: 0.5212582945823669\n",
      "epoch: 10, batch number: 622, loss: 0.5802861452102661\n",
      "epoch: 10, batch number: 623, loss: 0.7597929239273071\n",
      "epoch: 10, batch number: 624, loss: 0.6704896688461304\n",
      "epoch: 10, batch number: 625, loss: 0.4877554774284363\n",
      "epoch: 10, batch number: 626, loss: 0.3745221793651581\n",
      "epoch: 10, batch number: 627, loss: 1.0833076238632202\n",
      "epoch: 10, batch number: 628, loss: 0.5274799466133118\n",
      "epoch: 10, batch number: 629, loss: 0.40963634848594666\n",
      "epoch: 10, batch number: 630, loss: 0.551557719707489\n",
      "epoch: 10, batch number: 631, loss: 0.3777919113636017\n",
      "epoch: 10, batch number: 632, loss: 0.2804301381111145\n",
      "epoch: 10, batch number: 633, loss: 0.4019486606121063\n",
      "epoch: 10, batch number: 634, loss: 0.3865211606025696\n",
      "epoch: 10, batch number: 635, loss: 0.5952866673469543\n",
      "epoch: 10, batch number: 636, loss: 0.41266515851020813\n",
      "epoch: 10, batch number: 637, loss: 0.3598424196243286\n",
      "epoch: 10, batch number: 638, loss: 0.08598620444536209\n",
      "epoch: 10, batch number: 639, loss: 0.9295932650566101\n",
      "epoch: 10, batch number: 640, loss: 0.258493036031723\n",
      "epoch: 10, batch number: 641, loss: 0.34585028886795044\n",
      "epoch: 10, batch number: 642, loss: 0.30168673396110535\n",
      "epoch: 10, batch number: 643, loss: 0.49609869718551636\n",
      "epoch: 10, batch number: 644, loss: 0.3797200918197632\n",
      "epoch: 10, batch number: 645, loss: 0.6727731227874756\n",
      "epoch: 10, batch number: 646, loss: 0.08342191576957703\n",
      "epoch: 10, batch number: 647, loss: 0.48582786321640015\n",
      "epoch: 10, batch number: 648, loss: 0.4655560553073883\n",
      "epoch: 10, batch number: 649, loss: 0.19278797507286072\n",
      "epoch: 10, batch number: 650, loss: 0.7685193419456482\n",
      "epoch: 10, batch number: 651, loss: 0.513617217540741\n",
      "epoch: 10, batch number: 652, loss: 0.08777298033237457\n",
      "epoch: 10, batch number: 653, loss: 0.7837413549423218\n",
      "epoch: 10, batch number: 654, loss: 0.07696004211902618\n",
      "epoch: 10, batch number: 655, loss: 0.3868985176086426\n",
      "epoch: 10, batch number: 656, loss: 0.07436428964138031\n",
      "epoch: 10, batch number: 657, loss: 0.6209394335746765\n",
      "epoch: 10, batch number: 658, loss: 0.3836292326450348\n",
      "epoch: 10, batch number: 659, loss: 0.43581992387771606\n",
      "epoch: 10, batch number: 660, loss: 0.45454710721969604\n",
      "epoch: 10, batch number: 661, loss: 0.3633115291595459\n",
      "epoch: 10, batch number: 662, loss: 0.20750661194324493\n",
      "epoch: 10, batch number: 663, loss: 0.4585476517677307\n",
      "epoch: 10, batch number: 664, loss: 0.5017702579498291\n",
      "epoch: 10, batch number: 665, loss: 0.6920863389968872\n",
      "epoch: 10, batch number: 666, loss: 0.5002688765525818\n",
      "epoch: 10, batch number: 667, loss: 0.9415241479873657\n",
      "epoch: 10, batch number: 668, loss: 0.5079073905944824\n",
      "epoch: 10, batch number: 669, loss: 0.3397700786590576\n",
      "epoch: 10, batch number: 670, loss: 0.7049980759620667\n",
      "epoch: 10, batch number: 671, loss: 0.6231436133384705\n",
      "epoch: 10, batch number: 672, loss: 0.8315800428390503\n",
      "epoch: 11, batch number: 673, loss: 0.352977454662323\n",
      "epoch: 11, batch number: 674, loss: 0.4328239858150482\n",
      "epoch: 11, batch number: 675, loss: 0.192457914352417\n",
      "epoch: 11, batch number: 676, loss: 0.5362614989280701\n",
      "epoch: 11, batch number: 677, loss: 0.7630722522735596\n",
      "epoch: 11, batch number: 678, loss: 0.31975114345550537\n",
      "epoch: 11, batch number: 679, loss: 0.5772305130958557\n",
      "epoch: 11, batch number: 680, loss: 0.2872277796268463\n",
      "epoch: 11, batch number: 681, loss: 0.08877872675657272\n",
      "epoch: 11, batch number: 682, loss: 0.35376161336898804\n",
      "epoch: 11, batch number: 683, loss: 0.21151578426361084\n",
      "epoch: 11, batch number: 684, loss: 0.6927391886711121\n",
      "epoch: 11, batch number: 685, loss: 0.5091599225997925\n",
      "epoch: 11, batch number: 686, loss: 0.4542221426963806\n",
      "epoch: 11, batch number: 687, loss: 0.32925519347190857\n",
      "epoch: 11, batch number: 688, loss: 0.3469027280807495\n",
      "epoch: 11, batch number: 689, loss: 0.5476528406143188\n",
      "epoch: 11, batch number: 690, loss: 0.7627886533737183\n",
      "epoch: 11, batch number: 691, loss: 0.21677766740322113\n",
      "epoch: 11, batch number: 692, loss: 0.5186440348625183\n",
      "epoch: 11, batch number: 693, loss: 0.574680745601654\n",
      "epoch: 11, batch number: 694, loss: 0.650091826915741\n",
      "epoch: 11, batch number: 695, loss: 0.7323875427246094\n",
      "epoch: 11, batch number: 696, loss: 0.47183558344841003\n",
      "epoch: 11, batch number: 697, loss: 0.09500589966773987\n",
      "epoch: 11, batch number: 698, loss: 0.44804632663726807\n",
      "epoch: 11, batch number: 699, loss: 0.09126393496990204\n",
      "epoch: 11, batch number: 700, loss: 0.27983495593070984\n",
      "epoch: 11, batch number: 701, loss: 0.19072072207927704\n",
      "epoch: 11, batch number: 702, loss: 0.08935756981372833\n",
      "epoch: 11, batch number: 703, loss: 0.3245409429073334\n",
      "epoch: 11, batch number: 704, loss: 0.47990310192108154\n",
      "epoch: 11, batch number: 705, loss: 0.22166217863559723\n",
      "epoch: 11, batch number: 706, loss: 0.4980585277080536\n",
      "epoch: 11, batch number: 707, loss: 0.23325596749782562\n",
      "epoch: 11, batch number: 708, loss: 0.33726027607917786\n",
      "epoch: 11, batch number: 709, loss: 0.48247969150543213\n",
      "epoch: 11, batch number: 710, loss: 0.5372445583343506\n",
      "epoch: 11, batch number: 711, loss: 0.5093570351600647\n",
      "epoch: 11, batch number: 712, loss: 0.41006800532341003\n",
      "epoch: 11, batch number: 713, loss: 0.4655914008617401\n",
      "epoch: 11, batch number: 714, loss: 0.3208758234977722\n",
      "epoch: 11, batch number: 715, loss: 0.3736289441585541\n",
      "epoch: 11, batch number: 716, loss: 0.6862449049949646\n",
      "epoch: 11, batch number: 717, loss: 0.7435392141342163\n",
      "epoch: 11, batch number: 718, loss: 0.08342189341783524\n",
      "epoch: 11, batch number: 719, loss: 0.7433368563652039\n",
      "epoch: 11, batch number: 720, loss: 0.40084999799728394\n",
      "epoch: 11, batch number: 721, loss: 0.24121743440628052\n",
      "epoch: 11, batch number: 722, loss: 0.25332072377204895\n",
      "epoch: 11, batch number: 723, loss: 0.3985213041305542\n",
      "epoch: 11, batch number: 724, loss: 0.23481570184230804\n",
      "epoch: 11, batch number: 725, loss: 0.3130461573600769\n",
      "epoch: 11, batch number: 726, loss: 0.08566302061080933\n",
      "epoch: 11, batch number: 727, loss: 0.48888280987739563\n",
      "epoch: 11, batch number: 728, loss: 0.6589346528053284\n",
      "epoch: 11, batch number: 729, loss: 0.3722779452800751\n",
      "epoch: 11, batch number: 730, loss: 0.5290555357933044\n",
      "epoch: 11, batch number: 731, loss: 0.4030439853668213\n",
      "epoch: 11, batch number: 732, loss: 0.3400583267211914\n",
      "epoch: 11, batch number: 733, loss: 0.35986050963401794\n",
      "epoch: 12, batch number: 734, loss: 1.0461453199386597\n",
      "epoch: 12, batch number: 735, loss: 1.8139328956604004\n",
      "epoch: 12, batch number: 736, loss: 0.6049876809120178\n",
      "epoch: 12, batch number: 737, loss: 1.3357715606689453\n",
      "epoch: 12, batch number: 738, loss: 1.195756435394287\n",
      "epoch: 12, batch number: 739, loss: 1.0509536266326904\n",
      "epoch: 12, batch number: 740, loss: 0.7633477449417114\n",
      "epoch: 12, batch number: 741, loss: 1.0991923809051514\n",
      "epoch: 12, batch number: 742, loss: 0.7098715901374817\n",
      "epoch: 12, batch number: 743, loss: 0.5860018730163574\n",
      "epoch: 12, batch number: 744, loss: 1.049917221069336\n",
      "epoch: 12, batch number: 745, loss: 1.3509241342544556\n",
      "epoch: 12, batch number: 746, loss: 0.2535501718521118\n",
      "epoch: 12, batch number: 747, loss: 0.7193885445594788\n",
      "epoch: 12, batch number: 748, loss: 0.412692129611969\n",
      "epoch: 12, batch number: 749, loss: 0.08347573131322861\n",
      "epoch: 12, batch number: 750, loss: 0.8431919813156128\n",
      "epoch: 12, batch number: 751, loss: 0.6651619076728821\n",
      "epoch: 12, batch number: 752, loss: 0.6529603600502014\n",
      "epoch: 12, batch number: 753, loss: 0.7959575653076172\n",
      "epoch: 12, batch number: 754, loss: 1.5909676551818848\n",
      "epoch: 12, batch number: 755, loss: 0.5559419393539429\n",
      "epoch: 12, batch number: 756, loss: 1.1483582258224487\n",
      "epoch: 12, batch number: 757, loss: 1.3954663276672363\n",
      "epoch: 12, batch number: 758, loss: 1.0818201303482056\n",
      "epoch: 12, batch number: 759, loss: 0.5500525832176208\n",
      "epoch: 12, batch number: 760, loss: 1.1852108240127563\n",
      "epoch: 12, batch number: 761, loss: 0.5325398445129395\n",
      "epoch: 12, batch number: 762, loss: 0.5426019430160522\n",
      "epoch: 12, batch number: 763, loss: 0.9315686225891113\n",
      "epoch: 12, batch number: 764, loss: 1.0474777221679688\n",
      "epoch: 12, batch number: 765, loss: 0.4685990810394287\n",
      "epoch: 12, batch number: 766, loss: 0.515657365322113\n",
      "epoch: 12, batch number: 767, loss: 0.8182231783866882\n",
      "epoch: 12, batch number: 768, loss: 0.710586428642273\n",
      "epoch: 12, batch number: 769, loss: 1.0622763633728027\n",
      "epoch: 12, batch number: 770, loss: 0.6379877328872681\n",
      "epoch: 12, batch number: 771, loss: 1.4054527282714844\n",
      "epoch: 12, batch number: 772, loss: 0.6192928552627563\n",
      "epoch: 12, batch number: 773, loss: 0.8312998414039612\n",
      "epoch: 12, batch number: 774, loss: 0.6309897899627686\n",
      "epoch: 12, batch number: 775, loss: 0.4788750112056732\n",
      "epoch: 12, batch number: 776, loss: 1.1419895887374878\n",
      "epoch: 12, batch number: 777, loss: 0.9668596982955933\n",
      "epoch: 12, batch number: 778, loss: 0.7314019799232483\n",
      "epoch: 12, batch number: 779, loss: 0.5937513113021851\n",
      "epoch: 12, batch number: 780, loss: 1.0740402936935425\n",
      "epoch: 12, batch number: 781, loss: 0.5853627324104309\n",
      "epoch: 12, batch number: 782, loss: 1.0639753341674805\n",
      "epoch: 12, batch number: 783, loss: 0.7007283568382263\n",
      "epoch: 12, batch number: 784, loss: 1.1678987741470337\n",
      "epoch: 12, batch number: 785, loss: 0.8011595606803894\n",
      "epoch: 12, batch number: 786, loss: 0.7229890823364258\n",
      "epoch: 12, batch number: 787, loss: 0.9147689342498779\n",
      "epoch: 12, batch number: 788, loss: 0.5948707461357117\n",
      "epoch: 12, batch number: 789, loss: 0.8813794255256653\n",
      "epoch: 12, batch number: 790, loss: 0.8280109167098999\n",
      "epoch: 13, batch number: 791, loss: 1.8575114011764526\n",
      "epoch: 13, batch number: 792, loss: 2.0509862899780273\n",
      "epoch: 13, batch number: 793, loss: 1.4420535564422607\n",
      "epoch: 13, batch number: 794, loss: 1.208730936050415\n",
      "epoch: 13, batch number: 795, loss: 1.6654900312423706\n",
      "epoch: 13, batch number: 796, loss: 0.9299083352088928\n",
      "epoch: 13, batch number: 797, loss: 1.340087890625\n",
      "epoch: 13, batch number: 798, loss: 1.7957910299301147\n",
      "epoch: 13, batch number: 799, loss: 1.0990269184112549\n",
      "epoch: 13, batch number: 800, loss: 1.5628163814544678\n",
      "epoch: 13, batch number: 801, loss: 1.413249135017395\n",
      "epoch: 13, batch number: 802, loss: 1.68238365650177\n",
      "epoch: 13, batch number: 803, loss: 1.5575793981552124\n",
      "epoch: 13, batch number: 804, loss: 1.7111057043075562\n",
      "epoch: 13, batch number: 805, loss: 1.0300663709640503\n",
      "epoch: 13, batch number: 806, loss: 1.4847018718719482\n",
      "epoch: 13, batch number: 807, loss: 0.7674979567527771\n",
      "epoch: 13, batch number: 808, loss: 1.7332481145858765\n",
      "epoch: 13, batch number: 809, loss: 1.9650907516479492\n",
      "epoch: 13, batch number: 810, loss: 0.957023561000824\n",
      "epoch: 13, batch number: 811, loss: 1.3456676006317139\n",
      "epoch: 13, batch number: 812, loss: 1.334092617034912\n",
      "epoch: 13, batch number: 813, loss: 1.099913239479065\n",
      "epoch: 13, batch number: 814, loss: 1.5468920469284058\n",
      "epoch: 13, batch number: 815, loss: 1.502479076385498\n",
      "epoch: 13, batch number: 816, loss: 1.316863775253296\n",
      "epoch: 13, batch number: 817, loss: 0.9447696805000305\n",
      "epoch: 13, batch number: 818, loss: 1.4035392999649048\n",
      "epoch: 13, batch number: 819, loss: 1.0514190196990967\n",
      "epoch: 13, batch number: 820, loss: 1.2430155277252197\n",
      "epoch: 13, batch number: 821, loss: 1.0074468851089478\n",
      "epoch: 13, batch number: 822, loss: 1.4058685302734375\n",
      "epoch: 13, batch number: 823, loss: 1.5415641069412231\n",
      "epoch: 13, batch number: 824, loss: 1.4270788431167603\n",
      "epoch: 13, batch number: 825, loss: 1.3319236040115356\n",
      "epoch: 13, batch number: 826, loss: 0.9546718597412109\n",
      "epoch: 13, batch number: 827, loss: 1.2487913370132446\n",
      "epoch: 13, batch number: 828, loss: 1.2644643783569336\n",
      "epoch: 13, batch number: 829, loss: 1.351792812347412\n",
      "epoch: 13, batch number: 830, loss: 1.3401875495910645\n",
      "epoch: 13, batch number: 831, loss: 1.044543981552124\n",
      "epoch: 13, batch number: 832, loss: 1.1187758445739746\n",
      "epoch: 13, batch number: 833, loss: 1.0191620588302612\n",
      "epoch: 13, batch number: 834, loss: 1.0746642351150513\n",
      "epoch: 13, batch number: 835, loss: 1.04240882396698\n",
      "epoch: 13, batch number: 836, loss: 0.970699667930603\n",
      "epoch: 13, batch number: 837, loss: 1.1873562335968018\n",
      "epoch: 13, batch number: 838, loss: 1.15762197971344\n",
      "epoch: 13, batch number: 839, loss: 1.2367254495620728\n",
      "epoch: 13, batch number: 840, loss: 1.006402611732483\n",
      "epoch: 13, batch number: 841, loss: 1.1342368125915527\n",
      "epoch: 13, batch number: 842, loss: 1.2482455968856812\n",
      "epoch: 13, batch number: 843, loss: 1.2653343677520752\n",
      "epoch: 13, batch number: 844, loss: 1.1149877309799194\n",
      "epoch: 13, batch number: 845, loss: 1.046949028968811\n",
      "epoch: 13, batch number: 846, loss: 0.8712348937988281\n",
      "epoch: 13, batch number: 847, loss: 1.0267536640167236\n",
      "epoch: 13, batch number: 848, loss: 1.0600488185882568\n",
      "epoch: 13, batch number: 849, loss: 1.2971104383468628\n",
      "epoch: 13, batch number: 850, loss: 1.2775294780731201\n",
      "epoch: 13, batch number: 851, loss: 1.0752720832824707\n",
      "epoch: 13, batch number: 852, loss: 1.0678927898406982\n",
      "epoch: 14, batch number: 853, loss: 2.7704391479492188\n",
      "epoch: 14, batch number: 854, loss: 2.75523042678833\n",
      "epoch: 14, batch number: 855, loss: 2.1955645084381104\n",
      "epoch: 14, batch number: 856, loss: 2.4116153717041016\n",
      "epoch: 14, batch number: 857, loss: 2.406533718109131\n",
      "epoch: 14, batch number: 858, loss: 2.862020969390869\n",
      "epoch: 14, batch number: 859, loss: 2.7276787757873535\n",
      "epoch: 14, batch number: 860, loss: 2.3608648777008057\n",
      "epoch: 14, batch number: 861, loss: 2.7300076484680176\n",
      "epoch: 14, batch number: 862, loss: 2.697948694229126\n",
      "epoch: 14, batch number: 863, loss: 2.549163579940796\n",
      "epoch: 14, batch number: 864, loss: 2.598789930343628\n",
      "epoch: 14, batch number: 865, loss: 2.197164297103882\n",
      "epoch: 14, batch number: 866, loss: 2.712146043777466\n",
      "epoch: 14, batch number: 867, loss: 2.5031466484069824\n",
      "epoch: 14, batch number: 868, loss: 2.287977695465088\n",
      "epoch: 14, batch number: 869, loss: 2.5098483562469482\n",
      "epoch: 14, batch number: 870, loss: 2.538538694381714\n",
      "epoch: 14, batch number: 871, loss: 2.7418830394744873\n",
      "epoch: 14, batch number: 872, loss: 2.5230934619903564\n",
      "epoch: 14, batch number: 873, loss: 2.066283702850342\n",
      "epoch: 14, batch number: 874, loss: 2.3162500858306885\n",
      "epoch: 14, batch number: 875, loss: 2.3510584831237793\n",
      "epoch: 14, batch number: 876, loss: 1.8425863981246948\n",
      "epoch: 14, batch number: 877, loss: 2.088008165359497\n",
      "epoch: 14, batch number: 878, loss: 2.113426923751831\n",
      "epoch: 14, batch number: 879, loss: 2.009054183959961\n",
      "epoch: 14, batch number: 880, loss: 1.943163514137268\n",
      "epoch: 14, batch number: 881, loss: 2.3363077640533447\n",
      "epoch: 14, batch number: 882, loss: 2.2294418811798096\n",
      "epoch: 14, batch number: 883, loss: 1.887926697731018\n",
      "epoch: 14, batch number: 884, loss: 2.0401713848114014\n",
      "epoch: 14, batch number: 885, loss: 2.0575952529907227\n",
      "epoch: 14, batch number: 886, loss: 2.049503803253174\n",
      "epoch: 14, batch number: 887, loss: 2.3408761024475098\n",
      "epoch: 14, batch number: 888, loss: 1.9938863515853882\n",
      "epoch: 14, batch number: 889, loss: 1.9796022176742554\n",
      "epoch: 14, batch number: 890, loss: 1.8532572984695435\n",
      "epoch: 14, batch number: 891, loss: 1.935911774635315\n",
      "epoch: 14, batch number: 892, loss: 2.0257437229156494\n",
      "epoch: 14, batch number: 893, loss: 2.0631303787231445\n",
      "epoch: 14, batch number: 894, loss: 1.9219331741333008\n",
      "epoch: 14, batch number: 895, loss: 2.0343761444091797\n",
      "epoch: 14, batch number: 896, loss: 2.0693743228912354\n",
      "epoch: 14, batch number: 897, loss: 2.063638925552368\n",
      "epoch: 14, batch number: 898, loss: 2.161635637283325\n",
      "epoch: 14, batch number: 899, loss: 1.8652348518371582\n",
      "epoch: 14, batch number: 900, loss: 1.7382423877716064\n",
      "epoch: 14, batch number: 901, loss: 1.782112956047058\n",
      "epoch: 14, batch number: 902, loss: 2.001145601272583\n",
      "epoch: 14, batch number: 903, loss: 1.93789541721344\n",
      "epoch: 14, batch number: 904, loss: 1.9530564546585083\n",
      "epoch: 14, batch number: 905, loss: 1.8845887184143066\n",
      "epoch: 14, batch number: 906, loss: 2.0233612060546875\n",
      "epoch: 14, batch number: 907, loss: 1.9400062561035156\n",
      "epoch: 14, batch number: 908, loss: 2.354973793029785\n",
      "epoch: 14, batch number: 909, loss: 1.9766926765441895\n",
      "epoch: 14, batch number: 910, loss: 1.721208095550537\n",
      "epoch: 14, batch number: 911, loss: 1.814097285270691\n",
      "epoch: 14, batch number: 912, loss: 1.835361361503601\n",
      "epoch: 14, batch number: 913, loss: 1.8139607906341553\n",
      "epoch: 15, batch number: 914, loss: 1.5663479566574097\n",
      "epoch: 15, batch number: 915, loss: 1.4961957931518555\n",
      "epoch: 15, batch number: 916, loss: 1.4958776235580444\n",
      "epoch: 15, batch number: 917, loss: 1.48664391040802\n",
      "epoch: 15, batch number: 918, loss: 1.5930113792419434\n",
      "epoch: 15, batch number: 919, loss: 1.4084025621414185\n",
      "epoch: 15, batch number: 920, loss: 1.3235095739364624\n",
      "epoch: 15, batch number: 921, loss: 1.2984545230865479\n",
      "epoch: 15, batch number: 922, loss: 1.410396695137024\n",
      "epoch: 15, batch number: 923, loss: 1.2125203609466553\n",
      "epoch: 15, batch number: 924, loss: 1.3856384754180908\n",
      "epoch: 15, batch number: 925, loss: 1.3422471284866333\n",
      "epoch: 15, batch number: 926, loss: 1.3246077299118042\n",
      "epoch: 15, batch number: 927, loss: 1.4790265560150146\n",
      "epoch: 15, batch number: 928, loss: 1.2270125150680542\n",
      "epoch: 15, batch number: 929, loss: 1.0518757104873657\n",
      "epoch: 15, batch number: 930, loss: 1.239440679550171\n",
      "epoch: 15, batch number: 931, loss: 1.183199167251587\n",
      "epoch: 15, batch number: 932, loss: 1.2935563325881958\n",
      "epoch: 15, batch number: 933, loss: 1.1049525737762451\n",
      "epoch: 15, batch number: 934, loss: 1.130855917930603\n",
      "epoch: 15, batch number: 935, loss: 1.2785605192184448\n",
      "epoch: 15, batch number: 936, loss: 0.9645318984985352\n",
      "epoch: 15, batch number: 937, loss: 1.3570971488952637\n",
      "epoch: 15, batch number: 938, loss: 1.4146398305892944\n",
      "epoch: 15, batch number: 939, loss: 1.0891029834747314\n",
      "epoch: 15, batch number: 940, loss: 1.0371137857437134\n",
      "epoch: 15, batch number: 941, loss: 1.0951206684112549\n",
      "epoch: 15, batch number: 942, loss: 1.401185393333435\n",
      "epoch: 15, batch number: 943, loss: 0.8745849132537842\n",
      "epoch: 15, batch number: 944, loss: 0.9657461047172546\n",
      "epoch: 15, batch number: 945, loss: 0.8072498440742493\n",
      "epoch: 15, batch number: 946, loss: 0.8170920610427856\n",
      "epoch: 15, batch number: 947, loss: 1.1608631610870361\n",
      "epoch: 15, batch number: 948, loss: 1.0394165515899658\n",
      "epoch: 15, batch number: 949, loss: 1.209625482559204\n",
      "epoch: 15, batch number: 950, loss: 0.9475738406181335\n",
      "epoch: 15, batch number: 951, loss: 1.3331668376922607\n",
      "epoch: 15, batch number: 952, loss: 1.1628787517547607\n",
      "epoch: 15, batch number: 953, loss: 1.2142382860183716\n",
      "epoch: 15, batch number: 954, loss: 1.0538928508758545\n",
      "epoch: 15, batch number: 955, loss: 0.8885258436203003\n",
      "epoch: 15, batch number: 956, loss: 1.235244631767273\n",
      "epoch: 15, batch number: 957, loss: 0.9525318741798401\n",
      "epoch: 15, batch number: 958, loss: 1.1969400644302368\n",
      "epoch: 15, batch number: 959, loss: 1.0203062295913696\n",
      "epoch: 15, batch number: 960, loss: 0.8371532559394836\n",
      "epoch: 15, batch number: 961, loss: 0.821651816368103\n",
      "epoch: 15, batch number: 962, loss: 0.7470149397850037\n",
      "epoch: 15, batch number: 963, loss: 1.1870429515838623\n",
      "epoch: 15, batch number: 964, loss: 0.9938473105430603\n",
      "epoch: 15, batch number: 965, loss: 0.9984886646270752\n",
      "epoch: 15, batch number: 966, loss: 1.120470404624939\n",
      "epoch: 15, batch number: 967, loss: 1.0563822984695435\n",
      "epoch: 15, batch number: 968, loss: 0.725458025932312\n",
      "epoch: 15, batch number: 969, loss: 0.6434067487716675\n",
      "epoch: 15, batch number: 970, loss: 1.1972029209136963\n",
      "epoch: 15, batch number: 971, loss: 0.40984535217285156\n",
      "epoch: 16, batch number: 972, loss: 1.0087684392929077\n",
      "epoch: 16, batch number: 973, loss: 1.0068997144699097\n",
      "epoch: 16, batch number: 974, loss: 1.5172903537750244\n",
      "epoch: 16, batch number: 975, loss: 1.1109110116958618\n",
      "epoch: 16, batch number: 976, loss: 1.3492910861968994\n",
      "epoch: 16, batch number: 977, loss: 0.8178684115409851\n",
      "epoch: 16, batch number: 978, loss: 1.2574734687805176\n",
      "epoch: 16, batch number: 979, loss: 0.8074091076850891\n",
      "epoch: 16, batch number: 980, loss: 1.5458612442016602\n",
      "epoch: 16, batch number: 981, loss: 1.0596489906311035\n",
      "epoch: 16, batch number: 982, loss: 1.7988003492355347\n",
      "epoch: 16, batch number: 983, loss: 1.665420651435852\n",
      "epoch: 16, batch number: 984, loss: 1.11467444896698\n",
      "epoch: 16, batch number: 985, loss: 1.1413352489471436\n",
      "epoch: 16, batch number: 986, loss: 1.2348320484161377\n",
      "epoch: 16, batch number: 987, loss: 1.339760184288025\n",
      "epoch: 16, batch number: 988, loss: 1.2957537174224854\n",
      "epoch: 16, batch number: 989, loss: 1.5554238557815552\n",
      "epoch: 16, batch number: 990, loss: 1.3609141111373901\n",
      "epoch: 16, batch number: 991, loss: 1.7581169605255127\n",
      "epoch: 16, batch number: 992, loss: 0.7492219805717468\n",
      "epoch: 16, batch number: 993, loss: 1.3249155282974243\n",
      "epoch: 16, batch number: 994, loss: 1.1149808168411255\n",
      "epoch: 16, batch number: 995, loss: 0.9911054968833923\n",
      "epoch: 16, batch number: 996, loss: 1.5124417543411255\n",
      "epoch: 16, batch number: 997, loss: 1.2885425090789795\n",
      "epoch: 16, batch number: 998, loss: 1.3966511487960815\n",
      "epoch: 16, batch number: 999, loss: 0.7530192136764526\n",
      "epoch: 16, batch number: 1000, loss: 1.1718065738677979\n",
      "epoch: 16, batch number: 1001, loss: 1.0702544450759888\n",
      "epoch: 16, batch number: 1002, loss: 0.9186972379684448\n",
      "epoch: 16, batch number: 1003, loss: 1.091556429862976\n",
      "epoch: 16, batch number: 1004, loss: 0.8499091863632202\n",
      "epoch: 16, batch number: 1005, loss: 1.1426243782043457\n",
      "epoch: 16, batch number: 1006, loss: 1.3144066333770752\n",
      "epoch: 16, batch number: 1007, loss: 1.2975934743881226\n",
      "epoch: 16, batch number: 1008, loss: 0.7881050705909729\n",
      "epoch: 16, batch number: 1009, loss: 0.987710177898407\n",
      "epoch: 16, batch number: 1010, loss: 1.157814860343933\n",
      "epoch: 16, batch number: 1011, loss: 1.2152037620544434\n",
      "epoch: 16, batch number: 1012, loss: 0.9983291625976562\n",
      "epoch: 16, batch number: 1013, loss: 1.44357168674469\n",
      "epoch: 16, batch number: 1014, loss: 0.7670691013336182\n",
      "epoch: 16, batch number: 1015, loss: 1.6786649227142334\n",
      "epoch: 16, batch number: 1016, loss: 1.1766328811645508\n",
      "epoch: 16, batch number: 1017, loss: 1.2357938289642334\n",
      "epoch: 16, batch number: 1018, loss: 1.161865234375\n",
      "epoch: 16, batch number: 1019, loss: 1.0363465547561646\n",
      "epoch: 16, batch number: 1020, loss: 1.6130602359771729\n",
      "epoch: 16, batch number: 1021, loss: 1.237067461013794\n",
      "epoch: 16, batch number: 1022, loss: 1.080437183380127\n",
      "epoch: 16, batch number: 1023, loss: 1.0002436637878418\n",
      "epoch: 16, batch number: 1024, loss: 1.2515699863433838\n",
      "epoch: 17, batch number: 1025, loss: 1.3327250480651855\n",
      "epoch: 17, batch number: 1026, loss: 1.8389487266540527\n",
      "epoch: 17, batch number: 1027, loss: 1.383698582649231\n",
      "epoch: 17, batch number: 1028, loss: 1.2233827114105225\n",
      "epoch: 17, batch number: 1029, loss: 1.3777859210968018\n",
      "epoch: 17, batch number: 1030, loss: 1.4059858322143555\n",
      "epoch: 17, batch number: 1031, loss: 1.2565313577651978\n",
      "epoch: 17, batch number: 1032, loss: 1.2946339845657349\n",
      "epoch: 17, batch number: 1033, loss: 1.3649399280548096\n",
      "epoch: 17, batch number: 1034, loss: 1.2305431365966797\n",
      "epoch: 17, batch number: 1035, loss: 1.1576933860778809\n",
      "epoch: 17, batch number: 1036, loss: 1.3813196420669556\n",
      "epoch: 17, batch number: 1037, loss: 1.3228384256362915\n",
      "epoch: 17, batch number: 1038, loss: 1.1083314418792725\n",
      "epoch: 17, batch number: 1039, loss: 1.238549828529358\n",
      "epoch: 17, batch number: 1040, loss: 1.7868918180465698\n",
      "epoch: 17, batch number: 1041, loss: 1.0230698585510254\n",
      "epoch: 17, batch number: 1042, loss: 1.132519245147705\n",
      "epoch: 17, batch number: 1043, loss: 1.2668555974960327\n",
      "epoch: 17, batch number: 1044, loss: 1.0536601543426514\n",
      "epoch: 17, batch number: 1045, loss: 1.4629874229431152\n",
      "epoch: 17, batch number: 1046, loss: 1.037903904914856\n",
      "epoch: 17, batch number: 1047, loss: 1.0834729671478271\n",
      "epoch: 17, batch number: 1048, loss: 1.002333641052246\n",
      "epoch: 17, batch number: 1049, loss: 1.2103352546691895\n",
      "epoch: 17, batch number: 1050, loss: 0.9931443929672241\n",
      "epoch: 17, batch number: 1051, loss: 1.2603201866149902\n",
      "epoch: 17, batch number: 1052, loss: 0.856369137763977\n",
      "epoch: 17, batch number: 1053, loss: 0.9439259767532349\n",
      "epoch: 17, batch number: 1054, loss: 0.8437402248382568\n",
      "epoch: 17, batch number: 1055, loss: 0.884282112121582\n",
      "epoch: 17, batch number: 1056, loss: 1.1094294786453247\n",
      "epoch: 17, batch number: 1057, loss: 1.2101012468338013\n",
      "epoch: 17, batch number: 1058, loss: 0.9849890470504761\n",
      "epoch: 17, batch number: 1059, loss: 0.8802961707115173\n",
      "epoch: 17, batch number: 1060, loss: 0.9332456588745117\n",
      "epoch: 17, batch number: 1061, loss: 0.941807210445404\n",
      "epoch: 17, batch number: 1062, loss: 0.9412871599197388\n",
      "epoch: 17, batch number: 1063, loss: 0.9276970028877258\n",
      "epoch: 17, batch number: 1064, loss: 0.9572884440422058\n",
      "epoch: 17, batch number: 1065, loss: 0.9679821133613586\n",
      "epoch: 17, batch number: 1066, loss: 0.9326499700546265\n",
      "epoch: 17, batch number: 1067, loss: 0.9099969267845154\n",
      "epoch: 17, batch number: 1068, loss: 0.8953767418861389\n",
      "epoch: 17, batch number: 1069, loss: 1.2169359922409058\n",
      "epoch: 17, batch number: 1070, loss: 0.8346168398857117\n",
      "epoch: 17, batch number: 1071, loss: 0.9208913445472717\n",
      "epoch: 18, batch number: 1072, loss: 0.8503222465515137\n",
      "epoch: 18, batch number: 1073, loss: 0.865580141544342\n",
      "epoch: 18, batch number: 1074, loss: 0.8593233823776245\n",
      "epoch: 18, batch number: 1075, loss: 0.8412310481071472\n",
      "epoch: 18, batch number: 1076, loss: 0.825585126876831\n",
      "epoch: 18, batch number: 1077, loss: 0.8463171720504761\n",
      "epoch: 18, batch number: 1078, loss: 0.8750451803207397\n",
      "epoch: 18, batch number: 1079, loss: 0.8018206357955933\n",
      "epoch: 18, batch number: 1080, loss: 0.8135265707969666\n",
      "epoch: 18, batch number: 1081, loss: 0.776913046836853\n",
      "epoch: 18, batch number: 1082, loss: 0.7783904075622559\n",
      "epoch: 18, batch number: 1083, loss: 0.8711099624633789\n",
      "epoch: 18, batch number: 1084, loss: 0.7745050191879272\n",
      "epoch: 18, batch number: 1085, loss: 0.7676433324813843\n",
      "epoch: 18, batch number: 1086, loss: 0.7624256610870361\n",
      "epoch: 18, batch number: 1087, loss: 0.7671826481819153\n",
      "epoch: 18, batch number: 1088, loss: 0.7922557592391968\n",
      "epoch: 18, batch number: 1089, loss: 0.7187422513961792\n",
      "epoch: 18, batch number: 1090, loss: 0.7490140199661255\n",
      "epoch: 18, batch number: 1091, loss: 0.7484738826751709\n",
      "epoch: 18, batch number: 1092, loss: 0.7583581209182739\n",
      "epoch: 18, batch number: 1093, loss: 0.7723851203918457\n",
      "epoch: 18, batch number: 1094, loss: 0.7524707317352295\n",
      "epoch: 18, batch number: 1095, loss: 0.7978245615959167\n",
      "epoch: 18, batch number: 1096, loss: 0.6691009402275085\n",
      "epoch: 18, batch number: 1097, loss: 0.7787176966667175\n",
      "epoch: 18, batch number: 1098, loss: 0.7233819365501404\n",
      "epoch: 18, batch number: 1099, loss: 0.7447806596755981\n",
      "epoch: 18, batch number: 1100, loss: 0.7138980627059937\n",
      "epoch: 18, batch number: 1101, loss: 0.7348996996879578\n",
      "epoch: 18, batch number: 1102, loss: 0.6862578392028809\n",
      "epoch: 18, batch number: 1103, loss: 0.7345325946807861\n",
      "epoch: 18, batch number: 1104, loss: 0.7120813131332397\n",
      "epoch: 18, batch number: 1105, loss: 0.7104288935661316\n",
      "epoch: 18, batch number: 1106, loss: 0.7203570604324341\n",
      "epoch: 18, batch number: 1107, loss: 0.7149088978767395\n",
      "epoch: 18, batch number: 1108, loss: 0.6868358850479126\n",
      "epoch: 18, batch number: 1109, loss: 0.7754654288291931\n",
      "epoch: 18, batch number: 1110, loss: 0.7285542488098145\n",
      "epoch: 18, batch number: 1111, loss: 0.707858145236969\n",
      "epoch: 18, batch number: 1112, loss: 0.6939282417297363\n",
      "epoch: 18, batch number: 1113, loss: 0.735615074634552\n",
      "epoch: 18, batch number: 1114, loss: 0.6604090332984924\n",
      "epoch: 18, batch number: 1115, loss: 0.6981563568115234\n",
      "epoch: 18, batch number: 1116, loss: 0.6303601861000061\n",
      "epoch: 18, batch number: 1117, loss: 0.6854540705680847\n",
      "epoch: 18, batch number: 1118, loss: 0.6794537901878357\n",
      "epoch: 18, batch number: 1119, loss: 0.6867321133613586\n",
      "epoch: 18, batch number: 1120, loss: 0.6466626524925232\n",
      "epoch: 18, batch number: 1121, loss: 0.7402145862579346\n",
      "epoch: 18, batch number: 1122, loss: 0.6532743573188782\n",
      "epoch: 18, batch number: 1123, loss: 0.7692627906799316\n",
      "epoch: 18, batch number: 1124, loss: 0.6781134605407715\n",
      "epoch: 18, batch number: 1125, loss: 0.6240394711494446\n",
      "epoch: 18, batch number: 1126, loss: 0.789931058883667\n",
      "epoch: 19, batch number: 1127, loss: 1.286328911781311\n",
      "epoch: 19, batch number: 1128, loss: 1.7024911642074585\n",
      "epoch: 19, batch number: 1129, loss: 1.3771237134933472\n",
      "epoch: 19, batch number: 1130, loss: 1.0376235246658325\n",
      "epoch: 19, batch number: 1131, loss: 1.6567955017089844\n",
      "epoch: 19, batch number: 1132, loss: 1.0930143594741821\n",
      "epoch: 19, batch number: 1133, loss: 1.0654016733169556\n",
      "epoch: 19, batch number: 1134, loss: 1.0704820156097412\n",
      "epoch: 19, batch number: 1135, loss: 1.2173070907592773\n",
      "epoch: 19, batch number: 1136, loss: 1.1227155923843384\n",
      "epoch: 19, batch number: 1137, loss: 1.0296117067337036\n",
      "epoch: 19, batch number: 1138, loss: 1.376693606376648\n",
      "epoch: 19, batch number: 1139, loss: 1.1566495895385742\n",
      "epoch: 19, batch number: 1140, loss: 1.1027698516845703\n",
      "epoch: 19, batch number: 1141, loss: 1.1929994821548462\n",
      "epoch: 19, batch number: 1142, loss: 1.249563455581665\n",
      "epoch: 19, batch number: 1143, loss: 1.222690463066101\n",
      "epoch: 19, batch number: 1144, loss: 1.1397736072540283\n",
      "epoch: 19, batch number: 1145, loss: 1.3148586750030518\n",
      "epoch: 19, batch number: 1146, loss: 1.2119455337524414\n",
      "epoch: 19, batch number: 1147, loss: 1.2177965641021729\n",
      "epoch: 19, batch number: 1148, loss: 1.0996872186660767\n",
      "epoch: 19, batch number: 1149, loss: 1.1432232856750488\n",
      "epoch: 19, batch number: 1150, loss: 1.2412625551223755\n",
      "epoch: 19, batch number: 1151, loss: 1.123177170753479\n",
      "epoch: 19, batch number: 1152, loss: 0.929253339767456\n",
      "epoch: 19, batch number: 1153, loss: 1.0933411121368408\n",
      "epoch: 19, batch number: 1154, loss: 0.9513018727302551\n",
      "epoch: 19, batch number: 1155, loss: 1.027837872505188\n",
      "epoch: 19, batch number: 1156, loss: 1.2777824401855469\n",
      "epoch: 19, batch number: 1157, loss: 1.1246225833892822\n",
      "epoch: 19, batch number: 1158, loss: 1.2947826385498047\n",
      "epoch: 19, batch number: 1159, loss: 1.3391913175582886\n",
      "epoch: 19, batch number: 1160, loss: 1.1275553703308105\n",
      "epoch: 19, batch number: 1161, loss: 1.0913344621658325\n",
      "epoch: 19, batch number: 1162, loss: 1.0345637798309326\n",
      "epoch: 19, batch number: 1163, loss: 1.1184762716293335\n",
      "epoch: 19, batch number: 1164, loss: 0.7668684124946594\n",
      "epoch: 19, batch number: 1165, loss: 1.2200143337249756\n",
      "epoch: 19, batch number: 1166, loss: 0.9001973867416382\n",
      "epoch: 19, batch number: 1167, loss: 1.1024144887924194\n",
      "epoch: 19, batch number: 1168, loss: 1.053768277168274\n",
      "epoch: 19, batch number: 1169, loss: 1.4329744577407837\n",
      "epoch: 19, batch number: 1170, loss: 1.2361985445022583\n",
      "epoch: 19, batch number: 1171, loss: 1.1716337203979492\n",
      "epoch: 19, batch number: 1172, loss: 1.1337395906448364\n",
      "epoch: 19, batch number: 1173, loss: 1.3597321510314941\n",
      "epoch: 19, batch number: 1174, loss: 0.9842621088027954\n",
      "epoch: 19, batch number: 1175, loss: 1.030957818031311\n",
      "epoch: 19, batch number: 1176, loss: 0.9141836166381836\n",
      "epoch: 20, batch number: 1177, loss: 0.7951918840408325\n",
      "epoch: 20, batch number: 1178, loss: 0.8243067264556885\n",
      "epoch: 20, batch number: 1179, loss: 1.0136804580688477\n",
      "epoch: 20, batch number: 1180, loss: 0.7689211964607239\n",
      "epoch: 20, batch number: 1181, loss: 0.7203765511512756\n",
      "epoch: 20, batch number: 1182, loss: 0.7643386125564575\n",
      "epoch: 20, batch number: 1183, loss: 0.8410658240318298\n",
      "epoch: 20, batch number: 1184, loss: 0.7919934391975403\n",
      "epoch: 20, batch number: 1185, loss: 0.8056030869483948\n",
      "epoch: 20, batch number: 1186, loss: 0.7320998907089233\n",
      "epoch: 20, batch number: 1187, loss: 0.7922846674919128\n",
      "epoch: 20, batch number: 1188, loss: 0.7249530553817749\n",
      "epoch: 20, batch number: 1189, loss: 0.9841600656509399\n",
      "epoch: 20, batch number: 1190, loss: 0.8087906837463379\n",
      "epoch: 20, batch number: 1191, loss: 0.7429049611091614\n",
      "epoch: 20, batch number: 1192, loss: 0.7421194314956665\n",
      "epoch: 20, batch number: 1193, loss: 0.8523272275924683\n",
      "epoch: 20, batch number: 1194, loss: 0.7206529378890991\n",
      "epoch: 20, batch number: 1195, loss: 0.8128460049629211\n",
      "epoch: 20, batch number: 1196, loss: 0.8635033369064331\n",
      "epoch: 20, batch number: 1197, loss: 0.7279854416847229\n",
      "epoch: 20, batch number: 1198, loss: 0.8020270466804504\n",
      "epoch: 20, batch number: 1199, loss: 0.855414628982544\n",
      "epoch: 20, batch number: 1200, loss: 1.0582983493804932\n",
      "epoch: 20, batch number: 1201, loss: 0.8230506181716919\n",
      "epoch: 20, batch number: 1202, loss: 0.9177044034004211\n",
      "epoch: 20, batch number: 1203, loss: 0.8415735960006714\n",
      "epoch: 20, batch number: 1204, loss: 0.7236760258674622\n",
      "epoch: 20, batch number: 1205, loss: 0.6936152577400208\n",
      "epoch: 20, batch number: 1206, loss: 0.7325197458267212\n",
      "epoch: 20, batch number: 1207, loss: 0.8094886541366577\n",
      "epoch: 20, batch number: 1208, loss: 0.8391016125679016\n",
      "epoch: 20, batch number: 1209, loss: 0.7650225758552551\n",
      "epoch: 20, batch number: 1210, loss: 0.8667954802513123\n",
      "epoch: 20, batch number: 1211, loss: 0.8453661203384399\n",
      "epoch: 20, batch number: 1212, loss: 0.6774646639823914\n",
      "epoch: 20, batch number: 1213, loss: 0.7055800557136536\n",
      "epoch: 20, batch number: 1214, loss: 0.7875158786773682\n",
      "epoch: 20, batch number: 1215, loss: 0.7761577367782593\n",
      "epoch: 20, batch number: 1216, loss: 0.717080295085907\n",
      "epoch: 20, batch number: 1217, loss: 0.8975852727890015\n",
      "epoch: 20, batch number: 1218, loss: 0.8064634799957275\n",
      "epoch: 20, batch number: 1219, loss: 0.9247590899467468\n",
      "epoch: 20, batch number: 1220, loss: 0.8704112768173218\n",
      "epoch: 20, batch number: 1221, loss: 0.9236108660697937\n",
      "epoch: 20, batch number: 1222, loss: 0.661339282989502\n",
      "epoch: 20, batch number: 1223, loss: 0.7493816018104553\n",
      "epoch: 20, batch number: 1224, loss: 0.7037999629974365\n",
      "epoch: 20, batch number: 1225, loss: 0.97475665807724\n",
      "epoch: 20, batch number: 1226, loss: 0.724245548248291\n",
      "epoch: 20, batch number: 1227, loss: 0.7778487801551819\n",
      "epoch: 20, batch number: 1228, loss: 0.7497738003730774\n",
      "epoch: 20, batch number: 1229, loss: 0.8793885707855225\n",
      "epoch: 20, batch number: 1230, loss: 0.8242471218109131\n",
      "epoch: 21, batch number: 1231, loss: 0.7682105302810669\n",
      "epoch: 21, batch number: 1232, loss: 0.9382971525192261\n",
      "epoch: 21, batch number: 1233, loss: 0.7415226101875305\n",
      "epoch: 21, batch number: 1234, loss: 0.7487207651138306\n",
      "epoch: 21, batch number: 1235, loss: 0.6567906141281128\n",
      "epoch: 21, batch number: 1236, loss: 0.7040444016456604\n",
      "epoch: 21, batch number: 1237, loss: 1.124430775642395\n",
      "epoch: 21, batch number: 1238, loss: 0.7802971005439758\n",
      "epoch: 21, batch number: 1239, loss: 0.7282432913780212\n",
      "epoch: 21, batch number: 1240, loss: 0.8005442023277283\n",
      "epoch: 21, batch number: 1241, loss: 0.8005165457725525\n",
      "epoch: 21, batch number: 1242, loss: 0.7213946580886841\n",
      "epoch: 21, batch number: 1243, loss: 0.6760590672492981\n",
      "epoch: 21, batch number: 1244, loss: 0.717380166053772\n",
      "epoch: 21, batch number: 1245, loss: 0.7114000916481018\n",
      "epoch: 21, batch number: 1246, loss: 0.8291829824447632\n",
      "epoch: 21, batch number: 1247, loss: 0.6469921469688416\n",
      "epoch: 21, batch number: 1248, loss: 0.669931173324585\n",
      "epoch: 21, batch number: 1249, loss: 0.740357518196106\n",
      "epoch: 21, batch number: 1250, loss: 0.859070360660553\n",
      "epoch: 21, batch number: 1251, loss: 0.8495832085609436\n",
      "epoch: 21, batch number: 1252, loss: 0.6559376120567322\n",
      "epoch: 21, batch number: 1253, loss: 0.796131432056427\n",
      "epoch: 21, batch number: 1254, loss: 0.6984671354293823\n",
      "epoch: 21, batch number: 1255, loss: 0.8322187662124634\n",
      "epoch: 21, batch number: 1256, loss: 0.7249370813369751\n",
      "epoch: 21, batch number: 1257, loss: 0.7204910516738892\n",
      "epoch: 21, batch number: 1258, loss: 0.7076060175895691\n",
      "epoch: 21, batch number: 1259, loss: 0.7912696599960327\n",
      "epoch: 21, batch number: 1260, loss: 0.7907402515411377\n",
      "epoch: 21, batch number: 1261, loss: 0.6684929728507996\n",
      "epoch: 21, batch number: 1262, loss: 0.882606565952301\n",
      "epoch: 21, batch number: 1263, loss: 0.7873601317405701\n",
      "epoch: 21, batch number: 1264, loss: 0.7187356948852539\n",
      "epoch: 21, batch number: 1265, loss: 0.748071014881134\n",
      "epoch: 21, batch number: 1266, loss: 0.6606377959251404\n",
      "epoch: 21, batch number: 1267, loss: 0.6728070974349976\n",
      "epoch: 21, batch number: 1268, loss: 0.7157526016235352\n",
      "epoch: 21, batch number: 1269, loss: 0.8253360390663147\n",
      "epoch: 21, batch number: 1270, loss: 0.8163877129554749\n",
      "epoch: 21, batch number: 1271, loss: 0.6439229846000671\n",
      "epoch: 21, batch number: 1272, loss: 0.7786444425582886\n",
      "epoch: 21, batch number: 1273, loss: 0.9271019101142883\n",
      "epoch: 21, batch number: 1274, loss: 0.6960870027542114\n",
      "epoch: 21, batch number: 1275, loss: 1.013580560684204\n",
      "epoch: 21, batch number: 1276, loss: 0.7315502166748047\n",
      "epoch: 21, batch number: 1277, loss: 0.6580014824867249\n",
      "epoch: 21, batch number: 1278, loss: 0.8026227951049805\n",
      "epoch: 22, batch number: 1279, loss: 0.6545279622077942\n",
      "epoch: 22, batch number: 1280, loss: 0.6796210408210754\n",
      "epoch: 22, batch number: 1281, loss: 0.6908184289932251\n",
      "epoch: 22, batch number: 1282, loss: 0.6525246500968933\n",
      "epoch: 22, batch number: 1283, loss: 0.6245843172073364\n",
      "epoch: 22, batch number: 1284, loss: 0.6381673216819763\n",
      "epoch: 22, batch number: 1285, loss: 0.6099451184272766\n",
      "epoch: 22, batch number: 1286, loss: 0.6347568035125732\n",
      "epoch: 22, batch number: 1287, loss: 0.6071090698242188\n",
      "epoch: 22, batch number: 1288, loss: 0.6381111145019531\n",
      "epoch: 22, batch number: 1289, loss: 0.6692200303077698\n",
      "epoch: 22, batch number: 1290, loss: 0.6283828616142273\n",
      "epoch: 22, batch number: 1291, loss: 0.630921483039856\n",
      "epoch: 22, batch number: 1292, loss: 0.8108639717102051\n",
      "epoch: 22, batch number: 1293, loss: 0.632038950920105\n",
      "epoch: 22, batch number: 1294, loss: 0.5884020328521729\n",
      "epoch: 22, batch number: 1295, loss: 0.6352694034576416\n",
      "epoch: 22, batch number: 1296, loss: 0.6931827068328857\n",
      "epoch: 22, batch number: 1297, loss: 0.7486128807067871\n",
      "epoch: 22, batch number: 1298, loss: 0.5591097474098206\n",
      "epoch: 22, batch number: 1299, loss: 0.6692739725112915\n",
      "epoch: 22, batch number: 1300, loss: 0.6431536078453064\n",
      "epoch: 22, batch number: 1301, loss: 0.7902129888534546\n",
      "epoch: 22, batch number: 1302, loss: 0.6126692891120911\n",
      "epoch: 22, batch number: 1303, loss: 0.5880163908004761\n",
      "epoch: 22, batch number: 1304, loss: 0.6239598989486694\n",
      "epoch: 22, batch number: 1305, loss: 0.6011399030685425\n",
      "epoch: 22, batch number: 1306, loss: 0.6295482516288757\n",
      "epoch: 22, batch number: 1307, loss: 0.5663620829582214\n",
      "epoch: 22, batch number: 1308, loss: 0.5044358372688293\n",
      "epoch: 22, batch number: 1309, loss: 0.5755864977836609\n",
      "epoch: 22, batch number: 1310, loss: 0.5171535015106201\n",
      "epoch: 22, batch number: 1311, loss: 0.5762050747871399\n",
      "epoch: 22, batch number: 1312, loss: 0.5316789746284485\n",
      "epoch: 22, batch number: 1313, loss: 0.49212828278541565\n",
      "epoch: 22, batch number: 1314, loss: 0.6200246214866638\n",
      "epoch: 22, batch number: 1315, loss: 0.666898787021637\n",
      "epoch: 22, batch number: 1316, loss: 0.700783371925354\n",
      "epoch: 22, batch number: 1317, loss: 0.47561025619506836\n",
      "epoch: 22, batch number: 1318, loss: 0.7291266322135925\n",
      "epoch: 22, batch number: 1319, loss: 0.6672766208648682\n",
      "epoch: 22, batch number: 1320, loss: 0.6468745470046997\n",
      "epoch: 22, batch number: 1321, loss: 0.5968058705329895\n",
      "epoch: 22, batch number: 1322, loss: 0.5417787432670593\n",
      "epoch: 22, batch number: 1323, loss: 0.7054174542427063\n",
      "epoch: 22, batch number: 1324, loss: 0.6351775527000427\n",
      "epoch: 22, batch number: 1325, loss: 0.5569829344749451\n",
      "epoch: 22, batch number: 1326, loss: 0.5814599990844727\n",
      "epoch: 22, batch number: 1327, loss: 0.5550885796546936\n",
      "epoch: 22, batch number: 1328, loss: 0.6063137054443359\n",
      "epoch: 22, batch number: 1329, loss: 0.5297818779945374\n",
      "epoch: 22, batch number: 1330, loss: 0.48032864928245544\n",
      "epoch: 22, batch number: 1331, loss: 0.6206046342849731\n",
      "epoch: 23, batch number: 1332, loss: 0.708306074142456\n",
      "epoch: 23, batch number: 1333, loss: 0.6275662183761597\n",
      "epoch: 23, batch number: 1334, loss: 0.5862351655960083\n",
      "epoch: 23, batch number: 1335, loss: 0.7794176936149597\n",
      "epoch: 23, batch number: 1336, loss: 0.6294534206390381\n",
      "epoch: 23, batch number: 1337, loss: 0.6752893924713135\n",
      "epoch: 23, batch number: 1338, loss: 0.7357999682426453\n",
      "epoch: 23, batch number: 1339, loss: 0.5922481417655945\n",
      "epoch: 23, batch number: 1340, loss: 0.6029453277587891\n",
      "epoch: 23, batch number: 1341, loss: 0.695862352848053\n",
      "epoch: 23, batch number: 1342, loss: 0.5777608156204224\n",
      "epoch: 23, batch number: 1343, loss: 0.6468203067779541\n",
      "epoch: 23, batch number: 1344, loss: 0.5665386319160461\n",
      "epoch: 23, batch number: 1345, loss: 0.6270676851272583\n",
      "epoch: 23, batch number: 1346, loss: 0.5816717743873596\n",
      "epoch: 23, batch number: 1347, loss: 0.6601930856704712\n",
      "epoch: 23, batch number: 1348, loss: 0.6475434899330139\n",
      "epoch: 23, batch number: 1349, loss: 0.6042290329933167\n",
      "epoch: 23, batch number: 1350, loss: 0.7171400189399719\n",
      "epoch: 23, batch number: 1351, loss: 0.7105472683906555\n",
      "epoch: 23, batch number: 1352, loss: 0.5946153998374939\n",
      "epoch: 23, batch number: 1353, loss: 0.5570568442344666\n",
      "epoch: 23, batch number: 1354, loss: 0.6138404011726379\n",
      "epoch: 23, batch number: 1355, loss: 0.631165087223053\n",
      "epoch: 23, batch number: 1356, loss: 0.5917984843254089\n",
      "epoch: 23, batch number: 1357, loss: 0.6131203174591064\n",
      "epoch: 23, batch number: 1358, loss: 0.6978018283843994\n",
      "epoch: 23, batch number: 1359, loss: 0.8364796042442322\n",
      "epoch: 23, batch number: 1360, loss: 0.6905368566513062\n",
      "epoch: 23, batch number: 1361, loss: 0.647555947303772\n",
      "epoch: 23, batch number: 1362, loss: 0.6643970608711243\n",
      "epoch: 23, batch number: 1363, loss: 0.6351554989814758\n",
      "epoch: 23, batch number: 1364, loss: 0.7443805932998657\n",
      "epoch: 23, batch number: 1365, loss: 0.6086857914924622\n",
      "epoch: 23, batch number: 1366, loss: 0.6346726417541504\n",
      "epoch: 23, batch number: 1367, loss: 0.7158050537109375\n",
      "epoch: 23, batch number: 1368, loss: 0.6267865300178528\n",
      "epoch: 23, batch number: 1369, loss: 0.645880937576294\n",
      "epoch: 23, batch number: 1370, loss: 0.6487475037574768\n",
      "epoch: 23, batch number: 1371, loss: 0.6174315810203552\n",
      "epoch: 23, batch number: 1372, loss: 0.5555038452148438\n",
      "epoch: 23, batch number: 1373, loss: 0.6184031963348389\n",
      "epoch: 23, batch number: 1374, loss: 0.6904411911964417\n",
      "epoch: 23, batch number: 1375, loss: 0.5493305325508118\n",
      "epoch: 23, batch number: 1376, loss: 0.6554266214370728\n",
      "epoch: 23, batch number: 1377, loss: 0.5600407123565674\n",
      "epoch: 23, batch number: 1378, loss: 0.6307342052459717\n",
      "epoch: 23, batch number: 1379, loss: 0.5586842894554138\n",
      "epoch: 23, batch number: 1380, loss: 0.5690733790397644\n",
      "epoch: 23, batch number: 1381, loss: 0.6745730638504028\n",
      "epoch: 23, batch number: 1382, loss: 0.6015512943267822\n",
      "epoch: 23, batch number: 1383, loss: 0.55776047706604\n",
      "epoch: 23, batch number: 1384, loss: 0.5753688812255859\n",
      "epoch: 23, batch number: 1385, loss: 0.5694639086723328\n",
      "epoch: 23, batch number: 1386, loss: 0.587016224861145\n",
      "epoch: 23, batch number: 1387, loss: 0.6872602105140686\n",
      "epoch: 23, batch number: 1388, loss: 0.6113691926002502\n",
      "epoch: 24, batch number: 1389, loss: 0.5960400700569153\n",
      "epoch: 24, batch number: 1390, loss: 0.5617071986198425\n",
      "epoch: 24, batch number: 1391, loss: 0.6366830468177795\n",
      "epoch: 24, batch number: 1392, loss: 0.5882578492164612\n",
      "epoch: 24, batch number: 1393, loss: 0.5407000780105591\n",
      "epoch: 24, batch number: 1394, loss: 0.6122297644615173\n",
      "epoch: 24, batch number: 1395, loss: 0.569692075252533\n",
      "epoch: 24, batch number: 1396, loss: 0.7368919849395752\n",
      "epoch: 24, batch number: 1397, loss: 0.6580370664596558\n",
      "epoch: 24, batch number: 1398, loss: 0.7545562386512756\n",
      "epoch: 24, batch number: 1399, loss: 0.6408246159553528\n",
      "epoch: 24, batch number: 1400, loss: 0.8508750200271606\n",
      "epoch: 24, batch number: 1401, loss: 0.6216549873352051\n",
      "epoch: 24, batch number: 1402, loss: 0.7604993581771851\n",
      "epoch: 24, batch number: 1403, loss: 0.6168575882911682\n",
      "epoch: 24, batch number: 1404, loss: 0.7117267847061157\n",
      "epoch: 24, batch number: 1405, loss: 0.807109534740448\n",
      "epoch: 24, batch number: 1406, loss: 0.718460738658905\n",
      "epoch: 24, batch number: 1407, loss: 0.6511968970298767\n",
      "epoch: 24, batch number: 1408, loss: 0.7575653195381165\n",
      "epoch: 24, batch number: 1409, loss: 0.6660848259925842\n",
      "epoch: 24, batch number: 1410, loss: 0.6896551251411438\n",
      "epoch: 24, batch number: 1411, loss: 0.592312216758728\n",
      "epoch: 24, batch number: 1412, loss: 0.6407780051231384\n",
      "epoch: 24, batch number: 1413, loss: 0.5779260396957397\n",
      "epoch: 24, batch number: 1414, loss: 0.6127526760101318\n",
      "epoch: 24, batch number: 1415, loss: 0.631628155708313\n",
      "epoch: 24, batch number: 1416, loss: 0.6069820523262024\n",
      "epoch: 24, batch number: 1417, loss: 0.5769228935241699\n",
      "epoch: 24, batch number: 1418, loss: 0.5839795470237732\n",
      "epoch: 24, batch number: 1419, loss: 0.6563000679016113\n",
      "epoch: 24, batch number: 1420, loss: 0.5880685448646545\n",
      "epoch: 24, batch number: 1421, loss: 0.5943004488945007\n",
      "epoch: 24, batch number: 1422, loss: 0.602157711982727\n",
      "epoch: 24, batch number: 1423, loss: 0.6124683022499084\n",
      "epoch: 24, batch number: 1424, loss: 0.6078671813011169\n",
      "epoch: 24, batch number: 1425, loss: 0.6091228723526001\n",
      "epoch: 24, batch number: 1426, loss: 0.5741219520568848\n",
      "epoch: 24, batch number: 1427, loss: 0.6614350080490112\n",
      "epoch: 24, batch number: 1428, loss: 0.655554473400116\n",
      "epoch: 24, batch number: 1429, loss: 0.5961835980415344\n",
      "epoch: 24, batch number: 1430, loss: 0.6149508953094482\n",
      "epoch: 24, batch number: 1431, loss: 0.5562722682952881\n",
      "epoch: 24, batch number: 1432, loss: 0.6722409725189209\n",
      "epoch: 24, batch number: 1433, loss: 0.582099437713623\n",
      "epoch: 24, batch number: 1434, loss: 0.6213210225105286\n",
      "epoch: 24, batch number: 1435, loss: 0.8099793195724487\n",
      "epoch: 24, batch number: 1436, loss: 0.7124857306480408\n",
      "epoch: 24, batch number: 1437, loss: 0.5867589116096497\n",
      "epoch: 24, batch number: 1438, loss: 0.6327844858169556\n",
      "epoch: 24, batch number: 1439, loss: 0.6110784411430359\n",
      "epoch: 24, batch number: 1440, loss: 0.5735435485839844\n",
      "epoch: 24, batch number: 1441, loss: 0.5870278477668762\n",
      "epoch: 24, batch number: 1442, loss: 0.5502172708511353\n",
      "epoch: 24, batch number: 1443, loss: 0.5978147387504578\n",
      "epoch: 24, batch number: 1444, loss: 0.6663600206375122\n",
      "epoch: 24, batch number: 1445, loss: 0.6221120953559875\n",
      "epoch: 24, batch number: 1446, loss: 0.5578310489654541\n",
      "epoch: 24, batch number: 1447, loss: 0.6553359627723694\n",
      "epoch: 24, batch number: 1448, loss: 0.595579981803894\n",
      "epoch: 24, batch number: 1449, loss: 0.6426213383674622\n",
      "epoch: 24, batch number: 1450, loss: 0.6699962615966797\n",
      "epoch: 25, batch number: 1451, loss: 0.8258722424507141\n",
      "epoch: 25, batch number: 1452, loss: 0.5502340197563171\n",
      "epoch: 25, batch number: 1453, loss: 0.6227169632911682\n",
      "epoch: 25, batch number: 1454, loss: 0.7216396331787109\n",
      "epoch: 25, batch number: 1455, loss: 0.5750322937965393\n",
      "epoch: 25, batch number: 1456, loss: 0.5951155424118042\n",
      "epoch: 25, batch number: 1457, loss: 0.6631014943122864\n",
      "epoch: 25, batch number: 1458, loss: 0.6241152882575989\n",
      "epoch: 25, batch number: 1459, loss: 0.5953697562217712\n",
      "epoch: 25, batch number: 1460, loss: 0.6190912127494812\n",
      "epoch: 25, batch number: 1461, loss: 0.6521021723747253\n",
      "epoch: 25, batch number: 1462, loss: 0.6315736770629883\n",
      "epoch: 25, batch number: 1463, loss: 0.6247378587722778\n",
      "epoch: 25, batch number: 1464, loss: 0.6361805200576782\n",
      "epoch: 25, batch number: 1465, loss: 0.6291788220405579\n",
      "epoch: 25, batch number: 1466, loss: 0.6234737634658813\n",
      "epoch: 25, batch number: 1467, loss: 0.6410074234008789\n",
      "epoch: 25, batch number: 1468, loss: 0.5630706548690796\n",
      "epoch: 25, batch number: 1469, loss: 0.5957909226417542\n",
      "epoch: 25, batch number: 1470, loss: 0.5841027498245239\n",
      "epoch: 25, batch number: 1471, loss: 0.7397539615631104\n",
      "epoch: 25, batch number: 1472, loss: 0.6976554989814758\n",
      "epoch: 25, batch number: 1473, loss: 0.9489253759384155\n",
      "epoch: 25, batch number: 1474, loss: 0.6274455785751343\n",
      "epoch: 25, batch number: 1475, loss: 0.7361674308776855\n",
      "epoch: 25, batch number: 1476, loss: 0.7691600322723389\n",
      "epoch: 25, batch number: 1477, loss: 0.5396079421043396\n",
      "epoch: 25, batch number: 1478, loss: 0.7047390937805176\n",
      "epoch: 25, batch number: 1479, loss: 0.6148509979248047\n",
      "epoch: 25, batch number: 1480, loss: 0.570919394493103\n",
      "epoch: 25, batch number: 1481, loss: 0.6823029518127441\n",
      "epoch: 25, batch number: 1482, loss: 0.6441628336906433\n",
      "epoch: 25, batch number: 1483, loss: 0.6485340595245361\n",
      "epoch: 25, batch number: 1484, loss: 0.6580026149749756\n",
      "epoch: 25, batch number: 1485, loss: 0.7198424339294434\n",
      "epoch: 25, batch number: 1486, loss: 0.6433466672897339\n",
      "epoch: 25, batch number: 1487, loss: 0.6371671557426453\n",
      "epoch: 25, batch number: 1488, loss: 0.8658186197280884\n",
      "epoch: 25, batch number: 1489, loss: 0.5815765261650085\n",
      "epoch: 25, batch number: 1490, loss: 0.6146484017372131\n",
      "epoch: 25, batch number: 1491, loss: 0.9003933072090149\n",
      "epoch: 25, batch number: 1492, loss: 0.5655026435852051\n",
      "epoch: 25, batch number: 1493, loss: 0.6046916842460632\n",
      "epoch: 25, batch number: 1494, loss: 0.590937614440918\n",
      "epoch: 25, batch number: 1495, loss: 0.56501704454422\n",
      "epoch: 25, batch number: 1496, loss: 0.661984920501709\n",
      "epoch: 25, batch number: 1497, loss: 0.5844082236289978\n",
      "epoch: 25, batch number: 1498, loss: 0.5403165221214294\n",
      "epoch: 25, batch number: 1499, loss: 0.5695661902427673\n",
      "epoch: 25, batch number: 1500, loss: 1.0048880577087402\n",
      "epoch: 25, batch number: 1501, loss: 0.8092146515846252\n",
      "epoch: 25, batch number: 1502, loss: 0.5383325815200806\n",
      "epoch: 25, batch number: 1503, loss: 0.6257409453392029\n",
      "epoch: 25, batch number: 1504, loss: 0.5312209725379944\n",
      "epoch: 25, batch number: 1505, loss: 0.6199183464050293\n",
      "epoch: 25, batch number: 1506, loss: 0.5648103356361389\n",
      "epoch: 25, batch number: 1507, loss: 0.5707265138626099\n",
      "epoch: 25, batch number: 1508, loss: 0.64650559425354\n",
      "epoch: 25, batch number: 1509, loss: 0.6400336623191833\n",
      "epoch: 25, batch number: 1510, loss: 0.6383474469184875\n",
      "epoch: 25, batch number: 1511, loss: 0.725599467754364\n",
      "epoch: 25, batch number: 1512, loss: 0.5871605277061462\n",
      "epoch: 25, batch number: 1513, loss: 0.5492867231369019\n",
      "epoch: 26, batch number: 1514, loss: 0.6193141341209412\n",
      "epoch: 26, batch number: 1515, loss: 1.041980504989624\n",
      "epoch: 26, batch number: 1516, loss: 0.6644489169120789\n",
      "epoch: 26, batch number: 1517, loss: 0.8030280470848083\n",
      "epoch: 26, batch number: 1518, loss: 0.9800372123718262\n",
      "epoch: 26, batch number: 1519, loss: 1.0160449743270874\n",
      "epoch: 26, batch number: 1520, loss: 0.8821150660514832\n",
      "epoch: 26, batch number: 1521, loss: 0.8187605738639832\n",
      "epoch: 26, batch number: 1522, loss: 1.5758259296417236\n",
      "epoch: 26, batch number: 1523, loss: 0.988777220249176\n",
      "epoch: 26, batch number: 1524, loss: 0.8076756000518799\n",
      "epoch: 26, batch number: 1525, loss: 0.7641290426254272\n",
      "epoch: 26, batch number: 1526, loss: 1.244192361831665\n",
      "epoch: 26, batch number: 1527, loss: 0.7005876898765564\n",
      "epoch: 26, batch number: 1528, loss: 1.2137377262115479\n",
      "epoch: 26, batch number: 1529, loss: 0.5952682495117188\n",
      "epoch: 26, batch number: 1530, loss: 1.1463942527770996\n",
      "epoch: 26, batch number: 1531, loss: 0.6157938241958618\n",
      "epoch: 26, batch number: 1532, loss: 1.042264699935913\n",
      "epoch: 26, batch number: 1533, loss: 1.0175678730010986\n",
      "epoch: 26, batch number: 1534, loss: 1.187058448791504\n",
      "epoch: 26, batch number: 1535, loss: 0.934520423412323\n",
      "epoch: 26, batch number: 1536, loss: 1.0170563459396362\n",
      "epoch: 26, batch number: 1537, loss: 0.8769595623016357\n",
      "epoch: 26, batch number: 1538, loss: 0.8109528422355652\n",
      "epoch: 26, batch number: 1539, loss: 1.1382298469543457\n",
      "epoch: 26, batch number: 1540, loss: 0.9912480115890503\n",
      "epoch: 26, batch number: 1541, loss: 1.1942685842514038\n",
      "epoch: 26, batch number: 1542, loss: 1.4516388177871704\n",
      "epoch: 26, batch number: 1543, loss: 1.0171960592269897\n",
      "epoch: 26, batch number: 1544, loss: 0.7880982756614685\n",
      "epoch: 26, batch number: 1545, loss: 0.9292011260986328\n",
      "epoch: 26, batch number: 1546, loss: 1.2048460245132446\n",
      "epoch: 26, batch number: 1547, loss: 1.578684687614441\n",
      "epoch: 26, batch number: 1548, loss: 0.7684317827224731\n",
      "epoch: 26, batch number: 1549, loss: 1.1934188604354858\n",
      "epoch: 26, batch number: 1550, loss: 0.778399646282196\n",
      "epoch: 26, batch number: 1551, loss: 0.8058695793151855\n",
      "epoch: 26, batch number: 1552, loss: 0.5941462516784668\n",
      "epoch: 26, batch number: 1553, loss: 0.8412489891052246\n",
      "epoch: 26, batch number: 1554, loss: 1.1609801054000854\n",
      "epoch: 26, batch number: 1555, loss: 1.0013554096221924\n",
      "epoch: 26, batch number: 1556, loss: 0.7876490950584412\n",
      "epoch: 26, batch number: 1557, loss: 0.7430579662322998\n",
      "epoch: 26, batch number: 1558, loss: 0.73560631275177\n",
      "epoch: 26, batch number: 1559, loss: 0.8096172213554382\n",
      "epoch: 26, batch number: 1560, loss: 1.0164024829864502\n",
      "epoch: 26, batch number: 1561, loss: 0.8288393020629883\n",
      "epoch: 26, batch number: 1562, loss: 0.8745534420013428\n",
      "epoch: 26, batch number: 1563, loss: 0.589681088924408\n",
      "epoch: 26, batch number: 1564, loss: 0.7945299744606018\n",
      "epoch: 26, batch number: 1565, loss: 0.843028724193573\n",
      "epoch: 26, batch number: 1566, loss: 0.7977608442306519\n",
      "epoch: 26, batch number: 1567, loss: 0.767631471157074\n",
      "epoch: 26, batch number: 1568, loss: 0.8612754940986633\n",
      "epoch: 26, batch number: 1569, loss: 1.1287868022918701\n",
      "epoch: 26, batch number: 1570, loss: 0.6476032137870789\n",
      "epoch: 26, batch number: 1571, loss: 1.0132396221160889\n",
      "epoch: 26, batch number: 1572, loss: 0.9538445472717285\n",
      "epoch: 26, batch number: 1573, loss: 1.9374204874038696\n",
      "epoch: 26, batch number: 1574, loss: 1.1079988479614258\n",
      "epoch: 26, batch number: 1575, loss: 0.5935457348823547\n",
      "epoch: 26, batch number: 1576, loss: 0.5072821974754333\n",
      "epoch: 27, batch number: 1577, loss: 0.7449833750724792\n",
      "epoch: 27, batch number: 1578, loss: 0.9068869352340698\n",
      "epoch: 27, batch number: 1579, loss: 0.6817406415939331\n",
      "epoch: 27, batch number: 1580, loss: 0.7234383225440979\n",
      "epoch: 27, batch number: 1581, loss: 1.2819499969482422\n",
      "epoch: 27, batch number: 1582, loss: 1.1049599647521973\n",
      "epoch: 27, batch number: 1583, loss: 0.7502815127372742\n",
      "epoch: 27, batch number: 1584, loss: 0.7574605941772461\n",
      "epoch: 27, batch number: 1585, loss: 0.5434587597846985\n",
      "epoch: 27, batch number: 1586, loss: 0.7969536185264587\n",
      "epoch: 27, batch number: 1587, loss: 0.588074266910553\n",
      "epoch: 27, batch number: 1588, loss: 0.5475443005561829\n",
      "epoch: 27, batch number: 1589, loss: 0.8927294611930847\n",
      "epoch: 27, batch number: 1590, loss: 0.7196124792098999\n",
      "epoch: 27, batch number: 1591, loss: 0.536594569683075\n",
      "epoch: 27, batch number: 1592, loss: 0.6553269028663635\n",
      "epoch: 27, batch number: 1593, loss: 0.5545313358306885\n",
      "epoch: 27, batch number: 1594, loss: 0.8295367360115051\n",
      "epoch: 27, batch number: 1595, loss: 0.8965792655944824\n",
      "epoch: 27, batch number: 1596, loss: 1.236395001411438\n",
      "epoch: 27, batch number: 1597, loss: 0.7671092748641968\n",
      "epoch: 27, batch number: 1598, loss: 0.5986936688423157\n",
      "epoch: 27, batch number: 1599, loss: 0.7182885408401489\n",
      "epoch: 27, batch number: 1600, loss: 0.7715563178062439\n",
      "epoch: 27, batch number: 1601, loss: 0.5787673592567444\n",
      "epoch: 27, batch number: 1602, loss: 0.6565009951591492\n",
      "epoch: 27, batch number: 1603, loss: 0.8919366002082825\n",
      "epoch: 27, batch number: 1604, loss: 1.0141029357910156\n",
      "epoch: 27, batch number: 1605, loss: 0.9401611089706421\n",
      "epoch: 27, batch number: 1606, loss: 0.5720898509025574\n",
      "epoch: 27, batch number: 1607, loss: 0.749511182308197\n",
      "epoch: 27, batch number: 1608, loss: 0.5825716853141785\n",
      "epoch: 27, batch number: 1609, loss: 0.708463728427887\n",
      "epoch: 27, batch number: 1610, loss: 0.7127597332000732\n",
      "epoch: 27, batch number: 1611, loss: 0.9220199584960938\n",
      "epoch: 27, batch number: 1612, loss: 0.57547527551651\n",
      "epoch: 27, batch number: 1613, loss: 0.5483990907669067\n",
      "epoch: 27, batch number: 1614, loss: 0.6816122531890869\n",
      "epoch: 27, batch number: 1615, loss: 0.5832160711288452\n",
      "epoch: 27, batch number: 1616, loss: 0.9174718260765076\n",
      "epoch: 27, batch number: 1617, loss: 0.768277645111084\n",
      "epoch: 27, batch number: 1618, loss: 0.7164153456687927\n",
      "epoch: 27, batch number: 1619, loss: 0.697589635848999\n",
      "epoch: 27, batch number: 1620, loss: 0.7556614875793457\n",
      "epoch: 27, batch number: 1621, loss: 0.5232979655265808\n",
      "epoch: 27, batch number: 1622, loss: 0.7276238799095154\n",
      "epoch: 27, batch number: 1623, loss: 0.7295257449150085\n",
      "epoch: 27, batch number: 1624, loss: 0.5781443119049072\n",
      "epoch: 27, batch number: 1625, loss: 0.5981324911117554\n",
      "epoch: 27, batch number: 1626, loss: 0.9067168831825256\n",
      "epoch: 27, batch number: 1627, loss: 0.604371190071106\n",
      "epoch: 27, batch number: 1628, loss: 0.8237553238868713\n",
      "epoch: 27, batch number: 1629, loss: 0.5314023494720459\n",
      "epoch: 27, batch number: 1630, loss: 0.7038610577583313\n",
      "epoch: 27, batch number: 1631, loss: 1.1931052207946777\n",
      "epoch: 27, batch number: 1632, loss: 1.0954155921936035\n",
      "epoch: 27, batch number: 1633, loss: 1.1050244569778442\n",
      "epoch: 27, batch number: 1634, loss: 0.609284520149231\n",
      "epoch: 27, batch number: 1635, loss: 0.5580650568008423\n",
      "epoch: 27, batch number: 1636, loss: 0.9037190079689026\n",
      "epoch: 27, batch number: 1637, loss: 0.813220202922821\n",
      "epoch: 27, batch number: 1638, loss: 0.9249678254127502\n",
      "epoch: 27, batch number: 1639, loss: 0.4521505534648895\n",
      "epoch: 28, batch number: 1640, loss: 1.07297682762146\n",
      "epoch: 28, batch number: 1641, loss: 0.7382248640060425\n",
      "epoch: 28, batch number: 1642, loss: 0.6265069246292114\n",
      "epoch: 28, batch number: 1643, loss: 1.4159694910049438\n",
      "epoch: 28, batch number: 1644, loss: 0.8007658123970032\n",
      "epoch: 28, batch number: 1645, loss: 1.2390329837799072\n",
      "epoch: 28, batch number: 1646, loss: 0.7674869894981384\n",
      "epoch: 28, batch number: 1647, loss: 1.0520298480987549\n",
      "epoch: 28, batch number: 1648, loss: 1.0188310146331787\n",
      "epoch: 28, batch number: 1649, loss: 0.9954821467399597\n",
      "epoch: 28, batch number: 1650, loss: 0.7292345762252808\n",
      "epoch: 28, batch number: 1651, loss: 0.7387270927429199\n",
      "epoch: 28, batch number: 1652, loss: 0.7605881690979004\n",
      "epoch: 28, batch number: 1653, loss: 0.733170211315155\n",
      "epoch: 28, batch number: 1654, loss: 0.8139980435371399\n",
      "epoch: 28, batch number: 1655, loss: 0.7669571042060852\n",
      "epoch: 28, batch number: 1656, loss: 1.1818510293960571\n",
      "epoch: 28, batch number: 1657, loss: 0.7089816331863403\n",
      "epoch: 28, batch number: 1658, loss: 0.8366043567657471\n",
      "epoch: 28, batch number: 1659, loss: 0.8745794296264648\n",
      "epoch: 28, batch number: 1660, loss: 1.1482614278793335\n",
      "epoch: 28, batch number: 1661, loss: 1.0161442756652832\n",
      "epoch: 28, batch number: 1662, loss: 0.7040727138519287\n",
      "epoch: 28, batch number: 1663, loss: 0.7960898876190186\n",
      "epoch: 28, batch number: 1664, loss: 0.7194034457206726\n",
      "epoch: 28, batch number: 1665, loss: 0.7875595092773438\n",
      "epoch: 28, batch number: 1666, loss: 0.7052923440933228\n",
      "epoch: 28, batch number: 1667, loss: 0.821086585521698\n",
      "epoch: 28, batch number: 1668, loss: 0.6789268851280212\n",
      "epoch: 28, batch number: 1669, loss: 0.9011982083320618\n",
      "epoch: 28, batch number: 1670, loss: 0.6887069940567017\n",
      "epoch: 28, batch number: 1671, loss: 0.7715384364128113\n",
      "epoch: 28, batch number: 1672, loss: 0.9267135858535767\n",
      "epoch: 28, batch number: 1673, loss: 0.6748750805854797\n",
      "epoch: 28, batch number: 1674, loss: 1.0538841485977173\n",
      "epoch: 28, batch number: 1675, loss: 0.705996036529541\n",
      "epoch: 28, batch number: 1676, loss: 1.4729365110397339\n",
      "epoch: 28, batch number: 1677, loss: 0.7868650555610657\n",
      "epoch: 28, batch number: 1678, loss: 0.7211860418319702\n",
      "epoch: 28, batch number: 1679, loss: 0.5967010855674744\n",
      "epoch: 28, batch number: 1680, loss: 0.8850340843200684\n",
      "epoch: 28, batch number: 1681, loss: 0.790062427520752\n",
      "epoch: 28, batch number: 1682, loss: 0.7373320460319519\n",
      "epoch: 28, batch number: 1683, loss: 0.7132136225700378\n",
      "epoch: 28, batch number: 1684, loss: 0.6447852849960327\n",
      "epoch: 28, batch number: 1685, loss: 0.739319384098053\n",
      "epoch: 28, batch number: 1686, loss: 0.5650132298469543\n",
      "epoch: 28, batch number: 1687, loss: 0.6282020211219788\n",
      "epoch: 28, batch number: 1688, loss: 0.797292172908783\n",
      "epoch: 28, batch number: 1689, loss: 0.8328869342803955\n",
      "epoch: 28, batch number: 1690, loss: 1.245761513710022\n",
      "epoch: 28, batch number: 1691, loss: 0.7305350303649902\n",
      "epoch: 28, batch number: 1692, loss: 0.7021496891975403\n",
      "epoch: 28, batch number: 1693, loss: 0.8329560160636902\n",
      "epoch: 28, batch number: 1694, loss: 0.9558334946632385\n",
      "epoch: 28, batch number: 1695, loss: 1.0822889804840088\n",
      "epoch: 28, batch number: 1696, loss: 0.9938527941703796\n",
      "epoch: 28, batch number: 1697, loss: 0.8158614635467529\n",
      "epoch: 28, batch number: 1698, loss: 1.030828833580017\n",
      "epoch: 28, batch number: 1699, loss: 0.7218579649925232\n",
      "epoch: 28, batch number: 1700, loss: 1.119171380996704\n",
      "epoch: 28, batch number: 1701, loss: 0.8151139616966248\n",
      "epoch: 28, batch number: 1702, loss: 1.1046339273452759\n",
      "epoch: 29, batch number: 1703, loss: 2.1694939136505127\n",
      "epoch: 29, batch number: 1704, loss: 1.5290437936782837\n",
      "epoch: 29, batch number: 1705, loss: 1.9233570098876953\n",
      "epoch: 29, batch number: 1706, loss: 1.8806980848312378\n",
      "epoch: 29, batch number: 1707, loss: 2.035703420639038\n",
      "epoch: 29, batch number: 1708, loss: 1.7350945472717285\n",
      "epoch: 29, batch number: 1709, loss: 2.282423734664917\n",
      "epoch: 29, batch number: 1710, loss: 2.2803874015808105\n",
      "epoch: 29, batch number: 1711, loss: 1.7810159921646118\n",
      "epoch: 29, batch number: 1712, loss: 2.0145161151885986\n",
      "epoch: 29, batch number: 1713, loss: 1.839788794517517\n",
      "epoch: 29, batch number: 1714, loss: 2.299703598022461\n",
      "epoch: 29, batch number: 1715, loss: 1.924922227859497\n",
      "epoch: 29, batch number: 1716, loss: 1.6482815742492676\n",
      "epoch: 29, batch number: 1717, loss: 1.6726516485214233\n",
      "epoch: 29, batch number: 1718, loss: 2.3204548358917236\n",
      "epoch: 29, batch number: 1719, loss: 2.3130598068237305\n",
      "epoch: 29, batch number: 1720, loss: 1.7496471405029297\n",
      "epoch: 29, batch number: 1721, loss: 1.7099676132202148\n",
      "epoch: 29, batch number: 1722, loss: 1.3261903524398804\n",
      "epoch: 29, batch number: 1723, loss: 1.7491143941879272\n",
      "epoch: 29, batch number: 1724, loss: 1.5813440084457397\n",
      "epoch: 29, batch number: 1725, loss: 1.3857165575027466\n",
      "epoch: 29, batch number: 1726, loss: 1.7921268939971924\n",
      "epoch: 29, batch number: 1727, loss: 1.548809289932251\n",
      "epoch: 29, batch number: 1728, loss: 1.7634798288345337\n",
      "epoch: 29, batch number: 1729, loss: 1.386757493019104\n",
      "epoch: 29, batch number: 1730, loss: 1.4618210792541504\n",
      "epoch: 29, batch number: 1731, loss: 1.8429145812988281\n",
      "epoch: 29, batch number: 1732, loss: 1.166099190711975\n",
      "epoch: 29, batch number: 1733, loss: 1.3574284315109253\n",
      "epoch: 29, batch number: 1734, loss: 1.3820898532867432\n",
      "epoch: 29, batch number: 1735, loss: 1.4346731901168823\n",
      "epoch: 29, batch number: 1736, loss: 1.3276342153549194\n",
      "epoch: 29, batch number: 1737, loss: 1.5828174352645874\n",
      "epoch: 29, batch number: 1738, loss: 1.500929355621338\n",
      "epoch: 29, batch number: 1739, loss: 1.303465723991394\n",
      "epoch: 29, batch number: 1740, loss: 1.290419340133667\n",
      "epoch: 29, batch number: 1741, loss: 1.1457746028900146\n",
      "epoch: 29, batch number: 1742, loss: 1.1436210870742798\n",
      "epoch: 29, batch number: 1743, loss: 1.3178156614303589\n",
      "epoch: 29, batch number: 1744, loss: 1.2694220542907715\n",
      "epoch: 29, batch number: 1745, loss: 1.2541959285736084\n",
      "epoch: 29, batch number: 1746, loss: 1.178018569946289\n",
      "epoch: 29, batch number: 1747, loss: 1.1679469347000122\n",
      "epoch: 29, batch number: 1748, loss: 1.1605967283248901\n",
      "epoch: 29, batch number: 1749, loss: 1.2077138423919678\n",
      "epoch: 29, batch number: 1750, loss: 1.1335078477859497\n",
      "epoch: 29, batch number: 1751, loss: 1.0633553266525269\n",
      "epoch: 29, batch number: 1752, loss: 1.0574116706848145\n",
      "epoch: 29, batch number: 1753, loss: 0.9937266111373901\n",
      "epoch: 29, batch number: 1754, loss: 1.037766695022583\n",
      "epoch: 29, batch number: 1755, loss: 1.1164844036102295\n",
      "epoch: 29, batch number: 1756, loss: 1.1687657833099365\n",
      "epoch: 29, batch number: 1757, loss: 1.075493335723877\n",
      "epoch: 29, batch number: 1758, loss: 1.1113306283950806\n",
      "epoch: 29, batch number: 1759, loss: 1.0667470693588257\n",
      "epoch: 29, batch number: 1760, loss: 1.0203213691711426\n",
      "epoch: 29, batch number: 1761, loss: 1.079981803894043\n",
      "epoch: 29, batch number: 1762, loss: 1.0657196044921875\n",
      "epoch: 29, batch number: 1763, loss: 1.0721654891967773\n",
      "epoch: 29, batch number: 1764, loss: 1.044508934020996\n",
      "epoch: 29, batch number: 1765, loss: 1.1518696546554565\n",
      "epoch: 30, batch number: 1766, loss: 1.1832451820373535\n",
      "epoch: 30, batch number: 1767, loss: 1.1501277685165405\n",
      "epoch: 30, batch number: 1768, loss: 1.0502912998199463\n",
      "epoch: 30, batch number: 1769, loss: 1.2064582109451294\n",
      "epoch: 30, batch number: 1770, loss: 1.0938324928283691\n",
      "epoch: 30, batch number: 1771, loss: 0.9263904094696045\n",
      "epoch: 30, batch number: 1772, loss: 1.1335681676864624\n",
      "epoch: 30, batch number: 1773, loss: 1.0497123003005981\n",
      "epoch: 30, batch number: 1774, loss: 1.1965391635894775\n",
      "epoch: 30, batch number: 1775, loss: 0.9764773845672607\n",
      "epoch: 30, batch number: 1776, loss: 1.0508687496185303\n",
      "epoch: 30, batch number: 1777, loss: 1.0218441486358643\n",
      "epoch: 30, batch number: 1778, loss: 1.0043318271636963\n",
      "epoch: 30, batch number: 1779, loss: 1.0474992990493774\n",
      "epoch: 30, batch number: 1780, loss: 1.0114507675170898\n",
      "epoch: 30, batch number: 1781, loss: 1.0176624059677124\n",
      "epoch: 30, batch number: 1782, loss: 0.9800015687942505\n",
      "epoch: 30, batch number: 1783, loss: 0.966826856136322\n",
      "epoch: 30, batch number: 1784, loss: 1.0132614374160767\n",
      "epoch: 30, batch number: 1785, loss: 1.1320973634719849\n",
      "epoch: 30, batch number: 1786, loss: 1.0392847061157227\n",
      "epoch: 30, batch number: 1787, loss: 1.0858569145202637\n",
      "epoch: 30, batch number: 1788, loss: 1.1440485715866089\n",
      "epoch: 30, batch number: 1789, loss: 1.0072462558746338\n",
      "epoch: 30, batch number: 1790, loss: 0.9948065876960754\n",
      "epoch: 30, batch number: 1791, loss: 1.1197946071624756\n",
      "epoch: 30, batch number: 1792, loss: 1.072611689567566\n",
      "epoch: 30, batch number: 1793, loss: 1.01689612865448\n",
      "epoch: 30, batch number: 1794, loss: 1.1291249990463257\n",
      "epoch: 30, batch number: 1795, loss: 0.9387537837028503\n",
      "epoch: 30, batch number: 1796, loss: 1.0746181011199951\n",
      "epoch: 30, batch number: 1797, loss: 1.187465786933899\n",
      "epoch: 30, batch number: 1798, loss: 1.0718872547149658\n",
      "epoch: 30, batch number: 1799, loss: 1.0942822694778442\n",
      "epoch: 30, batch number: 1800, loss: 1.0332130193710327\n",
      "epoch: 30, batch number: 1801, loss: 1.152393102645874\n",
      "epoch: 30, batch number: 1802, loss: 0.9909675717353821\n",
      "epoch: 30, batch number: 1803, loss: 1.069904088973999\n",
      "epoch: 30, batch number: 1804, loss: 0.975145697593689\n",
      "epoch: 30, batch number: 1805, loss: 0.9514604210853577\n",
      "epoch: 30, batch number: 1806, loss: 1.0087635517120361\n",
      "epoch: 30, batch number: 1807, loss: 1.0953738689422607\n",
      "epoch: 30, batch number: 1808, loss: 0.9261196255683899\n",
      "epoch: 30, batch number: 1809, loss: 1.024268627166748\n",
      "epoch: 30, batch number: 1810, loss: 1.0318849086761475\n",
      "epoch: 30, batch number: 1811, loss: 1.0298432111740112\n",
      "epoch: 30, batch number: 1812, loss: 1.0043059587478638\n",
      "epoch: 30, batch number: 1813, loss: 1.0604143142700195\n",
      "epoch: 30, batch number: 1814, loss: 0.9978283047676086\n",
      "epoch: 30, batch number: 1815, loss: 1.0958890914916992\n",
      "epoch: 30, batch number: 1816, loss: 0.9663228392601013\n",
      "epoch: 30, batch number: 1817, loss: 1.047402024269104\n",
      "epoch: 30, batch number: 1818, loss: 1.0370278358459473\n",
      "epoch: 30, batch number: 1819, loss: 0.9680283665657043\n",
      "epoch: 30, batch number: 1820, loss: 1.0893818140029907\n",
      "epoch: 30, batch number: 1821, loss: 1.0182790756225586\n",
      "epoch: 30, batch number: 1822, loss: 1.0184944868087769\n",
      "epoch: 30, batch number: 1823, loss: 1.1105999946594238\n",
      "epoch: 30, batch number: 1824, loss: 1.0318979024887085\n",
      "epoch: 30, batch number: 1825, loss: 1.1318693161010742\n",
      "epoch: 30, batch number: 1826, loss: 1.0672703981399536\n",
      "epoch: 30, batch number: 1827, loss: 0.9950183629989624\n",
      "epoch: 30, batch number: 1828, loss: 1.059415340423584\n",
      "epoch: 31, batch number: 1829, loss: 1.1271357536315918\n",
      "epoch: 31, batch number: 1830, loss: 1.059209942817688\n",
      "epoch: 31, batch number: 1831, loss: 0.950474202632904\n",
      "epoch: 31, batch number: 1832, loss: 1.0880200862884521\n",
      "epoch: 31, batch number: 1833, loss: 1.028139591217041\n",
      "epoch: 31, batch number: 1834, loss: 1.0326368808746338\n",
      "epoch: 31, batch number: 1835, loss: 0.9876797199249268\n",
      "epoch: 31, batch number: 1836, loss: 0.9451243877410889\n",
      "epoch: 31, batch number: 1837, loss: 1.1447819471359253\n",
      "epoch: 31, batch number: 1838, loss: 1.0292296409606934\n",
      "epoch: 31, batch number: 1839, loss: 1.0170297622680664\n",
      "epoch: 31, batch number: 1840, loss: 1.001627802848816\n",
      "epoch: 31, batch number: 1841, loss: 1.1189286708831787\n",
      "epoch: 31, batch number: 1842, loss: 0.9910442233085632\n",
      "epoch: 31, batch number: 1843, loss: 0.94549959897995\n",
      "epoch: 31, batch number: 1844, loss: 1.018750786781311\n",
      "epoch: 31, batch number: 1845, loss: 0.9619516134262085\n",
      "epoch: 31, batch number: 1846, loss: 0.9539551138877869\n",
      "epoch: 31, batch number: 1847, loss: 1.097672939300537\n",
      "epoch: 31, batch number: 1848, loss: 0.9705877304077148\n",
      "epoch: 31, batch number: 1849, loss: 1.0233248472213745\n",
      "epoch: 31, batch number: 1850, loss: 1.1242367029190063\n",
      "epoch: 31, batch number: 1851, loss: 0.94364994764328\n",
      "epoch: 31, batch number: 1852, loss: 1.0865665674209595\n",
      "epoch: 31, batch number: 1853, loss: 0.9674424529075623\n",
      "epoch: 31, batch number: 1854, loss: 0.9990297555923462\n",
      "epoch: 31, batch number: 1855, loss: 1.0519686937332153\n",
      "epoch: 31, batch number: 1856, loss: 0.9186393618583679\n",
      "epoch: 31, batch number: 1857, loss: 0.948695719242096\n",
      "epoch: 31, batch number: 1858, loss: 0.88689786195755\n",
      "epoch: 31, batch number: 1859, loss: 0.8621692061424255\n",
      "epoch: 31, batch number: 1860, loss: 0.9749236106872559\n",
      "epoch: 31, batch number: 1861, loss: 1.0085216760635376\n",
      "epoch: 31, batch number: 1862, loss: 0.9825318455696106\n",
      "epoch: 31, batch number: 1863, loss: 1.0106216669082642\n",
      "epoch: 31, batch number: 1864, loss: 0.7906294465065002\n",
      "epoch: 31, batch number: 1865, loss: 0.9219935536384583\n",
      "epoch: 31, batch number: 1866, loss: 0.8393640518188477\n",
      "epoch: 31, batch number: 1867, loss: 1.001463770866394\n",
      "epoch: 31, batch number: 1868, loss: 1.0392671823501587\n",
      "epoch: 31, batch number: 1869, loss: 1.017673373222351\n",
      "epoch: 31, batch number: 1870, loss: 0.7984943985939026\n",
      "epoch: 31, batch number: 1871, loss: 0.8677272796630859\n",
      "epoch: 31, batch number: 1872, loss: 0.9776595234870911\n",
      "epoch: 31, batch number: 1873, loss: 0.911751389503479\n",
      "epoch: 31, batch number: 1874, loss: 0.9494615793228149\n",
      "epoch: 31, batch number: 1875, loss: 0.8207652568817139\n",
      "epoch: 31, batch number: 1876, loss: 1.0497430562973022\n",
      "epoch: 31, batch number: 1877, loss: 0.9976897239685059\n",
      "epoch: 31, batch number: 1878, loss: 0.8917214870452881\n",
      "epoch: 31, batch number: 1879, loss: 0.9410665035247803\n",
      "epoch: 31, batch number: 1880, loss: 1.1195528507232666\n",
      "epoch: 31, batch number: 1881, loss: 0.9272525310516357\n",
      "epoch: 31, batch number: 1882, loss: 0.922453761100769\n",
      "epoch: 31, batch number: 1883, loss: 1.0092562437057495\n",
      "epoch: 31, batch number: 1884, loss: 0.9907386302947998\n",
      "epoch: 31, batch number: 1885, loss: 1.0766748189926147\n",
      "epoch: 31, batch number: 1886, loss: 0.962314248085022\n",
      "epoch: 31, batch number: 1887, loss: 0.883776068687439\n",
      "epoch: 31, batch number: 1888, loss: 0.931365966796875\n",
      "epoch: 31, batch number: 1889, loss: 1.037617802619934\n",
      "epoch: 31, batch number: 1890, loss: 0.8551623821258545\n",
      "epoch: 31, batch number: 1891, loss: 0.8963174819946289\n",
      "epoch: 32, batch number: 1892, loss: 1.1436631679534912\n",
      "epoch: 32, batch number: 1893, loss: 0.7760149836540222\n",
      "epoch: 32, batch number: 1894, loss: 0.7751210331916809\n",
      "epoch: 32, batch number: 1895, loss: 0.8522496223449707\n",
      "epoch: 32, batch number: 1896, loss: 0.8791636228561401\n",
      "epoch: 32, batch number: 1897, loss: 0.8447762727737427\n",
      "epoch: 32, batch number: 1898, loss: 0.9421939253807068\n",
      "epoch: 32, batch number: 1899, loss: 0.8579980731010437\n",
      "epoch: 32, batch number: 1900, loss: 0.7622137069702148\n",
      "epoch: 32, batch number: 1901, loss: 0.9125868082046509\n",
      "epoch: 32, batch number: 1902, loss: 0.7550795674324036\n",
      "epoch: 32, batch number: 1903, loss: 0.7356272339820862\n",
      "epoch: 32, batch number: 1904, loss: 0.905391275882721\n",
      "epoch: 32, batch number: 1905, loss: 0.9659989476203918\n",
      "epoch: 32, batch number: 1906, loss: 0.9554373025894165\n",
      "epoch: 32, batch number: 1907, loss: 0.8483463525772095\n",
      "epoch: 32, batch number: 1908, loss: 0.8175033926963806\n",
      "epoch: 32, batch number: 1909, loss: 0.9948193430900574\n",
      "epoch: 32, batch number: 1910, loss: 0.7455976605415344\n",
      "epoch: 32, batch number: 1911, loss: 0.6420283913612366\n",
      "epoch: 32, batch number: 1912, loss: 0.7724489569664001\n",
      "epoch: 32, batch number: 1913, loss: 0.7395525574684143\n",
      "epoch: 32, batch number: 1914, loss: 0.7292571663856506\n",
      "epoch: 32, batch number: 1915, loss: 0.74098140001297\n",
      "epoch: 32, batch number: 1916, loss: 0.967240571975708\n",
      "epoch: 32, batch number: 1917, loss: 0.9011975526809692\n",
      "epoch: 32, batch number: 1918, loss: 0.7662999033927917\n",
      "epoch: 32, batch number: 1919, loss: 0.9120317101478577\n",
      "epoch: 32, batch number: 1920, loss: 0.8178059458732605\n",
      "epoch: 32, batch number: 1921, loss: 0.9177849888801575\n",
      "epoch: 32, batch number: 1922, loss: 0.7710402011871338\n",
      "epoch: 32, batch number: 1923, loss: 0.7693755626678467\n",
      "epoch: 32, batch number: 1924, loss: 1.101719617843628\n",
      "epoch: 32, batch number: 1925, loss: 0.7787325978279114\n",
      "epoch: 32, batch number: 1926, loss: 1.0994775295257568\n",
      "epoch: 32, batch number: 1927, loss: 0.8603686690330505\n",
      "epoch: 32, batch number: 1928, loss: 0.8162257075309753\n",
      "epoch: 32, batch number: 1929, loss: 0.9298540353775024\n",
      "epoch: 32, batch number: 1930, loss: 0.7850075364112854\n",
      "epoch: 32, batch number: 1931, loss: 1.062309980392456\n",
      "epoch: 32, batch number: 1932, loss: 0.9699812531471252\n",
      "epoch: 32, batch number: 1933, loss: 0.9478362798690796\n",
      "epoch: 32, batch number: 1934, loss: 0.8652946949005127\n",
      "epoch: 32, batch number: 1935, loss: 0.8741437792778015\n",
      "epoch: 32, batch number: 1936, loss: 0.8077207207679749\n",
      "epoch: 32, batch number: 1937, loss: 0.6808300018310547\n",
      "epoch: 32, batch number: 1938, loss: 0.9401143789291382\n",
      "epoch: 32, batch number: 1939, loss: 0.6723837852478027\n",
      "epoch: 32, batch number: 1940, loss: 1.0225905179977417\n",
      "epoch: 32, batch number: 1941, loss: 0.7978663444519043\n",
      "epoch: 32, batch number: 1942, loss: 0.7014676928520203\n",
      "epoch: 32, batch number: 1943, loss: 1.0162806510925293\n",
      "epoch: 32, batch number: 1944, loss: 0.8775209188461304\n",
      "epoch: 32, batch number: 1945, loss: 0.8171300888061523\n",
      "epoch: 32, batch number: 1946, loss: 0.7309670448303223\n",
      "epoch: 32, batch number: 1947, loss: 0.8514403104782104\n",
      "epoch: 32, batch number: 1948, loss: 1.0456892251968384\n",
      "epoch: 32, batch number: 1949, loss: 0.9574148058891296\n",
      "epoch: 32, batch number: 1950, loss: 0.940321147441864\n",
      "epoch: 32, batch number: 1951, loss: 0.9764190912246704\n",
      "epoch: 32, batch number: 1952, loss: 0.9174095988273621\n",
      "epoch: 32, batch number: 1953, loss: 0.7609401345252991\n",
      "epoch: 32, batch number: 1954, loss: 0.8029451966285706\n",
      "epoch: 33, batch number: 1955, loss: 0.9719748497009277\n",
      "epoch: 33, batch number: 1956, loss: 1.4699044227600098\n",
      "epoch: 33, batch number: 1957, loss: 1.158063292503357\n",
      "epoch: 33, batch number: 1958, loss: 1.134899377822876\n",
      "epoch: 33, batch number: 1959, loss: 1.061176061630249\n",
      "epoch: 33, batch number: 1960, loss: 1.3231163024902344\n",
      "epoch: 33, batch number: 1961, loss: 0.9838206171989441\n",
      "epoch: 33, batch number: 1962, loss: 1.3575531244277954\n",
      "epoch: 33, batch number: 1963, loss: 0.8956456780433655\n",
      "epoch: 33, batch number: 1964, loss: 1.3394914865493774\n",
      "epoch: 33, batch number: 1965, loss: 1.1103706359863281\n",
      "epoch: 33, batch number: 1966, loss: 1.1645928621292114\n",
      "epoch: 33, batch number: 1967, loss: 1.4179712533950806\n",
      "epoch: 33, batch number: 1968, loss: 1.0602121353149414\n",
      "epoch: 33, batch number: 1969, loss: 0.8167782425880432\n",
      "epoch: 33, batch number: 1970, loss: 1.0849663019180298\n",
      "epoch: 33, batch number: 1971, loss: 1.0696754455566406\n",
      "epoch: 33, batch number: 1972, loss: 1.3339684009552002\n",
      "epoch: 33, batch number: 1973, loss: 0.9920239448547363\n",
      "epoch: 33, batch number: 1974, loss: 1.0692102909088135\n",
      "epoch: 33, batch number: 1975, loss: 1.013641357421875\n",
      "epoch: 33, batch number: 1976, loss: 1.715039610862732\n",
      "epoch: 33, batch number: 1977, loss: 0.7440389394760132\n",
      "epoch: 33, batch number: 1978, loss: 1.2747281789779663\n",
      "epoch: 33, batch number: 1979, loss: 0.8301010727882385\n",
      "epoch: 33, batch number: 1980, loss: 0.9853272438049316\n",
      "epoch: 33, batch number: 1981, loss: 1.0298902988433838\n",
      "epoch: 33, batch number: 1982, loss: 1.033913016319275\n",
      "epoch: 33, batch number: 1983, loss: 1.142313003540039\n",
      "epoch: 33, batch number: 1984, loss: 0.9088537693023682\n",
      "epoch: 33, batch number: 1985, loss: 0.8544086813926697\n",
      "epoch: 33, batch number: 1986, loss: 1.1944366693496704\n",
      "epoch: 33, batch number: 1987, loss: 0.8513298630714417\n",
      "epoch: 33, batch number: 1988, loss: 0.8395557403564453\n",
      "epoch: 33, batch number: 1989, loss: 1.0262532234191895\n",
      "epoch: 33, batch number: 1990, loss: 1.144643783569336\n",
      "epoch: 33, batch number: 1991, loss: 0.9650899767875671\n",
      "epoch: 33, batch number: 1992, loss: 0.9088791608810425\n",
      "epoch: 33, batch number: 1993, loss: 1.0116876363754272\n",
      "epoch: 33, batch number: 1994, loss: 0.9097173810005188\n",
      "epoch: 33, batch number: 1995, loss: 1.160117745399475\n",
      "epoch: 33, batch number: 1996, loss: 1.2292780876159668\n",
      "epoch: 33, batch number: 1997, loss: 1.1149044036865234\n",
      "epoch: 33, batch number: 1998, loss: 0.9073674082756042\n",
      "epoch: 33, batch number: 1999, loss: 1.0110564231872559\n",
      "epoch: 33, batch number: 2000, loss: 0.9936460852622986\n",
      "epoch: 33, batch number: 2001, loss: 1.0756726264953613\n",
      "epoch: 33, batch number: 2002, loss: 1.073823094367981\n",
      "epoch: 33, batch number: 2003, loss: 1.034443736076355\n",
      "epoch: 33, batch number: 2004, loss: 1.0545408725738525\n",
      "epoch: 33, batch number: 2005, loss: 1.0349177122116089\n",
      "epoch: 33, batch number: 2006, loss: 1.120901346206665\n",
      "epoch: 33, batch number: 2007, loss: 0.7587528228759766\n",
      "epoch: 33, batch number: 2008, loss: 0.895036518573761\n",
      "epoch: 33, batch number: 2009, loss: 1.1305718421936035\n",
      "epoch: 33, batch number: 2010, loss: 0.8426132202148438\n",
      "epoch: 33, batch number: 2011, loss: 0.9140793085098267\n",
      "epoch: 33, batch number: 2012, loss: 1.0162441730499268\n",
      "epoch: 33, batch number: 2013, loss: 1.174035668373108\n",
      "epoch: 33, batch number: 2014, loss: 1.1061732769012451\n",
      "epoch: 33, batch number: 2015, loss: 0.8667119145393372\n",
      "epoch: 33, batch number: 2016, loss: 1.0220460891723633\n",
      "epoch: 33, batch number: 2017, loss: 1.5056246519088745\n",
      "epoch: 34, batch number: 2018, loss: 1.3112088441848755\n",
      "epoch: 34, batch number: 2019, loss: 1.246182918548584\n",
      "epoch: 34, batch number: 2020, loss: 1.1133642196655273\n",
      "epoch: 34, batch number: 2021, loss: 1.0451362133026123\n",
      "epoch: 34, batch number: 2022, loss: 1.2339922189712524\n",
      "epoch: 34, batch number: 2023, loss: 1.3856520652770996\n",
      "epoch: 34, batch number: 2024, loss: 1.1836918592453003\n",
      "epoch: 34, batch number: 2025, loss: 0.9346672296524048\n",
      "epoch: 34, batch number: 2026, loss: 0.876724898815155\n",
      "epoch: 34, batch number: 2027, loss: 1.234053134918213\n",
      "epoch: 34, batch number: 2028, loss: 1.4070168733596802\n",
      "epoch: 34, batch number: 2029, loss: 1.3829165697097778\n",
      "epoch: 34, batch number: 2030, loss: 0.8250166773796082\n",
      "epoch: 34, batch number: 2031, loss: 1.5089030265808105\n",
      "epoch: 34, batch number: 2032, loss: 1.2894675731658936\n",
      "epoch: 34, batch number: 2033, loss: 1.318288803100586\n",
      "epoch: 34, batch number: 2034, loss: 0.9449461102485657\n",
      "epoch: 34, batch number: 2035, loss: 1.0537456274032593\n",
      "epoch: 34, batch number: 2036, loss: 1.6044713258743286\n",
      "epoch: 34, batch number: 2037, loss: 1.2543542385101318\n",
      "epoch: 34, batch number: 2038, loss: 1.2081676721572876\n",
      "epoch: 34, batch number: 2039, loss: 0.9795805811882019\n",
      "epoch: 34, batch number: 2040, loss: 1.1308737993240356\n",
      "epoch: 34, batch number: 2041, loss: 1.0347232818603516\n",
      "epoch: 34, batch number: 2042, loss: 1.447541356086731\n",
      "epoch: 34, batch number: 2043, loss: 1.3083494901657104\n",
      "epoch: 34, batch number: 2044, loss: 1.3513346910476685\n",
      "epoch: 34, batch number: 2045, loss: 0.9124254584312439\n",
      "epoch: 34, batch number: 2046, loss: 1.3223680257797241\n",
      "epoch: 34, batch number: 2047, loss: 0.8219237327575684\n",
      "epoch: 34, batch number: 2048, loss: 0.994433581829071\n",
      "epoch: 34, batch number: 2049, loss: 1.182204246520996\n",
      "epoch: 34, batch number: 2050, loss: 1.1299467086791992\n",
      "epoch: 34, batch number: 2051, loss: 1.0647190809249878\n",
      "epoch: 34, batch number: 2052, loss: 0.9148316383361816\n",
      "epoch: 34, batch number: 2053, loss: 0.9927463531494141\n",
      "epoch: 34, batch number: 2054, loss: 1.2567387819290161\n",
      "epoch: 34, batch number: 2055, loss: 1.4872714281082153\n",
      "epoch: 34, batch number: 2056, loss: 1.3631495237350464\n",
      "epoch: 34, batch number: 2057, loss: 1.1051758527755737\n",
      "epoch: 34, batch number: 2058, loss: 0.9691988825798035\n",
      "epoch: 34, batch number: 2059, loss: 1.4343541860580444\n",
      "epoch: 34, batch number: 2060, loss: 1.309212327003479\n",
      "epoch: 34, batch number: 2061, loss: 1.0235503911972046\n",
      "epoch: 34, batch number: 2062, loss: 1.1712276935577393\n",
      "epoch: 34, batch number: 2063, loss: 0.9448782205581665\n",
      "epoch: 34, batch number: 2064, loss: 0.8031997680664062\n",
      "epoch: 34, batch number: 2065, loss: 1.3386462926864624\n",
      "epoch: 34, batch number: 2066, loss: 1.323625087738037\n",
      "epoch: 34, batch number: 2067, loss: 1.1348155736923218\n",
      "epoch: 34, batch number: 2068, loss: 1.2973839044570923\n",
      "epoch: 34, batch number: 2069, loss: 1.0976530313491821\n",
      "epoch: 34, batch number: 2070, loss: 1.2880277633666992\n",
      "epoch: 34, batch number: 2071, loss: 1.3976303339004517\n",
      "epoch: 34, batch number: 2072, loss: 1.2680367231369019\n",
      "epoch: 34, batch number: 2073, loss: 1.0616875886917114\n",
      "epoch: 34, batch number: 2074, loss: 1.146384596824646\n",
      "epoch: 34, batch number: 2075, loss: 1.0368739366531372\n",
      "epoch: 34, batch number: 2076, loss: 0.983841598033905\n",
      "epoch: 34, batch number: 2077, loss: 1.040307641029358\n",
      "epoch: 34, batch number: 2078, loss: 1.06389582157135\n",
      "epoch: 34, batch number: 2079, loss: 1.2159810066223145\n",
      "epoch: 35, batch number: 2080, loss: 0.9511585235595703\n",
      "epoch: 35, batch number: 2081, loss: 1.5390108823776245\n",
      "epoch: 35, batch number: 2082, loss: 1.0563684701919556\n",
      "epoch: 35, batch number: 2083, loss: 1.1876944303512573\n",
      "epoch: 35, batch number: 2084, loss: 0.7806806564331055\n",
      "epoch: 35, batch number: 2085, loss: 1.045440673828125\n",
      "epoch: 35, batch number: 2086, loss: 1.30337393283844\n",
      "epoch: 35, batch number: 2087, loss: 1.0945520401000977\n",
      "epoch: 35, batch number: 2088, loss: 0.9043497443199158\n",
      "epoch: 35, batch number: 2089, loss: 0.6623332500457764\n",
      "epoch: 35, batch number: 2090, loss: 1.1608084440231323\n",
      "epoch: 35, batch number: 2091, loss: 0.939466655254364\n",
      "epoch: 35, batch number: 2092, loss: 1.034348726272583\n",
      "epoch: 35, batch number: 2093, loss: 1.192266583442688\n",
      "epoch: 35, batch number: 2094, loss: 1.0976347923278809\n",
      "epoch: 35, batch number: 2095, loss: 0.8615249395370483\n",
      "epoch: 35, batch number: 2096, loss: 1.2355892658233643\n",
      "epoch: 35, batch number: 2097, loss: 0.9591382145881653\n",
      "epoch: 35, batch number: 2098, loss: 0.8483818769454956\n",
      "epoch: 35, batch number: 2099, loss: 0.9946029782295227\n",
      "epoch: 35, batch number: 2100, loss: 0.8808946013450623\n",
      "epoch: 35, batch number: 2101, loss: 0.8611962199211121\n",
      "epoch: 35, batch number: 2102, loss: 1.2430812120437622\n",
      "epoch: 35, batch number: 2103, loss: 0.900521457195282\n",
      "epoch: 35, batch number: 2104, loss: 0.8183685541152954\n",
      "epoch: 35, batch number: 2105, loss: 0.9302539229393005\n",
      "epoch: 35, batch number: 2106, loss: 1.1518641710281372\n",
      "epoch: 35, batch number: 2107, loss: 1.4598926305770874\n",
      "epoch: 35, batch number: 2108, loss: 0.838482141494751\n",
      "epoch: 35, batch number: 2109, loss: 0.7825253009796143\n",
      "epoch: 35, batch number: 2110, loss: 1.0947120189666748\n",
      "epoch: 35, batch number: 2111, loss: 0.7601827383041382\n",
      "epoch: 35, batch number: 2112, loss: 0.9891191124916077\n",
      "epoch: 35, batch number: 2113, loss: 1.2796086072921753\n",
      "epoch: 35, batch number: 2114, loss: 1.053143858909607\n",
      "epoch: 35, batch number: 2115, loss: 0.9858903288841248\n",
      "epoch: 35, batch number: 2116, loss: 1.0160728693008423\n",
      "epoch: 35, batch number: 2117, loss: 0.8634358048439026\n",
      "epoch: 35, batch number: 2118, loss: 1.166918158531189\n",
      "epoch: 35, batch number: 2119, loss: 0.8295456171035767\n",
      "epoch: 35, batch number: 2120, loss: 1.0851868391036987\n",
      "epoch: 35, batch number: 2121, loss: 1.1136008501052856\n",
      "epoch: 35, batch number: 2122, loss: 1.2041984796524048\n",
      "epoch: 35, batch number: 2123, loss: 0.7699841856956482\n",
      "epoch: 35, batch number: 2124, loss: 1.6889848709106445\n",
      "epoch: 35, batch number: 2125, loss: 1.2556538581848145\n",
      "epoch: 35, batch number: 2126, loss: 0.7725503444671631\n",
      "epoch: 35, batch number: 2127, loss: 1.2739746570587158\n",
      "epoch: 35, batch number: 2128, loss: 1.3354065418243408\n",
      "epoch: 35, batch number: 2129, loss: 0.8617382645606995\n",
      "epoch: 35, batch number: 2130, loss: 0.9013055562973022\n",
      "epoch: 35, batch number: 2131, loss: 0.8464833498001099\n",
      "epoch: 35, batch number: 2132, loss: 0.8647235631942749\n",
      "epoch: 35, batch number: 2133, loss: 1.2045968770980835\n",
      "epoch: 35, batch number: 2134, loss: 0.9802236557006836\n",
      "epoch: 35, batch number: 2135, loss: 0.7361222505569458\n",
      "epoch: 35, batch number: 2136, loss: 0.9927749633789062\n",
      "epoch: 35, batch number: 2137, loss: 1.0688706636428833\n",
      "epoch: 35, batch number: 2138, loss: 1.2556474208831787\n",
      "epoch: 35, batch number: 2139, loss: 1.346736192703247\n",
      "epoch: 35, batch number: 2140, loss: 0.9301325678825378\n",
      "epoch: 35, batch number: 2141, loss: 1.4526301622390747\n",
      "epoch: 36, batch number: 2142, loss: 0.9101970791816711\n",
      "epoch: 36, batch number: 2143, loss: 0.9105761051177979\n",
      "epoch: 36, batch number: 2144, loss: 0.7526402473449707\n",
      "epoch: 36, batch number: 2145, loss: 0.9311245679855347\n",
      "epoch: 36, batch number: 2146, loss: 0.7184154391288757\n",
      "epoch: 36, batch number: 2147, loss: 0.8257137537002563\n",
      "epoch: 36, batch number: 2148, loss: 0.9843596816062927\n",
      "epoch: 36, batch number: 2149, loss: 0.8053943514823914\n",
      "epoch: 36, batch number: 2150, loss: 1.1511157751083374\n",
      "epoch: 36, batch number: 2151, loss: 1.0448485612869263\n",
      "epoch: 36, batch number: 2152, loss: 1.304419755935669\n",
      "epoch: 36, batch number: 2153, loss: 0.8494797348976135\n",
      "epoch: 36, batch number: 2154, loss: 1.1157056093215942\n",
      "epoch: 36, batch number: 2155, loss: 0.7931802272796631\n",
      "epoch: 36, batch number: 2156, loss: 0.667290449142456\n",
      "epoch: 36, batch number: 2157, loss: 0.9421584010124207\n",
      "epoch: 36, batch number: 2158, loss: 0.795984148979187\n",
      "epoch: 36, batch number: 2159, loss: 0.7970151305198669\n",
      "epoch: 36, batch number: 2160, loss: 1.0423856973648071\n",
      "epoch: 36, batch number: 2161, loss: 0.9184790253639221\n",
      "epoch: 36, batch number: 2162, loss: 0.7917287349700928\n",
      "epoch: 36, batch number: 2163, loss: 0.7564524412155151\n",
      "epoch: 36, batch number: 2164, loss: 0.7808220386505127\n",
      "epoch: 36, batch number: 2165, loss: 0.8881277441978455\n",
      "epoch: 36, batch number: 2166, loss: 0.8031534552574158\n",
      "epoch: 36, batch number: 2167, loss: 0.765765368938446\n",
      "epoch: 36, batch number: 2168, loss: 0.7360401153564453\n",
      "epoch: 36, batch number: 2169, loss: 0.9523513913154602\n",
      "epoch: 36, batch number: 2170, loss: 0.948255181312561\n",
      "epoch: 36, batch number: 2171, loss: 0.7545666694641113\n",
      "epoch: 36, batch number: 2172, loss: 0.9073851108551025\n",
      "epoch: 36, batch number: 2173, loss: 0.8157225251197815\n",
      "epoch: 36, batch number: 2174, loss: 0.9714714288711548\n",
      "epoch: 36, batch number: 2175, loss: 0.796406090259552\n",
      "epoch: 36, batch number: 2176, loss: 0.6871311664581299\n",
      "epoch: 36, batch number: 2177, loss: 0.9851237535476685\n",
      "epoch: 36, batch number: 2178, loss: 0.919133722782135\n",
      "epoch: 36, batch number: 2179, loss: 0.7534528374671936\n",
      "epoch: 36, batch number: 2180, loss: 0.8599491715431213\n",
      "epoch: 36, batch number: 2181, loss: 0.999190092086792\n",
      "epoch: 36, batch number: 2182, loss: 0.9842526912689209\n",
      "epoch: 36, batch number: 2183, loss: 0.8920792937278748\n",
      "epoch: 36, batch number: 2184, loss: 0.6612601280212402\n",
      "epoch: 36, batch number: 2185, loss: 0.7728465795516968\n",
      "epoch: 36, batch number: 2186, loss: 0.8837615251541138\n",
      "epoch: 36, batch number: 2187, loss: 0.7839024066925049\n",
      "epoch: 36, batch number: 2188, loss: 0.868990957736969\n",
      "epoch: 36, batch number: 2189, loss: 0.8528261184692383\n",
      "epoch: 36, batch number: 2190, loss: 0.7375221848487854\n",
      "epoch: 36, batch number: 2191, loss: 0.6164996027946472\n",
      "epoch: 36, batch number: 2192, loss: 0.7092335820198059\n",
      "epoch: 36, batch number: 2193, loss: 0.7161862850189209\n",
      "epoch: 36, batch number: 2194, loss: 0.9852825403213501\n",
      "epoch: 36, batch number: 2195, loss: 0.7938898205757141\n",
      "epoch: 36, batch number: 2196, loss: 0.7408244013786316\n",
      "epoch: 36, batch number: 2197, loss: 0.8714732527732849\n",
      "epoch: 36, batch number: 2198, loss: 0.8379265666007996\n",
      "epoch: 36, batch number: 2199, loss: 0.7130931615829468\n",
      "epoch: 36, batch number: 2200, loss: 1.1221691370010376\n",
      "epoch: 36, batch number: 2201, loss: 0.6704513430595398\n",
      "epoch: 36, batch number: 2202, loss: 0.8858076333999634\n",
      "epoch: 36, batch number: 2203, loss: 0.7388486266136169\n",
      "epoch: 36, batch number: 2204, loss: 0.8484712243080139\n",
      "epoch: 37, batch number: 2205, loss: 0.8254914283752441\n",
      "epoch: 37, batch number: 2206, loss: 0.6822850108146667\n",
      "epoch: 37, batch number: 2207, loss: 0.9008682370185852\n",
      "epoch: 37, batch number: 2208, loss: 0.5572035312652588\n",
      "epoch: 37, batch number: 2209, loss: 0.8657781481742859\n",
      "epoch: 37, batch number: 2210, loss: 0.6084031462669373\n",
      "epoch: 37, batch number: 2211, loss: 1.00375497341156\n",
      "epoch: 37, batch number: 2212, loss: 0.6341108679771423\n",
      "epoch: 37, batch number: 2213, loss: 0.8125461339950562\n",
      "epoch: 37, batch number: 2214, loss: 0.7690944075584412\n",
      "epoch: 37, batch number: 2215, loss: 0.6155009269714355\n",
      "epoch: 37, batch number: 2216, loss: 0.5290836691856384\n",
      "epoch: 37, batch number: 2217, loss: 0.5471384525299072\n",
      "epoch: 37, batch number: 2218, loss: 0.7635622620582581\n",
      "epoch: 37, batch number: 2219, loss: 0.7307775020599365\n",
      "epoch: 37, batch number: 2220, loss: 0.6665800213813782\n",
      "epoch: 37, batch number: 2221, loss: 0.7099349498748779\n",
      "epoch: 37, batch number: 2222, loss: 0.6088590621948242\n",
      "epoch: 37, batch number: 2223, loss: 0.6090506315231323\n",
      "epoch: 37, batch number: 2224, loss: 0.9327107667922974\n",
      "epoch: 37, batch number: 2225, loss: 0.8706498742103577\n",
      "epoch: 37, batch number: 2226, loss: 0.9166576266288757\n",
      "epoch: 37, batch number: 2227, loss: 0.5864351987838745\n",
      "epoch: 37, batch number: 2228, loss: 0.9415647983551025\n",
      "epoch: 37, batch number: 2229, loss: 0.7208169102668762\n",
      "epoch: 37, batch number: 2230, loss: 0.4837532043457031\n",
      "epoch: 37, batch number: 2231, loss: 0.5402968525886536\n",
      "epoch: 37, batch number: 2232, loss: 0.8603346347808838\n",
      "epoch: 37, batch number: 2233, loss: 0.5533496141433716\n",
      "epoch: 37, batch number: 2234, loss: 0.7650104761123657\n",
      "epoch: 37, batch number: 2235, loss: 0.5790162086486816\n",
      "epoch: 37, batch number: 2236, loss: 0.6986028552055359\n",
      "epoch: 37, batch number: 2237, loss: 0.5609405636787415\n",
      "epoch: 37, batch number: 2238, loss: 0.796284556388855\n",
      "epoch: 37, batch number: 2239, loss: 0.5991281270980835\n",
      "epoch: 37, batch number: 2240, loss: 0.6644101142883301\n",
      "epoch: 37, batch number: 2241, loss: 0.389528751373291\n",
      "epoch: 37, batch number: 2242, loss: 0.7074020504951477\n",
      "epoch: 37, batch number: 2243, loss: 0.5444443225860596\n",
      "epoch: 37, batch number: 2244, loss: 0.8769974112510681\n",
      "epoch: 37, batch number: 2245, loss: 0.5460370779037476\n",
      "epoch: 37, batch number: 2246, loss: 0.9748040437698364\n",
      "epoch: 37, batch number: 2247, loss: 0.6964179873466492\n",
      "epoch: 37, batch number: 2248, loss: 0.42585843801498413\n",
      "epoch: 37, batch number: 2249, loss: 0.7342050075531006\n",
      "epoch: 37, batch number: 2250, loss: 0.7297033667564392\n",
      "epoch: 37, batch number: 2251, loss: 0.7751194834709167\n",
      "epoch: 37, batch number: 2252, loss: 0.7201560735702515\n",
      "epoch: 37, batch number: 2253, loss: 0.7551140189170837\n",
      "epoch: 37, batch number: 2254, loss: 0.4555143415927887\n",
      "epoch: 37, batch number: 2255, loss: 0.5365586876869202\n",
      "epoch: 37, batch number: 2256, loss: 0.5513447523117065\n",
      "epoch: 37, batch number: 2257, loss: 0.5590496063232422\n",
      "epoch: 37, batch number: 2258, loss: 0.7842648029327393\n",
      "epoch: 37, batch number: 2259, loss: 0.6046388745307922\n",
      "epoch: 37, batch number: 2260, loss: 0.6402474641799927\n",
      "epoch: 37, batch number: 2261, loss: 0.8065553307533264\n",
      "epoch: 37, batch number: 2262, loss: 0.869007408618927\n",
      "epoch: 37, batch number: 2263, loss: 0.6849647164344788\n",
      "epoch: 37, batch number: 2264, loss: 0.6382304430007935\n",
      "epoch: 37, batch number: 2265, loss: 0.6670401692390442\n",
      "epoch: 37, batch number: 2266, loss: 0.537451982498169\n",
      "epoch: 37, batch number: 2267, loss: 1.356074333190918\n",
      "epoch: 38, batch number: 2268, loss: 1.1327704191207886\n",
      "epoch: 38, batch number: 2269, loss: 1.1384786367416382\n",
      "epoch: 38, batch number: 2270, loss: 1.6050764322280884\n",
      "epoch: 38, batch number: 2271, loss: 1.183979868888855\n",
      "epoch: 38, batch number: 2272, loss: 0.704302966594696\n",
      "epoch: 38, batch number: 2273, loss: 1.0003092288970947\n",
      "epoch: 38, batch number: 2274, loss: 1.535240888595581\n",
      "epoch: 38, batch number: 2275, loss: 1.862178921699524\n",
      "epoch: 38, batch number: 2276, loss: 1.8629323244094849\n",
      "epoch: 38, batch number: 2277, loss: 1.576870083808899\n",
      "epoch: 38, batch number: 2278, loss: 1.437188744544983\n",
      "epoch: 38, batch number: 2279, loss: 2.079596519470215\n",
      "epoch: 38, batch number: 2280, loss: 1.3360235691070557\n",
      "epoch: 38, batch number: 2281, loss: 1.088692307472229\n",
      "epoch: 38, batch number: 2282, loss: 1.7443219423294067\n",
      "epoch: 38, batch number: 2283, loss: 1.537723183631897\n",
      "epoch: 38, batch number: 2284, loss: 1.220011591911316\n",
      "epoch: 38, batch number: 2285, loss: 1.7908259630203247\n",
      "epoch: 38, batch number: 2286, loss: 1.2316635847091675\n",
      "epoch: 38, batch number: 2287, loss: 1.5180068016052246\n",
      "epoch: 38, batch number: 2288, loss: 1.368055820465088\n",
      "epoch: 38, batch number: 2289, loss: 1.4386651515960693\n",
      "epoch: 38, batch number: 2290, loss: 1.3526304960250854\n",
      "epoch: 38, batch number: 2291, loss: 0.6813540458679199\n",
      "epoch: 38, batch number: 2292, loss: 1.2917790412902832\n",
      "epoch: 38, batch number: 2293, loss: 1.102480173110962\n",
      "epoch: 38, batch number: 2294, loss: 1.0902336835861206\n",
      "epoch: 38, batch number: 2295, loss: 1.824500560760498\n",
      "epoch: 38, batch number: 2296, loss: 1.35594642162323\n",
      "epoch: 38, batch number: 2297, loss: 1.4478803873062134\n",
      "epoch: 38, batch number: 2298, loss: 1.1543731689453125\n",
      "epoch: 38, batch number: 2299, loss: 1.2186366319656372\n",
      "epoch: 38, batch number: 2300, loss: 1.0155582427978516\n",
      "epoch: 38, batch number: 2301, loss: 1.1758352518081665\n",
      "epoch: 38, batch number: 2302, loss: 1.223303198814392\n",
      "epoch: 38, batch number: 2303, loss: 1.2716137170791626\n",
      "epoch: 38, batch number: 2304, loss: 1.0986980199813843\n",
      "epoch: 38, batch number: 2305, loss: 1.236994981765747\n",
      "epoch: 38, batch number: 2306, loss: 1.5239509344100952\n",
      "epoch: 38, batch number: 2307, loss: 0.9184700846672058\n",
      "epoch: 38, batch number: 2308, loss: 1.3322285413742065\n",
      "epoch: 38, batch number: 2309, loss: 1.4264146089553833\n",
      "epoch: 38, batch number: 2310, loss: 1.112083077430725\n",
      "epoch: 38, batch number: 2311, loss: 1.3141220808029175\n",
      "epoch: 38, batch number: 2312, loss: 1.018368124961853\n",
      "epoch: 38, batch number: 2313, loss: 1.195988655090332\n",
      "epoch: 38, batch number: 2314, loss: 1.3319439888000488\n",
      "epoch: 38, batch number: 2315, loss: 1.1635411977767944\n",
      "epoch: 38, batch number: 2316, loss: 1.4086918830871582\n",
      "epoch: 38, batch number: 2317, loss: 1.3344135284423828\n",
      "epoch: 38, batch number: 2318, loss: 0.9370538592338562\n",
      "epoch: 38, batch number: 2319, loss: 1.4328181743621826\n",
      "epoch: 38, batch number: 2320, loss: 1.653234839439392\n",
      "epoch: 38, batch number: 2321, loss: 1.077933430671692\n",
      "epoch: 38, batch number: 2322, loss: 0.9382271766662598\n",
      "epoch: 38, batch number: 2323, loss: 1.376409649848938\n",
      "epoch: 38, batch number: 2324, loss: 1.0222994089126587\n",
      "epoch: 38, batch number: 2325, loss: 0.9864763617515564\n",
      "epoch: 38, batch number: 2326, loss: 1.0027689933776855\n",
      "epoch: 38, batch number: 2327, loss: 0.8041377067565918\n",
      "epoch: 38, batch number: 2328, loss: 0.8854307532310486\n",
      "epoch: 38, batch number: 2329, loss: 1.0203397274017334\n",
      "epoch: 38, batch number: 2330, loss: 1.6921683549880981\n",
      "epoch: 39, batch number: 2331, loss: 1.4873603582382202\n",
      "epoch: 39, batch number: 2332, loss: 1.3140003681182861\n",
      "epoch: 39, batch number: 2333, loss: 1.2181000709533691\n",
      "epoch: 39, batch number: 2334, loss: 1.5067671537399292\n",
      "epoch: 39, batch number: 2335, loss: 1.500213623046875\n",
      "epoch: 39, batch number: 2336, loss: 1.0777156352996826\n",
      "epoch: 39, batch number: 2337, loss: 1.6031160354614258\n",
      "epoch: 39, batch number: 2338, loss: 1.329114317893982\n",
      "epoch: 39, batch number: 2339, loss: 1.0836249589920044\n",
      "epoch: 39, batch number: 2340, loss: 1.4914191961288452\n",
      "epoch: 39, batch number: 2341, loss: 0.9538414478302002\n",
      "epoch: 39, batch number: 2342, loss: 1.2433102130889893\n",
      "epoch: 39, batch number: 2343, loss: 1.0052748918533325\n",
      "epoch: 39, batch number: 2344, loss: 1.3074356317520142\n",
      "epoch: 39, batch number: 2345, loss: 1.4802337884902954\n",
      "epoch: 39, batch number: 2346, loss: 1.2531869411468506\n",
      "epoch: 39, batch number: 2347, loss: 1.343428373336792\n",
      "epoch: 39, batch number: 2348, loss: 1.1328099966049194\n",
      "epoch: 39, batch number: 2349, loss: 1.1325855255126953\n",
      "epoch: 39, batch number: 2350, loss: 1.1792752742767334\n",
      "epoch: 39, batch number: 2351, loss: 1.1865967512130737\n",
      "epoch: 39, batch number: 2352, loss: 1.2259856462478638\n",
      "epoch: 39, batch number: 2353, loss: 1.2071559429168701\n",
      "epoch: 39, batch number: 2354, loss: 1.285231351852417\n",
      "epoch: 39, batch number: 2355, loss: 1.4189528226852417\n",
      "epoch: 39, batch number: 2356, loss: 1.3619076013565063\n",
      "epoch: 39, batch number: 2357, loss: 1.0016262531280518\n",
      "epoch: 39, batch number: 2358, loss: 1.228975534439087\n",
      "epoch: 39, batch number: 2359, loss: 1.3208388090133667\n",
      "epoch: 39, batch number: 2360, loss: 1.0837947130203247\n",
      "epoch: 39, batch number: 2361, loss: 1.1987358331680298\n",
      "epoch: 39, batch number: 2362, loss: 1.1899151802062988\n",
      "epoch: 39, batch number: 2363, loss: 1.0434173345565796\n",
      "epoch: 39, batch number: 2364, loss: 1.238832712173462\n",
      "epoch: 39, batch number: 2365, loss: 1.095004916191101\n",
      "epoch: 39, batch number: 2366, loss: 1.0465803146362305\n",
      "epoch: 39, batch number: 2367, loss: 0.9610605835914612\n",
      "epoch: 39, batch number: 2368, loss: 1.093004822731018\n",
      "epoch: 39, batch number: 2369, loss: 1.1305593252182007\n",
      "epoch: 39, batch number: 2370, loss: 1.0740662813186646\n",
      "epoch: 39, batch number: 2371, loss: 1.1645976305007935\n",
      "epoch: 39, batch number: 2372, loss: 0.9626100659370422\n",
      "epoch: 39, batch number: 2373, loss: 1.0976533889770508\n",
      "epoch: 39, batch number: 2374, loss: 1.0920268297195435\n",
      "epoch: 39, batch number: 2375, loss: 1.1516445875167847\n",
      "epoch: 39, batch number: 2376, loss: 0.9372605681419373\n",
      "epoch: 39, batch number: 2377, loss: 1.2094396352767944\n",
      "epoch: 39, batch number: 2378, loss: 1.1905920505523682\n",
      "epoch: 39, batch number: 2379, loss: 1.1250518560409546\n",
      "epoch: 39, batch number: 2380, loss: 1.1409530639648438\n",
      "epoch: 39, batch number: 2381, loss: 1.2402184009552002\n",
      "epoch: 39, batch number: 2382, loss: 1.2885977029800415\n",
      "epoch: 39, batch number: 2383, loss: 1.3993141651153564\n",
      "epoch: 39, batch number: 2384, loss: 1.0270633697509766\n",
      "epoch: 39, batch number: 2385, loss: 1.039208173751831\n",
      "epoch: 39, batch number: 2386, loss: 1.241186499595642\n",
      "epoch: 39, batch number: 2387, loss: 1.1318104267120361\n",
      "epoch: 39, batch number: 2388, loss: 1.1660321950912476\n",
      "epoch: 39, batch number: 2389, loss: 0.9976629614830017\n",
      "epoch: 39, batch number: 2390, loss: 0.9839881062507629\n",
      "epoch: 39, batch number: 2391, loss: 1.0789649486541748\n",
      "epoch: 40, batch number: 2392, loss: 1.1266450881958008\n",
      "epoch: 40, batch number: 2393, loss: 1.607230544090271\n",
      "epoch: 40, batch number: 2394, loss: 1.2402594089508057\n",
      "epoch: 40, batch number: 2395, loss: 1.2079451084136963\n",
      "epoch: 40, batch number: 2396, loss: 1.2720948457717896\n",
      "epoch: 40, batch number: 2397, loss: 1.1292282342910767\n",
      "epoch: 40, batch number: 2398, loss: 1.0691938400268555\n",
      "epoch: 40, batch number: 2399, loss: 1.4963045120239258\n",
      "epoch: 40, batch number: 2400, loss: 1.3651918172836304\n",
      "epoch: 40, batch number: 2401, loss: 1.2095322608947754\n",
      "epoch: 40, batch number: 2402, loss: 1.1489779949188232\n",
      "epoch: 40, batch number: 2403, loss: 1.3366525173187256\n",
      "epoch: 40, batch number: 2404, loss: 1.3145326375961304\n",
      "epoch: 40, batch number: 2405, loss: 1.1575052738189697\n",
      "epoch: 40, batch number: 2406, loss: 1.1973273754119873\n",
      "epoch: 40, batch number: 2407, loss: 1.1062663793563843\n",
      "epoch: 40, batch number: 2408, loss: 1.4746476411819458\n",
      "epoch: 40, batch number: 2409, loss: 1.1774564981460571\n",
      "epoch: 40, batch number: 2410, loss: 1.3171526193618774\n",
      "epoch: 40, batch number: 2411, loss: 1.2821340560913086\n",
      "epoch: 40, batch number: 2412, loss: 1.0789529085159302\n",
      "epoch: 40, batch number: 2413, loss: 1.2445924282073975\n",
      "epoch: 40, batch number: 2414, loss: 0.8786997199058533\n",
      "epoch: 40, batch number: 2415, loss: 1.0988874435424805\n",
      "epoch: 40, batch number: 2416, loss: 1.2100999355316162\n",
      "epoch: 40, batch number: 2417, loss: 1.1931527853012085\n",
      "epoch: 40, batch number: 2418, loss: 1.083578109741211\n",
      "epoch: 40, batch number: 2419, loss: 1.0363891124725342\n",
      "epoch: 40, batch number: 2420, loss: 1.0542168617248535\n",
      "epoch: 40, batch number: 2421, loss: 1.517700433731079\n",
      "epoch: 40, batch number: 2422, loss: 1.2533786296844482\n",
      "epoch: 40, batch number: 2423, loss: 0.9442534446716309\n",
      "epoch: 40, batch number: 2424, loss: 1.0340864658355713\n",
      "epoch: 40, batch number: 2425, loss: 1.084624171257019\n",
      "epoch: 40, batch number: 2426, loss: 1.3348522186279297\n",
      "epoch: 40, batch number: 2427, loss: 1.2830628156661987\n",
      "epoch: 40, batch number: 2428, loss: 1.1347202062606812\n",
      "epoch: 40, batch number: 2429, loss: 0.9989649057388306\n",
      "epoch: 40, batch number: 2430, loss: 1.26958429813385\n",
      "epoch: 40, batch number: 2431, loss: 1.069004774093628\n",
      "epoch: 40, batch number: 2432, loss: 0.8878732919692993\n",
      "epoch: 40, batch number: 2433, loss: 1.1301114559173584\n",
      "epoch: 40, batch number: 2434, loss: 1.1582818031311035\n",
      "epoch: 40, batch number: 2435, loss: 1.1199584007263184\n",
      "epoch: 40, batch number: 2436, loss: 1.1800142526626587\n",
      "epoch: 40, batch number: 2437, loss: 1.0844780206680298\n",
      "epoch: 40, batch number: 2438, loss: 1.1309658288955688\n",
      "epoch: 40, batch number: 2439, loss: 1.3125944137573242\n",
      "epoch: 40, batch number: 2440, loss: 1.114758014678955\n",
      "epoch: 40, batch number: 2441, loss: 1.2888405323028564\n",
      "epoch: 40, batch number: 2442, loss: 1.1258007287979126\n",
      "epoch: 40, batch number: 2443, loss: 1.1675554513931274\n",
      "epoch: 40, batch number: 2444, loss: 1.1618887186050415\n",
      "epoch: 40, batch number: 2445, loss: 1.250139594078064\n",
      "epoch: 40, batch number: 2446, loss: 1.33617103099823\n",
      "epoch: 40, batch number: 2447, loss: 1.1391468048095703\n",
      "epoch: 40, batch number: 2448, loss: 1.3355886936187744\n",
      "epoch: 40, batch number: 2449, loss: 1.1016756296157837\n",
      "epoch: 40, batch number: 2450, loss: 1.1067297458648682\n",
      "epoch: 40, batch number: 2451, loss: 1.1138677597045898\n",
      "epoch: 40, batch number: 2452, loss: 1.003424882888794\n",
      "epoch: 40, batch number: 2453, loss: 1.3290225267410278\n",
      "epoch: 41, batch number: 2454, loss: 0.8553646206855774\n",
      "epoch: 41, batch number: 2455, loss: 1.1196584701538086\n",
      "epoch: 41, batch number: 2456, loss: 1.0890625715255737\n",
      "epoch: 41, batch number: 2457, loss: 1.1574615240097046\n",
      "epoch: 41, batch number: 2458, loss: 1.2938472032546997\n",
      "epoch: 41, batch number: 2459, loss: 1.0568656921386719\n",
      "epoch: 41, batch number: 2460, loss: 1.202401876449585\n",
      "epoch: 41, batch number: 2461, loss: 1.44023597240448\n",
      "epoch: 41, batch number: 2462, loss: 1.3987376689910889\n",
      "epoch: 41, batch number: 2463, loss: 1.2257307767868042\n",
      "epoch: 41, batch number: 2464, loss: 1.4620202779769897\n",
      "epoch: 41, batch number: 2465, loss: 1.0674736499786377\n",
      "epoch: 41, batch number: 2466, loss: 1.036650538444519\n",
      "epoch: 41, batch number: 2467, loss: 1.2206075191497803\n",
      "epoch: 41, batch number: 2468, loss: 1.2854504585266113\n",
      "epoch: 41, batch number: 2469, loss: 1.2999268770217896\n",
      "epoch: 41, batch number: 2470, loss: 1.1047786474227905\n",
      "epoch: 41, batch number: 2471, loss: 1.1484718322753906\n",
      "epoch: 41, batch number: 2472, loss: 1.2365020513534546\n",
      "epoch: 41, batch number: 2473, loss: 1.0318458080291748\n",
      "epoch: 41, batch number: 2474, loss: 1.2124884128570557\n",
      "epoch: 41, batch number: 2475, loss: 1.509642481803894\n",
      "epoch: 41, batch number: 2476, loss: 1.1365115642547607\n",
      "epoch: 41, batch number: 2477, loss: 1.3118207454681396\n",
      "epoch: 41, batch number: 2478, loss: 1.2792407274246216\n",
      "epoch: 41, batch number: 2479, loss: 0.9986337423324585\n",
      "epoch: 41, batch number: 2480, loss: 1.3091390132904053\n",
      "epoch: 41, batch number: 2481, loss: 1.839694857597351\n",
      "epoch: 41, batch number: 2482, loss: 2.0657572746276855\n",
      "epoch: 41, batch number: 2483, loss: 1.023294448852539\n",
      "epoch: 41, batch number: 2484, loss: 0.9919149875640869\n",
      "epoch: 41, batch number: 2485, loss: 1.5184423923492432\n",
      "epoch: 41, batch number: 2486, loss: 1.3653062582015991\n",
      "epoch: 41, batch number: 2487, loss: 1.2989850044250488\n",
      "epoch: 41, batch number: 2488, loss: 0.9908100962638855\n",
      "epoch: 41, batch number: 2489, loss: 0.8705096244812012\n",
      "epoch: 41, batch number: 2490, loss: 1.2838705778121948\n",
      "epoch: 41, batch number: 2491, loss: 0.7527162432670593\n",
      "epoch: 41, batch number: 2492, loss: 1.1570539474487305\n",
      "epoch: 41, batch number: 2493, loss: 1.259206771850586\n",
      "epoch: 41, batch number: 2494, loss: 1.037922978401184\n",
      "epoch: 41, batch number: 2495, loss: 1.1371272802352905\n",
      "epoch: 41, batch number: 2496, loss: 1.063628911972046\n",
      "epoch: 41, batch number: 2497, loss: 0.9917200207710266\n",
      "epoch: 41, batch number: 2498, loss: 1.3628730773925781\n",
      "epoch: 41, batch number: 2499, loss: 1.2683370113372803\n",
      "epoch: 41, batch number: 2500, loss: 1.393929123878479\n",
      "epoch: 41, batch number: 2501, loss: 0.991150438785553\n",
      "epoch: 41, batch number: 2502, loss: 1.271701455116272\n",
      "epoch: 41, batch number: 2503, loss: 1.0227959156036377\n",
      "epoch: 41, batch number: 2504, loss: 1.0783288478851318\n",
      "epoch: 41, batch number: 2505, loss: 1.2308480739593506\n",
      "epoch: 41, batch number: 2506, loss: 1.0990928411483765\n",
      "epoch: 41, batch number: 2507, loss: 1.3818763494491577\n",
      "epoch: 41, batch number: 2508, loss: 1.4614012241363525\n",
      "epoch: 41, batch number: 2509, loss: 1.3987947702407837\n",
      "epoch: 41, batch number: 2510, loss: 1.1732356548309326\n",
      "epoch: 41, batch number: 2511, loss: 1.2466648817062378\n",
      "epoch: 41, batch number: 2512, loss: 1.4775316715240479\n",
      "epoch: 41, batch number: 2513, loss: 1.0606614351272583\n",
      "epoch: 41, batch number: 2514, loss: 1.180245041847229\n",
      "epoch: 41, batch number: 2515, loss: 1.149532437324524\n",
      "epoch: 41, batch number: 2516, loss: 0.41116833686828613\n",
      "epoch: 42, batch number: 2517, loss: 1.5315200090408325\n",
      "epoch: 42, batch number: 2518, loss: 1.9691643714904785\n",
      "epoch: 42, batch number: 2519, loss: 1.1954506635665894\n",
      "epoch: 42, batch number: 2520, loss: 1.7160181999206543\n",
      "epoch: 42, batch number: 2521, loss: 2.543055295944214\n",
      "epoch: 42, batch number: 2522, loss: 2.1155710220336914\n",
      "epoch: 42, batch number: 2523, loss: 1.770514965057373\n",
      "epoch: 42, batch number: 2524, loss: 1.373356580734253\n",
      "epoch: 42, batch number: 2525, loss: 1.434535264968872\n",
      "epoch: 42, batch number: 2526, loss: 1.4810659885406494\n",
      "epoch: 42, batch number: 2527, loss: 1.1248455047607422\n",
      "epoch: 42, batch number: 2528, loss: 1.5593549013137817\n",
      "epoch: 42, batch number: 2529, loss: 2.2516419887542725\n",
      "epoch: 42, batch number: 2530, loss: 1.4348615407943726\n",
      "epoch: 42, batch number: 2531, loss: 1.7665976285934448\n",
      "epoch: 42, batch number: 2532, loss: 1.4878498315811157\n",
      "epoch: 42, batch number: 2533, loss: 1.9755717515945435\n",
      "epoch: 42, batch number: 2534, loss: 2.286770820617676\n",
      "epoch: 42, batch number: 2535, loss: 1.956203818321228\n",
      "epoch: 42, batch number: 2536, loss: 1.705827236175537\n",
      "epoch: 42, batch number: 2537, loss: 1.8050624132156372\n",
      "epoch: 42, batch number: 2538, loss: 0.9278416633605957\n",
      "epoch: 42, batch number: 2539, loss: 1.6736665964126587\n",
      "epoch: 42, batch number: 2540, loss: 1.6672947406768799\n",
      "epoch: 42, batch number: 2541, loss: 2.0182132720947266\n",
      "epoch: 42, batch number: 2542, loss: 1.4585084915161133\n",
      "epoch: 42, batch number: 2543, loss: 1.5501534938812256\n",
      "epoch: 42, batch number: 2544, loss: 1.6043648719787598\n",
      "epoch: 42, batch number: 2545, loss: 1.712870717048645\n",
      "epoch: 42, batch number: 2546, loss: 1.4541826248168945\n",
      "epoch: 42, batch number: 2547, loss: 1.6931933164596558\n",
      "epoch: 42, batch number: 2548, loss: 1.752640724182129\n",
      "epoch: 42, batch number: 2549, loss: 1.5109241008758545\n",
      "epoch: 42, batch number: 2550, loss: 1.6933318376541138\n",
      "epoch: 42, batch number: 2551, loss: 1.7814804315567017\n",
      "epoch: 42, batch number: 2552, loss: 2.1178839206695557\n",
      "epoch: 42, batch number: 2553, loss: 1.4200434684753418\n",
      "epoch: 42, batch number: 2554, loss: 1.484693169593811\n",
      "epoch: 42, batch number: 2555, loss: 1.4785398244857788\n",
      "epoch: 42, batch number: 2556, loss: 1.3377200365066528\n",
      "epoch: 42, batch number: 2557, loss: 1.549540638923645\n",
      "epoch: 42, batch number: 2558, loss: 1.4523303508758545\n",
      "epoch: 42, batch number: 2559, loss: 1.26912522315979\n",
      "epoch: 42, batch number: 2560, loss: 1.4216970205307007\n",
      "epoch: 42, batch number: 2561, loss: 1.3479174375534058\n",
      "epoch: 42, batch number: 2562, loss: 1.4337568283081055\n",
      "epoch: 42, batch number: 2563, loss: 1.4739614725112915\n",
      "epoch: 42, batch number: 2564, loss: 1.6254109144210815\n",
      "epoch: 42, batch number: 2565, loss: 1.1373038291931152\n",
      "epoch: 42, batch number: 2566, loss: 1.2176533937454224\n",
      "epoch: 42, batch number: 2567, loss: 1.231490135192871\n",
      "epoch: 42, batch number: 2568, loss: 1.4120559692382812\n",
      "epoch: 42, batch number: 2569, loss: 1.5466388463974\n",
      "epoch: 42, batch number: 2570, loss: 1.4763667583465576\n",
      "epoch: 42, batch number: 2571, loss: 0.9028410911560059\n",
      "epoch: 42, batch number: 2572, loss: 1.2730964422225952\n",
      "epoch: 42, batch number: 2573, loss: 1.3965835571289062\n",
      "epoch: 42, batch number: 2574, loss: 1.5822885036468506\n",
      "epoch: 42, batch number: 2575, loss: 1.330008864402771\n",
      "epoch: 42, batch number: 2576, loss: 1.3006740808486938\n",
      "epoch: 42, batch number: 2577, loss: 1.554321050643921\n",
      "epoch: 42, batch number: 2578, loss: 1.403692364692688\n",
      "epoch: 42, batch number: 2579, loss: 1.0962886810302734\n",
      "epoch: 43, batch number: 2580, loss: 0.9497731924057007\n",
      "epoch: 43, batch number: 2581, loss: 1.0194470882415771\n",
      "epoch: 43, batch number: 2582, loss: 1.1379510164260864\n",
      "epoch: 43, batch number: 2583, loss: 1.4881294965744019\n",
      "epoch: 43, batch number: 2584, loss: 0.8558732867240906\n",
      "epoch: 43, batch number: 2585, loss: 1.2114185094833374\n",
      "epoch: 43, batch number: 2586, loss: 1.120466709136963\n",
      "epoch: 43, batch number: 2587, loss: 1.1134141683578491\n",
      "epoch: 43, batch number: 2588, loss: 1.1566709280014038\n",
      "epoch: 43, batch number: 2589, loss: 1.14670991897583\n",
      "epoch: 43, batch number: 2590, loss: 1.3394989967346191\n",
      "epoch: 43, batch number: 2591, loss: 1.2441188097000122\n",
      "epoch: 43, batch number: 2592, loss: 1.0610814094543457\n",
      "epoch: 43, batch number: 2593, loss: 1.3230549097061157\n",
      "epoch: 43, batch number: 2594, loss: 1.1561975479125977\n",
      "epoch: 43, batch number: 2595, loss: 1.0749609470367432\n",
      "epoch: 43, batch number: 2596, loss: 1.2341328859329224\n",
      "epoch: 43, batch number: 2597, loss: 1.0153974294662476\n",
      "epoch: 43, batch number: 2598, loss: 0.9607322812080383\n",
      "epoch: 43, batch number: 2599, loss: 1.233389139175415\n",
      "epoch: 43, batch number: 2600, loss: 1.2271671295166016\n",
      "epoch: 43, batch number: 2601, loss: 0.9134120941162109\n",
      "epoch: 43, batch number: 2602, loss: 0.9483066201210022\n",
      "epoch: 43, batch number: 2603, loss: 1.2466585636138916\n",
      "epoch: 43, batch number: 2604, loss: 1.1624633073806763\n",
      "epoch: 43, batch number: 2605, loss: 1.2427836656570435\n",
      "epoch: 43, batch number: 2606, loss: 0.9240227937698364\n",
      "epoch: 43, batch number: 2607, loss: 1.3709566593170166\n",
      "epoch: 43, batch number: 2608, loss: 1.068912148475647\n",
      "epoch: 43, batch number: 2609, loss: 1.1814945936203003\n",
      "epoch: 43, batch number: 2610, loss: 1.0682734251022339\n",
      "epoch: 43, batch number: 2611, loss: 0.8949990272521973\n",
      "epoch: 43, batch number: 2612, loss: 1.006020426750183\n",
      "epoch: 43, batch number: 2613, loss: 0.9980953335762024\n",
      "epoch: 43, batch number: 2614, loss: 1.0197796821594238\n",
      "epoch: 43, batch number: 2615, loss: 0.9645043611526489\n",
      "epoch: 43, batch number: 2616, loss: 1.1408066749572754\n",
      "epoch: 43, batch number: 2617, loss: 1.2004519701004028\n",
      "epoch: 43, batch number: 2618, loss: 1.3057538270950317\n",
      "epoch: 43, batch number: 2619, loss: 1.1158722639083862\n",
      "epoch: 43, batch number: 2620, loss: 1.0249254703521729\n",
      "epoch: 43, batch number: 2621, loss: 1.2526464462280273\n",
      "epoch: 43, batch number: 2622, loss: 1.2699583768844604\n",
      "epoch: 43, batch number: 2623, loss: 1.3590857982635498\n",
      "epoch: 43, batch number: 2624, loss: 1.1335728168487549\n",
      "epoch: 43, batch number: 2625, loss: 1.2049262523651123\n",
      "epoch: 43, batch number: 2626, loss: 1.1962014436721802\n",
      "epoch: 43, batch number: 2627, loss: 1.0135397911071777\n",
      "epoch: 43, batch number: 2628, loss: 1.1627658605575562\n",
      "epoch: 43, batch number: 2629, loss: 0.9698281288146973\n",
      "epoch: 43, batch number: 2630, loss: 1.1019881963729858\n",
      "epoch: 43, batch number: 2631, loss: 1.1358606815338135\n",
      "epoch: 43, batch number: 2632, loss: 1.0529454946517944\n",
      "epoch: 43, batch number: 2633, loss: 0.9746946096420288\n",
      "epoch: 43, batch number: 2634, loss: 1.117173194885254\n",
      "epoch: 43, batch number: 2635, loss: 1.104934811592102\n",
      "epoch: 43, batch number: 2636, loss: 1.0515456199645996\n",
      "epoch: 43, batch number: 2637, loss: 1.1993446350097656\n",
      "epoch: 43, batch number: 2638, loss: 1.2937456369400024\n",
      "epoch: 43, batch number: 2639, loss: 1.1656535863876343\n",
      "epoch: 43, batch number: 2640, loss: 1.1219704151153564\n",
      "epoch: 43, batch number: 2641, loss: 1.1905690431594849\n",
      "epoch: 43, batch number: 2642, loss: 1.0167887210845947\n",
      "epoch: 44, batch number: 2643, loss: 1.2207337617874146\n",
      "epoch: 44, batch number: 2644, loss: 1.1086422204971313\n",
      "epoch: 44, batch number: 2645, loss: 1.4149136543273926\n",
      "epoch: 44, batch number: 2646, loss: 1.3258445262908936\n",
      "epoch: 44, batch number: 2647, loss: 1.1299164295196533\n",
      "epoch: 44, batch number: 2648, loss: 1.257704496383667\n",
      "epoch: 44, batch number: 2649, loss: 1.1496868133544922\n",
      "epoch: 44, batch number: 2650, loss: 0.9328100681304932\n",
      "epoch: 44, batch number: 2651, loss: 1.0386685132980347\n",
      "epoch: 44, batch number: 2652, loss: 1.348616123199463\n",
      "epoch: 44, batch number: 2653, loss: 1.2361209392547607\n",
      "epoch: 44, batch number: 2654, loss: 1.12296724319458\n",
      "epoch: 44, batch number: 2655, loss: 1.118227243423462\n",
      "epoch: 44, batch number: 2656, loss: 1.139844536781311\n",
      "epoch: 44, batch number: 2657, loss: 1.1206916570663452\n",
      "epoch: 44, batch number: 2658, loss: 1.0806010961532593\n",
      "epoch: 44, batch number: 2659, loss: 1.0701872110366821\n",
      "epoch: 44, batch number: 2660, loss: 1.107914924621582\n",
      "epoch: 44, batch number: 2661, loss: 1.0466878414154053\n",
      "epoch: 44, batch number: 2662, loss: 0.9888024926185608\n",
      "epoch: 44, batch number: 2663, loss: 1.2128709554672241\n",
      "epoch: 44, batch number: 2664, loss: 1.0128328800201416\n",
      "epoch: 44, batch number: 2665, loss: 1.080214262008667\n",
      "epoch: 44, batch number: 2666, loss: 1.0924690961837769\n",
      "epoch: 44, batch number: 2667, loss: 1.1487888097763062\n",
      "epoch: 44, batch number: 2668, loss: 0.901685357093811\n",
      "epoch: 44, batch number: 2669, loss: 1.1152435541152954\n",
      "epoch: 44, batch number: 2670, loss: 0.9026685953140259\n",
      "epoch: 44, batch number: 2671, loss: 1.0033677816390991\n",
      "epoch: 44, batch number: 2672, loss: 1.0052525997161865\n",
      "epoch: 44, batch number: 2673, loss: 0.9744414687156677\n",
      "epoch: 44, batch number: 2674, loss: 0.966088593006134\n",
      "epoch: 44, batch number: 2675, loss: 0.8584392666816711\n",
      "epoch: 44, batch number: 2676, loss: 1.0356101989746094\n",
      "epoch: 44, batch number: 2677, loss: 1.04682457447052\n",
      "epoch: 44, batch number: 2678, loss: 1.0512720346450806\n",
      "epoch: 44, batch number: 2679, loss: 1.0577099323272705\n",
      "epoch: 44, batch number: 2680, loss: 1.0002033710479736\n",
      "epoch: 44, batch number: 2681, loss: 0.9820352792739868\n",
      "epoch: 44, batch number: 2682, loss: 1.003227710723877\n",
      "epoch: 44, batch number: 2683, loss: 0.9135637879371643\n",
      "epoch: 44, batch number: 2684, loss: 0.8737372159957886\n",
      "epoch: 44, batch number: 2685, loss: 1.0918532609939575\n",
      "epoch: 44, batch number: 2686, loss: 1.01213800907135\n",
      "epoch: 44, batch number: 2687, loss: 1.021255373954773\n",
      "epoch: 44, batch number: 2688, loss: 1.0634452104568481\n",
      "epoch: 44, batch number: 2689, loss: 0.9142982363700867\n",
      "epoch: 44, batch number: 2690, loss: 1.0042181015014648\n",
      "epoch: 44, batch number: 2691, loss: 0.9525574445724487\n",
      "epoch: 44, batch number: 2692, loss: 0.9921689629554749\n",
      "epoch: 44, batch number: 2693, loss: 0.9800092577934265\n",
      "epoch: 44, batch number: 2694, loss: 0.852809727191925\n",
      "epoch: 44, batch number: 2695, loss: 0.9698336124420166\n",
      "epoch: 44, batch number: 2696, loss: 0.886122465133667\n",
      "epoch: 44, batch number: 2697, loss: 0.9231645464897156\n",
      "epoch: 44, batch number: 2698, loss: 1.0130398273468018\n",
      "epoch: 44, batch number: 2699, loss: 0.8592913150787354\n",
      "epoch: 44, batch number: 2700, loss: 0.9972899556159973\n",
      "epoch: 44, batch number: 2701, loss: 1.0310730934143066\n",
      "epoch: 44, batch number: 2702, loss: 1.0277413129806519\n",
      "epoch: 44, batch number: 2703, loss: 0.8816080689430237\n",
      "epoch: 44, batch number: 2704, loss: 0.8637826442718506\n",
      "epoch: 44, batch number: 2705, loss: 1.3093408346176147\n",
      "epoch: 45, batch number: 2706, loss: 1.1184444427490234\n",
      "epoch: 45, batch number: 2707, loss: 1.5146729946136475\n",
      "epoch: 45, batch number: 2708, loss: 1.1032195091247559\n",
      "epoch: 45, batch number: 2709, loss: 1.511233925819397\n",
      "epoch: 45, batch number: 2710, loss: 1.5999600887298584\n",
      "epoch: 45, batch number: 2711, loss: 1.577862024307251\n",
      "epoch: 45, batch number: 2712, loss: 1.1356549263000488\n",
      "epoch: 45, batch number: 2713, loss: 1.278334140777588\n",
      "epoch: 45, batch number: 2714, loss: 1.135992169380188\n",
      "epoch: 45, batch number: 2715, loss: 1.259036660194397\n",
      "epoch: 45, batch number: 2716, loss: 1.1307138204574585\n",
      "epoch: 45, batch number: 2717, loss: 0.9424367547035217\n",
      "epoch: 45, batch number: 2718, loss: 0.867492139339447\n",
      "epoch: 45, batch number: 2719, loss: 1.176287293434143\n",
      "epoch: 45, batch number: 2720, loss: 0.8399118185043335\n",
      "epoch: 45, batch number: 2721, loss: 1.2260444164276123\n",
      "epoch: 45, batch number: 2722, loss: 1.2103729248046875\n",
      "epoch: 45, batch number: 2723, loss: 0.8810068964958191\n",
      "epoch: 45, batch number: 2724, loss: 0.9073187112808228\n",
      "epoch: 45, batch number: 2725, loss: 0.8362162113189697\n",
      "epoch: 45, batch number: 2726, loss: 1.2579305171966553\n",
      "epoch: 45, batch number: 2727, loss: 1.3327553272247314\n",
      "epoch: 45, batch number: 2728, loss: 1.2195626497268677\n",
      "epoch: 45, batch number: 2729, loss: 1.548478364944458\n",
      "epoch: 45, batch number: 2730, loss: 1.1297231912612915\n",
      "epoch: 45, batch number: 2731, loss: 0.7945395708084106\n",
      "epoch: 45, batch number: 2732, loss: 0.9292948246002197\n",
      "epoch: 45, batch number: 2733, loss: 0.8871238231658936\n",
      "epoch: 45, batch number: 2734, loss: 0.9742539525032043\n",
      "epoch: 45, batch number: 2735, loss: 1.3350706100463867\n",
      "epoch: 45, batch number: 2736, loss: 0.7682961225509644\n",
      "epoch: 45, batch number: 2737, loss: 0.6384711265563965\n",
      "epoch: 45, batch number: 2738, loss: 1.0110009908676147\n",
      "epoch: 45, batch number: 2739, loss: 1.0854926109313965\n",
      "epoch: 45, batch number: 2740, loss: 1.1099458932876587\n",
      "epoch: 45, batch number: 2741, loss: 0.6968063116073608\n",
      "epoch: 45, batch number: 2742, loss: 1.1394504308700562\n",
      "epoch: 45, batch number: 2743, loss: 1.1023629903793335\n",
      "epoch: 45, batch number: 2744, loss: 0.8823580145835876\n",
      "epoch: 45, batch number: 2745, loss: 0.7366624474525452\n",
      "epoch: 45, batch number: 2746, loss: 1.0277081727981567\n",
      "epoch: 45, batch number: 2747, loss: 1.016694188117981\n",
      "epoch: 45, batch number: 2748, loss: 1.0846679210662842\n",
      "epoch: 45, batch number: 2749, loss: 1.0321346521377563\n",
      "epoch: 45, batch number: 2750, loss: 0.9913979768753052\n",
      "epoch: 45, batch number: 2751, loss: 1.0831183195114136\n",
      "epoch: 45, batch number: 2752, loss: 0.9707887172698975\n",
      "epoch: 45, batch number: 2753, loss: 0.8595396280288696\n",
      "epoch: 45, batch number: 2754, loss: 0.8327140808105469\n",
      "epoch: 45, batch number: 2755, loss: 0.9389649629592896\n",
      "epoch: 45, batch number: 2756, loss: 0.7194322347640991\n",
      "epoch: 45, batch number: 2757, loss: 1.122761607170105\n",
      "epoch: 45, batch number: 2758, loss: 0.9631224870681763\n",
      "epoch: 45, batch number: 2759, loss: 0.8073673844337463\n",
      "epoch: 45, batch number: 2760, loss: 1.161561369895935\n",
      "epoch: 45, batch number: 2761, loss: 0.8873963356018066\n",
      "epoch: 45, batch number: 2762, loss: 0.818031907081604\n",
      "epoch: 45, batch number: 2763, loss: 0.8347644805908203\n",
      "epoch: 45, batch number: 2764, loss: 0.9792661070823669\n",
      "epoch: 45, batch number: 2765, loss: 0.8691241145133972\n",
      "epoch: 45, batch number: 2766, loss: 0.9579349756240845\n",
      "epoch: 45, batch number: 2767, loss: 0.6555027365684509\n",
      "epoch: 45, batch number: 2768, loss: 0.8664472699165344\n",
      "epoch: 46, batch number: 2769, loss: 0.5180021524429321\n",
      "epoch: 46, batch number: 2770, loss: 0.5453614592552185\n",
      "epoch: 46, batch number: 2771, loss: 0.6155144572257996\n",
      "epoch: 46, batch number: 2772, loss: 0.4746076464653015\n",
      "epoch: 46, batch number: 2773, loss: 0.7325944304466248\n",
      "epoch: 46, batch number: 2774, loss: 0.4618380069732666\n",
      "epoch: 46, batch number: 2775, loss: 0.5692800283432007\n",
      "epoch: 46, batch number: 2776, loss: 0.7308407425880432\n",
      "epoch: 46, batch number: 2777, loss: 0.7581194639205933\n",
      "epoch: 46, batch number: 2778, loss: 0.654064416885376\n",
      "epoch: 46, batch number: 2779, loss: 0.8592691421508789\n",
      "epoch: 46, batch number: 2780, loss: 0.6510486006736755\n",
      "epoch: 46, batch number: 2781, loss: 0.6070574522018433\n",
      "epoch: 46, batch number: 2782, loss: 0.6917257308959961\n",
      "epoch: 46, batch number: 2783, loss: 0.7778730392456055\n",
      "epoch: 46, batch number: 2784, loss: 0.8132394552230835\n",
      "epoch: 46, batch number: 2785, loss: 0.8777509331703186\n",
      "epoch: 46, batch number: 2786, loss: 0.6133582592010498\n",
      "epoch: 46, batch number: 2787, loss: 0.500845730304718\n",
      "epoch: 46, batch number: 2788, loss: 0.589589536190033\n",
      "epoch: 46, batch number: 2789, loss: 0.4680175185203552\n",
      "epoch: 46, batch number: 2790, loss: 0.8725119829177856\n",
      "epoch: 46, batch number: 2791, loss: 0.5870922207832336\n",
      "epoch: 46, batch number: 2792, loss: 0.6960240602493286\n",
      "epoch: 46, batch number: 2793, loss: 0.8240729570388794\n",
      "epoch: 46, batch number: 2794, loss: 0.8218399286270142\n",
      "epoch: 46, batch number: 2795, loss: 0.8973305225372314\n",
      "epoch: 46, batch number: 2796, loss: 0.6611547470092773\n",
      "epoch: 46, batch number: 2797, loss: 0.5173598527908325\n",
      "epoch: 46, batch number: 2798, loss: 0.8923864960670471\n",
      "epoch: 46, batch number: 2799, loss: 0.5584883093833923\n",
      "epoch: 46, batch number: 2800, loss: 0.709304928779602\n",
      "epoch: 46, batch number: 2801, loss: 0.5713562369346619\n",
      "epoch: 46, batch number: 2802, loss: 0.7249018549919128\n",
      "epoch: 46, batch number: 2803, loss: 0.660649836063385\n",
      "epoch: 46, batch number: 2804, loss: 0.5918011665344238\n",
      "epoch: 46, batch number: 2805, loss: 0.6852722764015198\n",
      "epoch: 46, batch number: 2806, loss: 0.6774188876152039\n",
      "epoch: 46, batch number: 2807, loss: 0.8268435597419739\n",
      "epoch: 46, batch number: 2808, loss: 0.554034948348999\n",
      "epoch: 46, batch number: 2809, loss: 0.566925048828125\n",
      "epoch: 46, batch number: 2810, loss: 0.6231504678726196\n",
      "epoch: 46, batch number: 2811, loss: 0.5449060797691345\n",
      "epoch: 46, batch number: 2812, loss: 0.8676289916038513\n",
      "epoch: 46, batch number: 2813, loss: 0.4818284511566162\n",
      "epoch: 46, batch number: 2814, loss: 1.1114882230758667\n",
      "epoch: 46, batch number: 2815, loss: 0.6686724424362183\n",
      "epoch: 46, batch number: 2816, loss: 0.5097158551216125\n",
      "epoch: 46, batch number: 2817, loss: 0.7166057229042053\n",
      "epoch: 46, batch number: 2818, loss: 0.680966317653656\n",
      "epoch: 46, batch number: 2819, loss: 0.540560245513916\n",
      "epoch: 46, batch number: 2820, loss: 0.7710336446762085\n",
      "epoch: 46, batch number: 2821, loss: 0.5328701138496399\n",
      "epoch: 46, batch number: 2822, loss: 0.5921517014503479\n",
      "epoch: 46, batch number: 2823, loss: 0.5037785172462463\n",
      "epoch: 47, batch number: 2824, loss: 0.5479071140289307\n",
      "epoch: 47, batch number: 2825, loss: 0.5110399127006531\n",
      "epoch: 47, batch number: 2826, loss: 0.5446028113365173\n",
      "epoch: 47, batch number: 2827, loss: 0.6368103623390198\n",
      "epoch: 47, batch number: 2828, loss: 0.5688146948814392\n",
      "epoch: 47, batch number: 2829, loss: 0.6091305017471313\n",
      "epoch: 47, batch number: 2830, loss: 0.3597363233566284\n",
      "epoch: 47, batch number: 2831, loss: 0.5811924338340759\n",
      "epoch: 47, batch number: 2832, loss: 0.4998401999473572\n",
      "epoch: 47, batch number: 2833, loss: 0.39772528409957886\n",
      "epoch: 47, batch number: 2834, loss: 0.5782520174980164\n",
      "epoch: 47, batch number: 2835, loss: 0.6103534698486328\n",
      "epoch: 47, batch number: 2836, loss: 0.5679764747619629\n",
      "epoch: 47, batch number: 2837, loss: 0.34064432978630066\n",
      "epoch: 47, batch number: 2838, loss: 0.38918158411979675\n",
      "epoch: 47, batch number: 2839, loss: 0.3993416130542755\n",
      "epoch: 47, batch number: 2840, loss: 0.7631545066833496\n",
      "epoch: 47, batch number: 2841, loss: 0.4342189431190491\n",
      "epoch: 47, batch number: 2842, loss: 0.4401755928993225\n",
      "epoch: 47, batch number: 2843, loss: 0.5786634087562561\n",
      "epoch: 47, batch number: 2844, loss: 0.5722978115081787\n",
      "epoch: 47, batch number: 2845, loss: 0.5358497500419617\n",
      "epoch: 47, batch number: 2846, loss: 0.49307823181152344\n",
      "epoch: 47, batch number: 2847, loss: 0.4582434296607971\n",
      "epoch: 47, batch number: 2848, loss: 0.7665075659751892\n",
      "epoch: 47, batch number: 2849, loss: 0.45125532150268555\n",
      "epoch: 47, batch number: 2850, loss: 0.6935305595397949\n",
      "epoch: 47, batch number: 2851, loss: 0.6219244003295898\n",
      "epoch: 47, batch number: 2852, loss: 0.5600943565368652\n",
      "epoch: 47, batch number: 2853, loss: 0.4446330964565277\n",
      "epoch: 47, batch number: 2854, loss: 0.6096819043159485\n",
      "epoch: 47, batch number: 2855, loss: 0.9565878510475159\n",
      "epoch: 47, batch number: 2856, loss: 0.5518026947975159\n",
      "epoch: 47, batch number: 2857, loss: 0.6826019287109375\n",
      "epoch: 47, batch number: 2858, loss: 0.4241485893726349\n",
      "epoch: 47, batch number: 2859, loss: 0.541296660900116\n",
      "epoch: 47, batch number: 2860, loss: 0.7665756940841675\n",
      "epoch: 47, batch number: 2861, loss: 0.3926222622394562\n",
      "epoch: 47, batch number: 2862, loss: 0.5059754252433777\n",
      "epoch: 47, batch number: 2863, loss: 0.5030394792556763\n",
      "epoch: 47, batch number: 2864, loss: 0.38552698493003845\n",
      "epoch: 47, batch number: 2865, loss: 0.37456125020980835\n",
      "epoch: 47, batch number: 2866, loss: 0.6290105581283569\n",
      "epoch: 47, batch number: 2867, loss: 0.5105378031730652\n",
      "epoch: 47, batch number: 2868, loss: 0.6694626808166504\n",
      "epoch: 47, batch number: 2869, loss: 0.5107596516609192\n",
      "epoch: 47, batch number: 2870, loss: 0.5435606837272644\n",
      "epoch: 47, batch number: 2871, loss: 0.56891268491745\n",
      "epoch: 47, batch number: 2872, loss: 0.750209629535675\n",
      "epoch: 47, batch number: 2873, loss: 0.4338189959526062\n",
      "epoch: 47, batch number: 2874, loss: 0.5553678870201111\n",
      "epoch: 47, batch number: 2875, loss: 0.6901388168334961\n",
      "epoch: 47, batch number: 2876, loss: 0.5095363259315491\n",
      "epoch: 47, batch number: 2877, loss: 0.5327364802360535\n",
      "epoch: 47, batch number: 2878, loss: 0.4227530062198639\n",
      "epoch: 47, batch number: 2879, loss: 0.437015563249588\n",
      "epoch: 47, batch number: 2880, loss: 0.6061145067214966\n",
      "epoch: 47, batch number: 2881, loss: 0.5904532670974731\n",
      "epoch: 47, batch number: 2882, loss: 0.5897789001464844\n",
      "epoch: 47, batch number: 2883, loss: 0.7006297707557678\n",
      "epoch: 47, batch number: 2884, loss: 0.649511456489563\n",
      "epoch: 48, batch number: 2885, loss: 1.8061964511871338\n",
      "epoch: 48, batch number: 2886, loss: 2.921971321105957\n",
      "epoch: 48, batch number: 2887, loss: 2.8114242553710938\n",
      "epoch: 48, batch number: 2888, loss: 1.915398359298706\n",
      "epoch: 48, batch number: 2889, loss: 2.261645793914795\n",
      "epoch: 48, batch number: 2890, loss: 1.817500352859497\n",
      "epoch: 48, batch number: 2891, loss: 2.554227352142334\n",
      "epoch: 48, batch number: 2892, loss: 2.6471309661865234\n",
      "epoch: 48, batch number: 2893, loss: 1.7709486484527588\n",
      "epoch: 48, batch number: 2894, loss: 2.159547805786133\n",
      "epoch: 48, batch number: 2895, loss: 2.634209632873535\n",
      "epoch: 48, batch number: 2896, loss: 1.8323107957839966\n",
      "epoch: 48, batch number: 2897, loss: 1.4059332609176636\n",
      "epoch: 48, batch number: 2898, loss: 2.6098761558532715\n",
      "epoch: 48, batch number: 2899, loss: 1.7709227800369263\n",
      "epoch: 48, batch number: 2900, loss: 1.2468948364257812\n",
      "epoch: 48, batch number: 2901, loss: 1.8884437084197998\n",
      "epoch: 48, batch number: 2902, loss: 1.6653138399124146\n",
      "epoch: 48, batch number: 2903, loss: 3.3490958213806152\n",
      "epoch: 48, batch number: 2904, loss: 1.4596799612045288\n",
      "epoch: 48, batch number: 2905, loss: 2.012615203857422\n",
      "epoch: 48, batch number: 2906, loss: 1.486368179321289\n",
      "epoch: 48, batch number: 2907, loss: 2.41268253326416\n",
      "epoch: 48, batch number: 2908, loss: 2.120598316192627\n",
      "epoch: 48, batch number: 2909, loss: 1.0991145372390747\n",
      "epoch: 48, batch number: 2910, loss: 2.1416666507720947\n",
      "epoch: 48, batch number: 2911, loss: 2.0732710361480713\n",
      "epoch: 48, batch number: 2912, loss: 1.3297876119613647\n",
      "epoch: 48, batch number: 2913, loss: 1.599341630935669\n",
      "epoch: 48, batch number: 2914, loss: 1.6724456548690796\n",
      "epoch: 48, batch number: 2915, loss: 1.5944547653198242\n",
      "epoch: 48, batch number: 2916, loss: 1.7120052576065063\n",
      "epoch: 48, batch number: 2917, loss: 1.9806164503097534\n",
      "epoch: 48, batch number: 2918, loss: 1.7510757446289062\n",
      "epoch: 48, batch number: 2919, loss: 1.7141131162643433\n",
      "epoch: 48, batch number: 2920, loss: 2.1499600410461426\n",
      "epoch: 48, batch number: 2921, loss: 1.240897297859192\n",
      "epoch: 48, batch number: 2922, loss: 1.6697452068328857\n",
      "epoch: 48, batch number: 2923, loss: 1.3376598358154297\n",
      "epoch: 48, batch number: 2924, loss: 1.6173186302185059\n",
      "epoch: 48, batch number: 2925, loss: 1.2403041124343872\n",
      "epoch: 48, batch number: 2926, loss: 1.8906100988388062\n",
      "epoch: 48, batch number: 2927, loss: 1.6177313327789307\n",
      "epoch: 48, batch number: 2928, loss: 1.4337115287780762\n",
      "epoch: 48, batch number: 2929, loss: 1.4169565439224243\n",
      "epoch: 48, batch number: 2930, loss: 1.6718711853027344\n",
      "epoch: 48, batch number: 2931, loss: 1.2557810544967651\n",
      "epoch: 48, batch number: 2932, loss: 1.4800487756729126\n",
      "epoch: 48, batch number: 2933, loss: 1.422082781791687\n",
      "epoch: 48, batch number: 2934, loss: 1.4848028421401978\n",
      "epoch: 48, batch number: 2935, loss: 1.268494725227356\n",
      "epoch: 48, batch number: 2936, loss: 1.7920254468917847\n",
      "epoch: 48, batch number: 2937, loss: 1.463795781135559\n",
      "epoch: 48, batch number: 2938, loss: 1.6290030479431152\n",
      "epoch: 48, batch number: 2939, loss: 1.2631953954696655\n",
      "epoch: 48, batch number: 2940, loss: 1.0835576057434082\n",
      "epoch: 48, batch number: 2941, loss: 1.1277884244918823\n",
      "epoch: 48, batch number: 2942, loss: 1.5860295295715332\n",
      "epoch: 48, batch number: 2943, loss: 1.4423315525054932\n",
      "epoch: 48, batch number: 2944, loss: 1.2153294086456299\n",
      "epoch: 48, batch number: 2945, loss: 1.4912203550338745\n",
      "epoch: 48, batch number: 2946, loss: 1.1637548208236694\n",
      "epoch: 48, batch number: 2947, loss: 1.0649434328079224\n",
      "epoch: 49, batch number: 2948, loss: 1.5844626426696777\n",
      "epoch: 49, batch number: 2949, loss: 1.2896562814712524\n",
      "epoch: 49, batch number: 2950, loss: 1.413366675376892\n",
      "epoch: 49, batch number: 2951, loss: 1.2780704498291016\n",
      "epoch: 49, batch number: 2952, loss: 1.375005841255188\n",
      "epoch: 49, batch number: 2953, loss: 1.584262490272522\n",
      "epoch: 49, batch number: 2954, loss: 1.4125431776046753\n",
      "epoch: 49, batch number: 2955, loss: 1.5083775520324707\n",
      "epoch: 49, batch number: 2956, loss: 1.2622510194778442\n",
      "epoch: 49, batch number: 2957, loss: 1.4423223733901978\n",
      "epoch: 49, batch number: 2958, loss: 1.3413492441177368\n",
      "epoch: 49, batch number: 2959, loss: 1.679774284362793\n",
      "epoch: 49, batch number: 2960, loss: 1.7770054340362549\n",
      "epoch: 49, batch number: 2961, loss: 1.4913665056228638\n",
      "epoch: 49, batch number: 2962, loss: 1.296743392944336\n",
      "epoch: 49, batch number: 2963, loss: 1.2133992910385132\n",
      "epoch: 49, batch number: 2964, loss: 1.351426362991333\n",
      "epoch: 49, batch number: 2965, loss: 1.0590711832046509\n",
      "epoch: 49, batch number: 2966, loss: 1.4399365186691284\n",
      "epoch: 49, batch number: 2967, loss: 0.9646291136741638\n",
      "epoch: 49, batch number: 2968, loss: 1.0116223096847534\n",
      "epoch: 49, batch number: 2969, loss: 1.250733494758606\n",
      "epoch: 49, batch number: 2970, loss: 1.1459095478057861\n",
      "epoch: 49, batch number: 2971, loss: 1.0862075090408325\n",
      "epoch: 49, batch number: 2972, loss: 1.2680071592330933\n",
      "epoch: 49, batch number: 2973, loss: 1.1549400091171265\n",
      "epoch: 49, batch number: 2974, loss: 1.2002143859863281\n",
      "epoch: 49, batch number: 2975, loss: 1.361989140510559\n",
      "epoch: 49, batch number: 2976, loss: 1.1902815103530884\n",
      "epoch: 49, batch number: 2977, loss: 1.2119741439819336\n",
      "epoch: 49, batch number: 2978, loss: 1.4152222871780396\n",
      "epoch: 49, batch number: 2979, loss: 1.377342939376831\n",
      "epoch: 49, batch number: 2980, loss: 1.1187254190444946\n",
      "epoch: 49, batch number: 2981, loss: 1.0931628942489624\n",
      "epoch: 49, batch number: 2982, loss: 1.1235055923461914\n",
      "epoch: 49, batch number: 2983, loss: 1.072058081626892\n",
      "epoch: 49, batch number: 2984, loss: 1.2044203281402588\n",
      "epoch: 49, batch number: 2985, loss: 1.0378161668777466\n",
      "epoch: 49, batch number: 2986, loss: 1.0315455198287964\n",
      "epoch: 49, batch number: 2987, loss: 1.181149959564209\n",
      "epoch: 49, batch number: 2988, loss: 0.9819387197494507\n",
      "epoch: 49, batch number: 2989, loss: 1.1230970621109009\n",
      "epoch: 49, batch number: 2990, loss: 1.1079996824264526\n",
      "epoch: 49, batch number: 2991, loss: 1.0238341093063354\n",
      "epoch: 49, batch number: 2992, loss: 1.053843379020691\n",
      "epoch: 49, batch number: 2993, loss: 1.0573424100875854\n",
      "epoch: 49, batch number: 2994, loss: 1.2066357135772705\n",
      "epoch: 49, batch number: 2995, loss: 1.3381800651550293\n",
      "epoch: 49, batch number: 2996, loss: 0.9923884868621826\n",
      "epoch: 49, batch number: 2997, loss: 1.0615794658660889\n",
      "epoch: 49, batch number: 2998, loss: 1.254847764968872\n",
      "epoch: 49, batch number: 2999, loss: 1.0436357259750366\n",
      "epoch: 49, batch number: 3000, loss: 1.1843024492263794\n",
      "epoch: 49, batch number: 3001, loss: 1.0220621824264526\n",
      "epoch: 49, batch number: 3002, loss: 0.9935138821601868\n",
      "epoch: 49, batch number: 3003, loss: 1.119991421699524\n",
      "epoch: 49, batch number: 3004, loss: 1.0050350427627563\n",
      "epoch: 49, batch number: 3005, loss: 1.044959306716919\n",
      "epoch: 49, batch number: 3006, loss: 1.117727279663086\n",
      "epoch: 49, batch number: 3007, loss: 1.1703932285308838\n",
      "epoch: 49, batch number: 3008, loss: 1.001190185546875\n",
      "epoch: 49, batch number: 3009, loss: 0.9942293167114258\n",
      "epoch: 50, batch number: 3010, loss: 1.0889707803726196\n",
      "epoch: 50, batch number: 3011, loss: 1.0766501426696777\n",
      "epoch: 50, batch number: 3012, loss: 1.015167236328125\n",
      "epoch: 50, batch number: 3013, loss: 1.1519196033477783\n",
      "epoch: 50, batch number: 3014, loss: 1.210245966911316\n",
      "epoch: 50, batch number: 3015, loss: 0.9745730757713318\n",
      "epoch: 50, batch number: 3016, loss: 1.0436290502548218\n",
      "epoch: 50, batch number: 3017, loss: 0.9304744005203247\n",
      "epoch: 50, batch number: 3018, loss: 0.9827708601951599\n",
      "epoch: 50, batch number: 3019, loss: 1.0469212532043457\n",
      "epoch: 50, batch number: 3020, loss: 0.9269173741340637\n",
      "epoch: 50, batch number: 3021, loss: 1.0976548194885254\n",
      "epoch: 50, batch number: 3022, loss: 0.8546096086502075\n",
      "epoch: 50, batch number: 3023, loss: 1.0725189447402954\n",
      "epoch: 50, batch number: 3024, loss: 0.8762676119804382\n",
      "epoch: 50, batch number: 3025, loss: 1.1218876838684082\n",
      "epoch: 50, batch number: 3026, loss: 1.2247138023376465\n",
      "epoch: 50, batch number: 3027, loss: 0.9976706504821777\n",
      "epoch: 50, batch number: 3028, loss: 1.1317659616470337\n",
      "epoch: 50, batch number: 3029, loss: 0.9250175952911377\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m batch_num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[39m# print progress\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, batch number: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, batch_num, loss\u001b[39m.\u001b[39;49mitem()))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Train the model on your training dataset\n",
    "num_epochs = 1000\n",
    "batch_num = 0\n",
    "loop_count = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        # get the next chunk of data\n",
    "        train_dataset = TrainingData(next(data), tokenizer)\n",
    "    except StopIteration:\n",
    "        data = get_data()\n",
    "        loop_count += 1\n",
    "        train_dataset = TrainingData(next(data), tokenizer)\n",
    "\n",
    "    # data loader for dataset\n",
    "    batch_size = 32\n",
    "    data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for batch in data_loader:\n",
    "        # Get the input and label tensors for this batch\n",
    "        inputs, labels = batch\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        # Move the input and label tensors to the GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradients for this batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass the inputs through the model and compute the logits\n",
    "        logits = model(inputs)[0]\n",
    "\n",
    "        # Compute the loss for this batch\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backpropagate the loss and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_num += 1\n",
    "\n",
    "        # print progress\n",
    "        print('epoch: {}, batch number: {}, loss: {}'.format(epoch, batch_num, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv('E:/ML/DS_fake_news/fake_news_cleaned.csv', usecols=['type'], chunksize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m inputs, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     16\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mLongTensor)\n\u001b[1;32m---> 18\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     19\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m logits \u001b[39m=\u001b[39m model(inputs)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test dataset\n",
    "test = pd.read_csv('E:/ML/DS_fake_news/fake_news_cleaned.csv', usecols=['content', 'type'], nrows=2000)\n",
    "test = test[test['type'] != 'unknown']\n",
    "test = test[test['type'].notna()]\n",
    "test['type'] = test['type'].map(cat_dict)\n",
    "\n",
    "test_dataset = TrainingData(test, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(inputs)[0]\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# save the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49msave_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mE:/ML/DS_fake_news/models/distilbert\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\transformers\\modeling_utils.py:1758\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, **kwargs)\u001b[0m\n\u001b[0;32m   1756\u001b[0m         safe_save_file(shard, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m   1757\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1758\u001b[0m         save_function(shard, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_directory, shard_file))\n\u001b[0;32m   1760\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1761\u001b[0m     path_to_weights \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\torch\\serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\torch\\serialization.py:665\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[39m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[39m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[39mif\u001b[39;00m storage\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 665\u001b[0m     storage \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39;49mcpu()\n\u001b[0;32m    666\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    667\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\torch\\storage.py:121\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mUntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39;49mcopy_(\u001b[39mself\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save_pretrained('E:/ML/DS_fake_news/models/distilbert')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeNewsProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(input, answers, test_size=0.1, val_size=0.1):\n",
    "    # split the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input, answers, test_size=test_size+val_size, random_state=42, stratify=answers)\n",
    "    # split the test data into test and validation\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=val_size/(test_size+val_size), random_state=42, stratify=y_test)\n",
    "    return X_train, X_test, y_train, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#loading data\n",
    "skiprows = pd.read_parquet('E:/ML/DS_fake_news/skip_list.parquet')\n",
    "df = pd.read_csv('E:/ML/DS_fake_news/fake_news_cleaned.csv', nrows=10000, usecols=['content', 'type'], skiprows=skiprows['bad_row_index'].tolist())\n",
    "df = df[df['content'].apply(lambda x: isinstance(x, str))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clean the data\n",
    "from cleantext import clean\n",
    "import swifter\n",
    "def clean_text(s):\n",
    "    return clean(s,lower=True,                     # lowercase text\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,               # replace all numbers with a special token\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_number=\"<NUM>\",\n",
    "        lang=\"en\"                   \n",
    "    )\n",
    "\n",
    "from os import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "# clean the text\n",
    "def clean_column(series):\n",
    "    # parallelized operation\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(clean_text)(s) for s in series)\n",
    "\n",
    "df['content'] = clean_column(df['content'])\n",
    "\n",
    "# tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "def tokenize_column(series):\n",
    "    # parallelized operation\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(word_tokenize)(s) for s in series)\n",
    "\n",
    "df['content'] = pd.Series(tokenize_column(df['content']))\n",
    "\n",
    "# stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(s):\n",
    "    return [w for w in s if not w in stop_words]\n",
    "\n",
    "def remove_stopwords_column(series):\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(remove_stopwords)(s) for s in series)\n",
    "\n",
    "df['content'] = pd.Series(remove_stopwords_column(df['content']))\n",
    "\n",
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(s):\n",
    "    return [lemmatizer.lemmatize(w) for w in s]\n",
    "def lemmatize_column(series):\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(lemmatize)(s) for s in series)\n",
    "\n",
    "df['content'] = pd.Series(lemmatize_column(df['content']))\n",
    "\n",
    "# remove punctuation\n",
    "import string\n",
    "def remove_punctuation(s):\n",
    "    return [w for w in s if w not in string.punctuation]\n",
    "\n",
    "def remove_punctuation_column(series):\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(remove_punctuation)(s) for s in series)\n",
    "\n",
    "df['content'] = pd.Series(remove_punctuation_column(df['content']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a word embedding for each word in the vocab\n",
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "vocab = pd.read_pickle('E:/ML/DS_fake_news/vocab.pkl')\n",
    "# filter out words that appear less than 2000 times\n",
    "vocab = [(word, count) for word, count in vocab if count > 2000]\n",
    "# add stopwords to the vocab\n",
    "#from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#for word in stop_words:\n",
    "#    vocab.append((word, 0))\n",
    "\n",
    "vocab = [word for word, count in vocab]\n",
    "word_to_ix = {vocab[i]: i for i in range(len(vocab))}\n",
    "vocab = set(vocab)\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "# computing ngrams\n",
    "def ngrams(input, n):\n",
    "    ngrams = [\n",
    "        (\n",
    "            [input[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "            input[i]\n",
    "        )\n",
    "        for i in range(CONTEXT_SIZE, len(input))]\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# define the model\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        #print embedding properties\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# define loss function\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "# run loss function on gpu\n",
    "#if torch.cuda.is_available():\n",
    "#    loss_function.cuda()\n",
    "\n",
    "# initialize the model\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# gpu support\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "#    print('Using GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least\n",
      "quantum\n",
      "level\n",
      "theory\n",
      "recently\n",
      "confirmed\n",
      "set\n",
      "researcher\n",
      "finally\n",
      "mean\n",
      "test\n",
      "john\n",
      "wheeler\n",
      "'s\n",
      "delayed-choice\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'delayed-choice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Step 4. Compute your loss function. (Again, Torch wants the target\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# word wrapped in a tensor)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(target)\n\u001b[1;32m---> 21\u001b[0m loss \u001b[39m=\u001b[39m loss_function(log_probs, torch\u001b[39m.\u001b[39mtensor([word_to_ix[target]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong))\n\u001b[0;32m     23\u001b[0m \u001b[39m# Step 5. Do the backward pass and update the gradient\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'delayed-choice'"
     ]
    }
   ],
   "source": [
    "\n",
    "# train the model\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams(df['content'][0], CONTEXT_SIZE):\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words using cuda\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        print(target)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[word_to_ix[\"trump\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data\n",
    "X_train, X_test, y_train, y_test, X_val, y_val = split_data(bow, df['type'], test_size=0.1, val_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi layer classifier with pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# convert to tensors\n",
    "X_train = torch.from_numpy(X_train.toarray()).float()\n",
    "X_test = torch.from_numpy(X_test.toarray()).float()\n",
    "X_val = torch.from_numpy(X_val.toarray()).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()\n",
    "y_val = torch.from_numpy(y_val.values).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "# print where the model is running on (cpu or gpu)\n",
    "print('running on', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# train model\n",
    "batch_size = 100\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(train_loader)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeNewsProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

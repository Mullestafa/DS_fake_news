{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data import RandomSampler\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator(csv_file='E:/ML/DS_fake_news/fake_news_cleaned.csv'):\n",
    "    data = pd.read_csv(csv_file, usecols=['content', 'type'], chunksize=10000)\n",
    "    label_map = {'bias': 0,\n",
    "                        'clickbait': 0,\n",
    "                        'conspiracy': 0,\n",
    "                        'fake': 1,\n",
    "                        'hate': 1,\n",
    "                        'junksci': 1,\n",
    "                        'political': 0,\n",
    "                        'reliable': 0,\n",
    "                        'rumor': 0,\n",
    "                        'satire': 0,\n",
    "                        'unreliable': 1}\n",
    "    for chunk in data:\n",
    "        # throw away rows with missing type\n",
    "        chunk = chunk.dropna(subset=['type'])\n",
    "        # drop rows with 'unknown' type\n",
    "        chunk = chunk[chunk['type'] != 'unknown']\n",
    "        chunk['type'] = chunk['type'].map(label_map)\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=2**20, stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_iterator()\n",
    "\n",
    "for chunk in data:\n",
    "    # transform the text to tf-idf\n",
    "    tfidf = vectorizer.transform(chunk['content'])\n",
    "    labels = chunk['type']\n",
    "    # convert labels to tensor\n",
    "    labels = torch.tensor(labels.values)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chunk 2\n",
    "for chunk in data:\n",
    "    # transform the text to tf-idf\n",
    "    tfidf = vectorizer.transform(chunk['content'])\n",
    "    labels = chunk['type']\n",
    "    # convert labels to tensor\n",
    "    labels = torch.tensor(labels.values)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = tfidf\n",
    "y_data = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias': 4908, 'clickbait': 4983}\n"
     ]
    }
   ],
   "source": [
    "label_map = {'bias': 0,\n",
    "                        'clickbait': 1,\n",
    "                        'conspiracy': 2,\n",
    "                        'fake': 3,\n",
    "                        'hate': 4,\n",
    "                        'junksci': 5,\n",
    "                        'political': 6,\n",
    "                        'reliable': 7,\n",
    "                        'rumor': 8,\n",
    "                        'satire': 9,\n",
    "                        'unreliable': 10}\n",
    "# flip label_map\n",
    "label_map = {v: k for k, v in label_map.items()}\n",
    "# statistics with label names\n",
    "label_count = Counter(y_data.numpy())\n",
    "# map keys to label dict\n",
    "label_count = {label_map[k]: v for k, v in label_count.items()}\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 4983, 0: 4908})\n",
      "Counter({1: 4983, 0: 4908})\n"
     ]
    }
   ],
   "source": [
    "label_count = Counter(y_data.numpy())\n",
    "print(label_count)\n",
    "\n",
    "# remove labels with less than 2 samples\n",
    "for label in label_count:\n",
    "    if label_count[label] < 11:\n",
    "        X_data = X_data[y_data != label]\n",
    "        y_data = y_data[y_data != label]\n",
    "\n",
    "label_count = Counter(y_data.numpy())\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to coo matrix\n",
    "X_train = X_train.tocoo()\n",
    "X_test = X_test.tocoo()\n",
    "X_val = X_val.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musta\\AppData\\Local\\Temp\\ipykernel_12888\\2848747606.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  X_train = torch.sparse_coo_tensor(torch.LongTensor([X_train.row, X_train.col]), torch.FloatTensor(X_train.data), X_train.shape).to(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# convert to torch tensors\n",
    "X_train = torch.sparse_coo_tensor(torch.LongTensor([X_train.row, X_train.col]), torch.FloatTensor(X_train.data), X_train.shape).to(device)\n",
    "X_test = torch.sparse_coo_tensor(torch.LongTensor([X_test.row, X_test.col]), torch.FloatTensor(X_test.data), X_test.shape).to(device)\n",
    "X_val = torch.sparse_coo_tensor(torch.LongTensor([X_val.row, X_val.col]), torch.FloatTensor(X_val.data), X_val.shape).to(device)\n",
    "\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "y_val = y_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "             nn.Linear(X_train.shape[1], 64),\n",
    "             nn.ReLU(),\n",
    "             nn.Dropout(0.2),\n",
    "             nn.Linear(64, len(set(y_train))),\n",
    "             nn.LogSoftmax(dim=1)).to(device)\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "# Forward pass, log  \n",
    "logps = model(X_train)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logps, y_train)\n",
    "loss.backward()\n",
    "# Optimizers need parameters to optimize and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100..  Train loss: 2.423..  Test loss: 2.210..  Test accuracy: 0.148\n",
      "Epoch 2/100..  Train loss: 2.225..  Test loss: 1.955..  Test accuracy: 0.280\n",
      "Epoch 3/100..  Train loss: 1.961..  Test loss: 1.692..  Test accuracy: 0.632\n",
      "Epoch 4/100..  Train loss: 1.687..  Test loss: 1.461..  Test accuracy: 0.693\n",
      "Epoch 5/100..  Train loss: 1.445..  Test loss: 1.282..  Test accuracy: 0.747\n",
      "Epoch 6/100..  Train loss: 1.254..  Test loss: 1.153..  Test accuracy: 0.768\n",
      "Epoch 7/100..  Train loss: 1.114..  Test loss: 1.061..  Test accuracy: 0.774\n",
      "Epoch 8/100..  Train loss: 1.010..  Test loss: 0.990..  Test accuracy: 0.792\n",
      "Epoch 9/100..  Train loss: 0.927..  Test loss: 0.933..  Test accuracy: 0.793\n",
      "Epoch 10/100..  Train loss: 0.857..  Test loss: 0.884..  Test accuracy: 0.797\n",
      "Epoch 11/100..  Train loss: 0.796..  Test loss: 0.841..  Test accuracy: 0.798\n",
      "Epoch 12/100..  Train loss: 0.740..  Test loss: 0.803..  Test accuracy: 0.803\n",
      "Epoch 13/100..  Train loss: 0.690..  Test loss: 0.770..  Test accuracy: 0.806\n",
      "Epoch 14/100..  Train loss: 0.644..  Test loss: 0.739..  Test accuracy: 0.811\n",
      "Epoch 15/100..  Train loss: 0.601..  Test loss: 0.710..  Test accuracy: 0.810\n",
      "Epoch 16/100..  Train loss: 0.560..  Test loss: 0.681..  Test accuracy: 0.806\n",
      "Epoch 17/100..  Train loss: 0.520..  Test loss: 0.653..  Test accuracy: 0.810\n",
      "Epoch 18/100..  Train loss: 0.481..  Test loss: 0.626..  Test accuracy: 0.812\n",
      "Epoch 19/100..  Train loss: 0.443..  Test loss: 0.600..  Test accuracy: 0.818\n",
      "Epoch 20/100..  Train loss: 0.408..  Test loss: 0.576..  Test accuracy: 0.824\n",
      "Epoch 21/100..  Train loss: 0.375..  Test loss: 0.555..  Test accuracy: 0.825\n",
      "Epoch 22/100..  Train loss: 0.346..  Test loss: 0.537..  Test accuracy: 0.835\n",
      "Epoch 23/100..  Train loss: 0.321..  Test loss: 0.522..  Test accuracy: 0.838\n",
      "Epoch 24/100..  Train loss: 0.300..  Test loss: 0.511..  Test accuracy: 0.840\n",
      "Epoch 25/100..  Train loss: 0.282..  Test loss: 0.502..  Test accuracy: 0.842\n",
      "Epoch 26/100..  Train loss: 0.267..  Test loss: 0.495..  Test accuracy: 0.844\n",
      "Epoch 27/100..  Train loss: 0.255..  Test loss: 0.489..  Test accuracy: 0.846\n",
      "Epoch 28/100..  Train loss: 0.244..  Test loss: 0.484..  Test accuracy: 0.852\n",
      "Epoch 29/100..  Train loss: 0.234..  Test loss: 0.480..  Test accuracy: 0.853\n",
      "Epoch 30/100..  Train loss: 0.225..  Test loss: 0.476..  Test accuracy: 0.852\n",
      "Epoch 31/100..  Train loss: 0.216..  Test loss: 0.472..  Test accuracy: 0.854\n",
      "Epoch 32/100..  Train loss: 0.207..  Test loss: 0.467..  Test accuracy: 0.858\n",
      "Epoch 33/100..  Train loss: 0.198..  Test loss: 0.463..  Test accuracy: 0.860\n",
      "Epoch 34/100..  Train loss: 0.190..  Test loss: 0.458..  Test accuracy: 0.862\n",
      "Epoch 35/100..  Train loss: 0.181..  Test loss: 0.454..  Test accuracy: 0.862\n",
      "Epoch 36/100..  Train loss: 0.173..  Test loss: 0.450..  Test accuracy: 0.862\n",
      "Epoch 37/100..  Train loss: 0.165..  Test loss: 0.446..  Test accuracy: 0.862\n",
      "Epoch 38/100..  Train loss: 0.157..  Test loss: 0.442..  Test accuracy: 0.861\n",
      "Epoch 39/100..  Train loss: 0.150..  Test loss: 0.439..  Test accuracy: 0.859\n",
      "Epoch 40/100..  Train loss: 0.143..  Test loss: 0.436..  Test accuracy: 0.862\n",
      "Epoch 41/100..  Train loss: 0.137..  Test loss: 0.434..  Test accuracy: 0.863\n",
      "Epoch 42/100..  Train loss: 0.132..  Test loss: 0.432..  Test accuracy: 0.863\n",
      "Epoch 43/100..  Train loss: 0.127..  Test loss: 0.431..  Test accuracy: 0.859\n",
      "Epoch 44/100..  Train loss: 0.122..  Test loss: 0.429..  Test accuracy: 0.859\n",
      "Epoch 45/100..  Train loss: 0.118..  Test loss: 0.428..  Test accuracy: 0.860\n",
      "Epoch 46/100..  Train loss: 0.114..  Test loss: 0.427..  Test accuracy: 0.862\n",
      "Epoch 47/100..  Train loss: 0.110..  Test loss: 0.426..  Test accuracy: 0.865\n",
      "Epoch 48/100..  Train loss: 0.107..  Test loss: 0.425..  Test accuracy: 0.867\n",
      "Epoch 49/100..  Train loss: 0.104..  Test loss: 0.424..  Test accuracy: 0.867\n",
      "Epoch 50/100..  Train loss: 0.101..  Test loss: 0.423..  Test accuracy: 0.867\n",
      "Epoch 51/100..  Train loss: 0.098..  Test loss: 0.422..  Test accuracy: 0.867\n",
      "Epoch 52/100..  Train loss: 0.095..  Test loss: 0.421..  Test accuracy: 0.867\n",
      "Epoch 53/100..  Train loss: 0.092..  Test loss: 0.420..  Test accuracy: 0.867\n",
      "Epoch 54/100..  Train loss: 0.089..  Test loss: 0.418..  Test accuracy: 0.868\n",
      "Epoch 55/100..  Train loss: 0.087..  Test loss: 0.417..  Test accuracy: 0.868\n",
      "Epoch 56/100..  Train loss: 0.084..  Test loss: 0.416..  Test accuracy: 0.868\n",
      "Epoch 57/100..  Train loss: 0.082..  Test loss: 0.414..  Test accuracy: 0.868\n",
      "Epoch 58/100..  Train loss: 0.080..  Test loss: 0.413..  Test accuracy: 0.868\n",
      "Epoch 59/100..  Train loss: 0.078..  Test loss: 0.412..  Test accuracy: 0.867\n",
      "Epoch 60/100..  Train loss: 0.075..  Test loss: 0.411..  Test accuracy: 0.867\n",
      "Epoch 61/100..  Train loss: 0.073..  Test loss: 0.410..  Test accuracy: 0.867\n",
      "Epoch 62/100..  Train loss: 0.072..  Test loss: 0.408..  Test accuracy: 0.867\n",
      "Epoch 63/100..  Train loss: 0.070..  Test loss: 0.407..  Test accuracy: 0.867\n",
      "Epoch 64/100..  Train loss: 0.068..  Test loss: 0.407..  Test accuracy: 0.868\n",
      "Epoch 65/100..  Train loss: 0.067..  Test loss: 0.406..  Test accuracy: 0.868\n",
      "Epoch 66/100..  Train loss: 0.065..  Test loss: 0.405..  Test accuracy: 0.868\n",
      "Epoch 67/100..  Train loss: 0.064..  Test loss: 0.404..  Test accuracy: 0.868\n",
      "Epoch 68/100..  Train loss: 0.062..  Test loss: 0.403..  Test accuracy: 0.869\n",
      "Epoch 69/100..  Train loss: 0.061..  Test loss: 0.403..  Test accuracy: 0.869\n",
      "Epoch 70/100..  Train loss: 0.060..  Test loss: 0.402..  Test accuracy: 0.869\n",
      "Epoch 71/100..  Train loss: 0.058..  Test loss: 0.402..  Test accuracy: 0.869\n",
      "Epoch 72/100..  Train loss: 0.057..  Test loss: 0.401..  Test accuracy: 0.869\n",
      "Epoch 73/100..  Train loss: 0.056..  Test loss: 0.401..  Test accuracy: 0.869\n",
      "Epoch 74/100..  Train loss: 0.055..  Test loss: 0.400..  Test accuracy: 0.869\n",
      "Epoch 75/100..  Train loss: 0.054..  Test loss: 0.400..  Test accuracy: 0.872\n",
      "Epoch 76/100..  Train loss: 0.053..  Test loss: 0.399..  Test accuracy: 0.869\n",
      "Epoch 77/100..  Train loss: 0.052..  Test loss: 0.399..  Test accuracy: 0.869\n",
      "Epoch 78/100..  Train loss: 0.051..  Test loss: 0.399..  Test accuracy: 0.870\n",
      "Epoch 79/100..  Train loss: 0.050..  Test loss: 0.398..  Test accuracy: 0.870\n",
      "Epoch 80/100..  Train loss: 0.049..  Test loss: 0.398..  Test accuracy: 0.870\n",
      "Epoch 81/100..  Train loss: 0.048..  Test loss: 0.398..  Test accuracy: 0.870\n",
      "Epoch 82/100..  Train loss: 0.047..  Test loss: 0.398..  Test accuracy: 0.870\n",
      "Epoch 83/100..  Train loss: 0.047..  Test loss: 0.397..  Test accuracy: 0.870\n",
      "Epoch 84/100..  Train loss: 0.046..  Test loss: 0.397..  Test accuracy: 0.870\n",
      "Epoch 85/100..  Train loss: 0.045..  Test loss: 0.397..  Test accuracy: 0.872\n",
      "Epoch 86/100..  Train loss: 0.044..  Test loss: 0.397..  Test accuracy: 0.873\n",
      "Epoch 87/100..  Train loss: 0.044..  Test loss: 0.396..  Test accuracy: 0.873\n",
      "Epoch 88/100..  Train loss: 0.043..  Test loss: 0.396..  Test accuracy: 0.874\n",
      "Epoch 89/100..  Train loss: 0.042..  Test loss: 0.396..  Test accuracy: 0.874\n",
      "Epoch 90/100..  Train loss: 0.042..  Test loss: 0.396..  Test accuracy: 0.875\n",
      "Epoch 91/100..  Train loss: 0.041..  Test loss: 0.396..  Test accuracy: 0.875\n",
      "Epoch 92/100..  Train loss: 0.040..  Test loss: 0.395..  Test accuracy: 0.875\n",
      "Epoch 93/100..  Train loss: 0.040..  Test loss: 0.395..  Test accuracy: 0.875\n",
      "Epoch 94/100..  Train loss: 0.039..  Test loss: 0.395..  Test accuracy: 0.875\n",
      "Epoch 95/100..  Train loss: 0.039..  Test loss: 0.395..  Test accuracy: 0.875\n",
      "Epoch 96/100..  Train loss: 0.038..  Test loss: 0.395..  Test accuracy: 0.875\n",
      "Epoch 97/100..  Train loss: 0.037..  Test loss: 0.395..  Test accuracy: 0.875\n",
      "Epoch 98/100..  Train loss: 0.037..  Test loss: 0.395..  Test accuracy: 0.875\n",
      "Epoch 99/100..  Train loss: 0.036..  Test loss: 0.394..  Test accuracy: 0.875\n",
      "Epoch 100/100..  Train loss: 0.036..  Test loss: 0.394..  Test accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model.forward(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        log_ps = model(X_test)\n",
    "        test_loss = criterion(log_ps, y_test)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == y_test.view(*top_class.shape)\n",
    "        test_accuracy = torch.mean(equals.float())\n",
    "    \n",
    "    print(f\"Epoch {e+1}/{epochs}.. \",\n",
    "            f\"Train loss: {loss:.3f}.. \",\n",
    "            f\"Test loss: {test_loss:.3f}.. \",\n",
    "            f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./models/tf-idf-{chunk}.pth')\n",
    "        print(f'Saved model at chunk {chunk}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model from file\n",
    "model.load_state_dict(torch.load('./tf-idf-100.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    log_ps = model(X_train)\n",
    "    test_loss = criterion(log_ps, y_train)\n",
    "    ps = torch.exp(log_ps)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == y_train.view(*top_class.shape)\n",
    "    test_accuracy = torch.mean(equals.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.358..  Test accuracy: 0.507\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {test_loss:.3f}.. \", f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "# %%\n",
    "def data_iterator(csv_file='E:/ML/DS_fake_news/fake_news_cleaned.csv'):\n",
    "    data = pd.read_csv(csv_file, usecols=['content', 'type'], chunksize=2000)\n",
    "    label_map = {'bias': 0,\n",
    "                        'clickbait': 0,\n",
    "                        'conspiracy': 0,\n",
    "                        'fake': 1,\n",
    "                        'hate': 1,\n",
    "                        'junksci': 1,\n",
    "                        'political': 0,\n",
    "                        'reliable': 0,\n",
    "                        'rumor': 0,\n",
    "                        'satire': 0,\n",
    "                        'unreliable': 1}\n",
    "    for chunk in data:\n",
    "        # throw away rows with missing type\n",
    "        chunk = chunk.dropna(subset=['type'])\n",
    "        # drop rows with 'unknown' type\n",
    "        chunk = chunk[chunk['type'].isin(label_map.keys())]\n",
    "        chunk['type'] = chunk['type'].map(label_map)\n",
    "        yield chunk\n",
    "\n",
    "# %%\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=2**20, stop_words='english')\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "data = data_iterator()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "first = True\n",
    "chunk_num = 0\n",
    "for chunk in data:\n",
    "    # transform the text to tf-idf\n",
    "    try:\n",
    "        tfidf = vectorizer.transform(chunk['content'])\n",
    "    except:\n",
    "        chunk_num += 1\n",
    "        continue\n",
    "    labels = chunk['type']\n",
    "    # convert labels to tensor\n",
    "    labels = torch.tensor(labels.values)\n",
    "\n",
    "\n",
    "    # %%\n",
    "    X_data = tfidf\n",
    "    y_data = labels\n",
    "\n",
    "    # %%\n",
    "    # count each label\n",
    "    label_count = Counter(y_data.numpy())\n",
    "\n",
    "    # remove labels with less than 2 samples\n",
    "    for label in label_count:\n",
    "        if label_count[label] < 2:\n",
    "            X_data = X_data[y_data != label]\n",
    "            y_data = y_data[y_data != label]\n",
    "    label_count = Counter(y_data.numpy())\n",
    "    # check if any of the labels have 0 samples\n",
    "    if len(label_count) < 2:\n",
    "        chunk_num += 1\n",
    "        continue\n",
    "    # if the two classes are not balanced, skip chunk\n",
    "    label_count = Counter(y_data.numpy())\n",
    "    print(label_count)\n",
    "    if label_count[0]/label_count[1] < 0.5 or label_count[0]/ label_count[1] > 2:\n",
    "        chunk_num += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(Counter(y_data.numpy()))\n",
    "\n",
    "    # %%\n",
    "    # split the dataset using sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "\n",
    "    # %%\n",
    "    # convert to coo matrix\n",
    "    X_train = X_train.tocoo()\n",
    "    X_test = X_test.tocoo()\n",
    "    X_val = X_val.tocoo()\n",
    "\n",
    "    # %%\n",
    "\n",
    "    # convert to torch tensors\n",
    "    X_train = torch.sparse_coo_tensor(torch.LongTensor([X_train.row, X_train.col]), torch.FloatTensor(X_train.data), X_train.shape).to(device)\n",
    "    X_test = torch.sparse_coo_tensor(torch.LongTensor([X_test.row, X_test.col]), torch.FloatTensor(X_test.data), X_test.shape).to(device)\n",
    "    X_val = torch.sparse_coo_tensor(torch.LongTensor([X_val.row, X_val.col]), torch.FloatTensor(X_val.data), X_val.shape).to(device)\n",
    "\n",
    "    y_train = y_train.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "\n",
    "    # %%\n",
    "    # run once only\n",
    "\n",
    "    if first:\n",
    "        model = nn.Sequential(\n",
    "                    nn.Linear(X_train.shape[1], 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(64, len(set(y_train))),\n",
    "                    nn.LogSoftmax(dim=1)).to(device)\n",
    "        # Define the loss\n",
    "        criterion = nn.NLLLoss()\n",
    "        # Forward pass, log  \n",
    "        logps = model(X_train)\n",
    "        # Calculate the loss with the logits and the labels\n",
    "        loss = criterion(logps, y_train)\n",
    "        loss.backward()\n",
    "        # Optimizers need parameters to optimize and a learning rate\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        first = False\n",
    "\n",
    "    # %%\n",
    "    epochs = 20\n",
    "    for e in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            log_ps = model(X_test)\n",
    "            test_loss = criterion(log_ps, y_test)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == y_test.view(*top_class.shape)\n",
    "            test_accuracy = torch.mean(equals.float())\n",
    "        \n",
    "        print(f\"Epoch {e+1}/{epochs}.. \",\n",
    "                f\"Train loss: {loss:.3f}.. \",\n",
    "                f\"Test loss: {test_loss:.3f}.. \",\n",
    "                f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "    chunk_num += 1\n",
    "    print(f'Finished chunk {chunk_num}')\n",
    "    # save model every 10 chunks\n",
    "    if chunk_num % 100 == 0:\n",
    "        torch.save(model.state_dict(), f'./tf-idf-{chunk_num}.pth')\n",
    "        print(f'Saved model at chunk {chunk_num}')\n",
    "torch.save(model.state_dict(), f'./tf-idf-{finished}.pth')\n",
    "print(f'Saved model at chunk {finished}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeNewsProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from os import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "import swifter\n",
    "# first lets run clean_text on the 'content' column\n",
    "from cleantext import clean\n",
    "def clean_text(s):\n",
    "    return clean(s,lower=True,                     # lowercase text\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,               # replace all numbers with a special token\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_number=\"<NUM>\",\n",
    "        lang=\"en\"                   \n",
    "    )\n",
    "\n",
    "# clean the text\n",
    "def clean_column(series):\n",
    "    # parallelized operation\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(clean_text)(s) for s in series)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize_column(series):\n",
    "    # parallelized operation\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(word_tokenize)(s) for s in series)\n",
    "    \n",
    "state = False\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# removing generic stopwords\n",
    "def remove_stopwords(series):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # parallelized operation\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(lambda x: [w for w in x if not w in stop_words])(s) for s in series)\n",
    "\n",
    "# lemmatizing the text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_column(series):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # parallelized operation\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(lambda x: [lemmatizer.lemmatize(w) for w in x])(s) for s in series)\n",
    "    \n",
    "\n",
    "# remove punctiuation\n",
    "import string\n",
    "def remove_punctuation(series):\n",
    "    # parallelized operation\n",
    "    return Parallel(n_jobs=cpu_count())(delayed(lambda x: [w for w in x if w not in string.punctuation])(s) for s in series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv file in chunks\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fake_news_cleaned.csv', parse_dates=['scraped_at', 'inserted_at', 'updated_at'], chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 rows\n",
      "processed 2000 rows\n",
      "processed 3000 rows\n",
      "processed 4000 rows\n",
      "processed 5000 rows\n",
      "processed 6000 rows\n",
      "processed 7000 rows\n",
      "processed 8000 rows\n",
      "processed 9000 rows\n",
      "processed 10000 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m chunk \u001b[39m=\u001b[39m chunk[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[39m# clean the text\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m chunk \u001b[39m=\u001b[39m clean_column(chunk)\n\u001b[0;32m      7\u001b[0m chunk \u001b[39m=\u001b[39m tokenize_column(chunk)\n\u001b[0;32m      8\u001b[0m chunk \u001b[39m=\u001b[39m remove_stopwords(chunk)\n",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m, in \u001b[0;36mclean_column\u001b[1;34m(series)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_column\u001b[39m(series):\n\u001b[0;32m     19\u001b[0m     \u001b[39m# parallelized operation\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49mcpu_count())(delayed(clean_text)(s) \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m series)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(df):\n",
    "    # Drop the unnecessary columns\n",
    "    chunk = chunk['content']\n",
    "\n",
    "    # clean the text\n",
    "    chunk = clean_column(chunk)\n",
    "    chunk = tokenize_column(chunk)\n",
    "    chunk = remove_stopwords(chunk)\n",
    "    chunk = lemmatize_column(chunk)\n",
    "    chunk = remove_punctuation(chunk)\n",
    "\n",
    "    # convert list back to series\n",
    "    chunk = pd.Series(chunk)\n",
    "    \n",
    "    # save to file. Append if file exists, otherwise create new file\n",
    "    if i == 0:\n",
    "        chunk.to_csv('fake_news_cleaned_tokenized.csv', mode='w')\n",
    "    else:\n",
    "        chunk.to_csv('fake_news_cleaned_tokenized.csv', mode='a', header=False)\n",
    "\n",
    "    # print progress\n",
    "    print('processed {} rows'.format((i+1)*1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeNewsProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

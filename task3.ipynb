{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets run clean_text on the 'content' column\n",
    "from cleantext import clean\n",
    "def clean_text(s):\n",
    "    return clean(s,lower=True,                     # lowercase text\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,               # replace all numbers with a special token\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_number=\"<NUM>\",\n",
    "        lang=\"en\"                   \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in chunks and run in parallel\n",
    "from joblib import Parallel, delayed\n",
    "from os import cpu_count\n",
    "\n",
    "# run in parallel\n",
    "def run_parallel(df, n_jobs, func):\n",
    "    # call every element in the chunks in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(func)(element) for element in df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def clean_column(df):\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, clean_text)\n",
    "    # replace column with cleaned text\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize the text. run in parallel\n",
    "def tokenize_column(df):\n",
    "    # run the function on the data\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df['content'], n_jobs, word_tokenize)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# removing generic stopwords\n",
    "def remove_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove stopwords from the text\n",
    "    def r(s):\n",
    "        return [w for w in s if not w in stop_words]\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small part of the data for testing\n",
    "df = pd.read_csv('D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv', nrows=10000, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>https://www.express.co.uk/news/science/738402/...</td>\n",
       "      <td>[life, illusion, ,, least, quantum, level, ,, ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n",
       "      <td>Sean Martin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>THE UNIVERSE ceases to exist when we are not l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>[unfortunately, ,, n't, yet, attacked, islamic...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/category/donald-trum...</td>\n",
       "      <td>[los, angeles, police, department, denied, $, ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/24/more-winn...</td>\n",
       "      <td>[white, house, decided, quietly, withdraw, tie...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>MORE WINNING! Israeli intelligence source, DEB...</td>\n",
       "      <td>Cleavis Nowell, Cleavisnowell, Clarence J. Fei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>http://barenakedislam.com/2017/12/25/oh-trump-...</td>\n",
       "      <td>[``, time, come, cut, tongues, support, peace,...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>â€œOh, Trump, you coward, you just wait, we will...</td>\n",
       "      <td>F.N. Lehner, Don Spilman, Clarence J. Feinour,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id              domain   type  \\\n",
       "0           0   2       express.co.uk  rumor   \n",
       "1           1   6  barenakedislam.com   hate   \n",
       "2           2   7  barenakedislam.com   hate   \n",
       "3           3   8  barenakedislam.com   hate   \n",
       "4           4   9  barenakedislam.com   hate   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.express.co.uk/news/science/738402/...   \n",
       "1  http://barenakedislam.com/category/donald-trum...   \n",
       "2  http://barenakedislam.com/category/donald-trum...   \n",
       "3  http://barenakedislam.com/2017/12/24/more-winn...   \n",
       "4  http://barenakedislam.com/2017/12/25/oh-trump-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  [life, illusion, ,, least, quantum, level, ,, ...   \n",
       "1  [unfortunately, ,, n't, yet, attacked, islamic...   \n",
       "2  [los, angeles, police, department, denied, $, ...   \n",
       "3  [white, house, decided, quietly, withdraw, tie...   \n",
       "4  [``, time, come, cut, tongues, support, peace,...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-02 01:19:41.756664   \n",
       "1  2018-02-02 01:19:41.756664   \n",
       "2  2018-02-02 01:19:41.756664   \n",
       "3  2018-02-02 01:19:41.756664   \n",
       "4  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                               title  \\\n",
       "0  Is life an ILLUSION? Researchers prove 'realit...   \n",
       "1                                       Donald Trump   \n",
       "2                                       Donald Trump   \n",
       "3  MORE WINNING! Israeli intelligence source, DEB...   \n",
       "4  â€œOh, Trump, you coward, you just wait, we will...   \n",
       "\n",
       "                                             authors  keywords meta_keywords  \\\n",
       "0                                        Sean Martin       NaN          ['']   \n",
       "1  Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN          ['']   \n",
       "2  Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...       NaN          ['']   \n",
       "3  Cleavis Nowell, Cleavisnowell, Clarence J. Fei...       NaN          ['']   \n",
       "4  F.N. Lehner, Don Spilman, Clarence J. Feinour,...       NaN          ['']   \n",
       "\n",
       "                                    meta_description tags  summary  source  \n",
       "0  THE UNIVERSE ceases to exist when we are not l...  NaN      NaN     NaN  \n",
       "1                                                NaN  NaN      NaN     NaN  \n",
       "2                                                NaN  NaN      NaN     NaN  \n",
       "3                                                NaN  NaN      NaN     NaN  \n",
       "4                                                NaN  NaN      NaN     NaN  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process data\n",
    "df['content'] = clean_column(df['content'])\n",
    "df['content'] = tokenize_column(df)\n",
    "df['content'] = remove_stopwords(df['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from os import remove\n",
    "# create a file to be used for storing the data\n",
    "def intialize_file(name):\n",
    "    # check if file exists\n",
    "    if path.exists(name):\n",
    "        # if the file exists, delete it\n",
    "        remove(name)\n",
    "    # create the file\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('') # write an empty string to the file to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append data to csv file\n",
    "def append_to_file(name, data):\n",
    "    with open(name, 'a') as f:\n",
    "        # write the data to the file\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file to store the data\n",
    "tokenized_file = 'tokenized_temp.csv'\n",
    "intialize_file(tokenized_file)\n",
    "\n",
    "# append header to the file\n",
    "header = df.columns.values\n",
    "append_to_file(tokenized_file, ','.join(header))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeNewsProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37ef1759c6e89e8d70b9e29662a61b8d9623ea595ee6c595f52ac5c809a12b57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

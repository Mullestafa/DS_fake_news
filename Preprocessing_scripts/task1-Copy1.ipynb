{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# importing data from csv file, using the first row as column labels 'headers'\n",
    "data = pd.read_csv('E:/ML/fake_news_data/news.csv/news_cleaned_2018_02_13.csv', chunksize=10000, index_col=0, lineterminator='\\n')\n",
    "output = pd.DataFrame({'cleaned':[], 'tokenized':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from cleantext import clean\n",
    "import numpy as np\n",
    "\n",
    "# using the clean-text library to clean the text\n",
    "def clean_text(s):\n",
    "    return clean(s,lower=True,                     # lowercase text\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,               # replace all numbers with a special token\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_number=\"<NUM>\",\n",
    "        lang=\"en\"                   \n",
    "    )\n",
    "\n",
    "\n",
    "def run_clean_text(chunk):\n",
    "    return chunk['content'].apply(clean_text)\n",
    "    \n",
    "\n",
    "vectorized_clean_text = np.vectorize(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "threads = cpu_count()\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def start_cleaning_thread(chunks):\n",
    "    executor = ThreadPoolExecutor(max_workers=threads)\n",
    "    futures = [executor.submit(run_clean_text, chunk) for chunk in chunks]\n",
    "    executor.shutdown(wait=True)\n",
    "    return(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading chunk\n",
      "generating subchunks\n",
      "running threads\n",
      "concatenating results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musta\\AppData\\Local\\Temp\\ipykernel_16844\\3768539914.py:31: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  output['cleaned'] = output['cleaned'].append(result['cleaned'])\n",
      "C:\\Users\\musta\\AppData\\Local\\Temp\\ipykernel_16844\\3768539914.py:31: FutureWarning: reindexing with a non-unique Index is deprecated and will raise in a future version.\n",
      "  output['cleaned'] = output['cleaned'].append(result['cleaned'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m#output['cleaned'] = pd.concat([output,result['cleaned']], ignore_index=True)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m output[\u001b[39m'\u001b[39;49m\u001b[39mcleaned\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(result[\u001b[39m'\u001b[39m\u001b[39mcleaned\u001b[39m\u001b[39m'\u001b[39m])    \n\u001b[0;32m     33\u001b[0m rows_number \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[0;32m     34\u001b[0m clear_output()\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3978\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3979\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3980\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4166\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4172\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4173\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4174\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   4176\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4177\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   4178\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   4179\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4180\u001b[0m     ):\n\u001b[0;32m   4181\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4182\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\frame.py:4912\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4910\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4911\u001b[0m \u001b[39melif\u001b[39;00m is_dict_like(value):\n\u001b[1;32m-> 4912\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4914\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m   4915\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\frame.py:12025\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m  12022\u001b[0m     \u001b[39m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[0;32m  12023\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique:\n\u001b[0;32m  12024\u001b[0m         \u001b[39m# duplicate axis\u001b[39;00m\n\u001b[1;32m> 12025\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m  12027\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m  12028\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mincompatible index of inserted column with frame index\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m  12029\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m  12030\u001b[0m \u001b[39mreturn\u001b[39;00m reindexed_value\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\frame.py:12020\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  12018\u001b[0m \u001b[39m# GH#4107\u001b[39;00m\n\u001b[0;32m  12019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m> 12020\u001b[0m     reindexed_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mreindex(index)\u001b[39m.\u001b[39m_values\n\u001b[0;32m  12021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m  12022\u001b[0m     \u001b[39m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[0;32m  12023\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique:\n\u001b[0;32m  12024\u001b[0m         \u001b[39m# duplicate axis\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\series.py:5094\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5090\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   5091\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m passed as both positional and keyword argument\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5092\u001b[0m         )\n\u001b[0;32m   5093\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index})\n\u001b[1;32m-> 5094\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\generic.py:5289\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5286\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   5288\u001b[0m \u001b[39m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reindex_axes(\n\u001b[0;32m   5290\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[0;32m   5291\u001b[0m )\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\generic.py:5309\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5304\u001b[0m new_index, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mreindex(\n\u001b[0;32m   5305\u001b[0m     labels, level\u001b[39m=\u001b[39mlevel, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance, method\u001b[39m=\u001b[39mmethod\n\u001b[0;32m   5306\u001b[0m )\n\u001b[0;32m   5308\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m-> 5309\u001b[0m obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_reindex_with_indexers(\n\u001b[0;32m   5310\u001b[0m     {axis: [new_index, indexer]},\n\u001b[0;32m   5311\u001b[0m     fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m   5312\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   5313\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   5314\u001b[0m )\n\u001b[0;32m   5315\u001b[0m \u001b[39m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[0;32m   5316\u001b[0m copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\generic.py:5355\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   5352\u001b[0m     indexer \u001b[39m=\u001b[39m ensure_platform_int(indexer)\n\u001b[0;32m   5354\u001b[0m \u001b[39m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[39;00m\n\u001b[1;32m-> 5355\u001b[0m new_data \u001b[39m=\u001b[39m new_data\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[0;32m   5356\u001b[0m     index,\n\u001b[0;32m   5357\u001b[0m     indexer,\n\u001b[0;32m   5358\u001b[0m     axis\u001b[39m=\u001b[39;49mbaxis,\n\u001b[0;32m   5359\u001b[0m     fill_value\u001b[39m=\u001b[39;49mfill_value,\n\u001b[0;32m   5360\u001b[0m     allow_dups\u001b[39m=\u001b[39;49mallow_dups,\n\u001b[0;32m   5361\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   5362\u001b[0m )\n\u001b[0;32m   5363\u001b[0m \u001b[39m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[0;32m   5364\u001b[0m copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\internals\\managers.py:737\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[39m# some axes don't allow reindexing with dups\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_dups:\n\u001b[1;32m--> 737\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxes[axis]\u001b[39m.\u001b[39;49m_validate_can_reindex(indexer)\n\u001b[0;32m    739\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim:\n\u001b[0;32m    740\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRequested axis not found in manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4316\u001b[0m, in \u001b[0;36mIndex._validate_can_reindex\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   4314\u001b[0m \u001b[39m# trying to reindex on an axis with duplicates\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(indexer):\n\u001b[1;32m-> 4316\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "# cleaning the text with a custom multithreaded process\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "chunk_size = 500\n",
    "save_every = 10000\n",
    "rows_number = 0\n",
    "while rows_number < 30000:\n",
    "    print('loading chunk')\n",
    "    chunk = next(data)\n",
    "    \n",
    "    print('generating subchunks')\n",
    "    chunks = [chunk[i:i+chunk_size] for i in range(0,chunk.shape[0],chunk_size)]\n",
    "    \n",
    "    print('running threads')\n",
    "    futures = start_cleaning_thread(chunks)\n",
    "    \n",
    "\n",
    "    print('concatenating results')\n",
    "    result = pd.DataFrame()\n",
    "    for future in futures:\n",
    "        #print((future.result().to_frame()['content']))\n",
    "        #wait()\n",
    "        df = pd.DataFrame()\n",
    "        df['cleaned'] = future.result()\n",
    "        result = pd.concat([result,df], ignore_index=True)\n",
    "\n",
    "        \n",
    "    #output['cleaned'] = pd.concat([output,result['cleaned']], ignore_index=True)\n",
    "    result = result.reset_index(drop=True)\n",
    "    output['cleaned'] = output['cleaned'].append(result['cleaned'])    \n",
    "    \n",
    "    rows_number += len(chunk)\n",
    "    clear_output()\n",
    "    print('finished', rows_number, 'rows')\n",
    "\n",
    "    # save the output every save_every rows\n",
    "    if rows_number % save_every == 0:\n",
    "        output.to_csv('output.csv', index=False, mode='a', header= (not os.path.exists('output.csv')))\n",
    "        output = pd.DataFrame()  # clear the output to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544287c64e8941de815362a358e671b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using swifter did not yield any speedup, rather it is a bit slower\n",
    "import swifter\n",
    "data_temp = next(data)\n",
    "result = data_temp['content'].swifter.apply(clean_text)\n",
    "output = pd.concat([output,result.to_frame()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas apply is also slighty slower\n",
    "data_temp = next(data)\n",
    "content = data_temp['content'].values\n",
    "content = vectorized_clean_text(content)\n",
    "output = pd.concat([output,result.to_frame()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>life is an illusion, at least on a quantum lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unfortunately, he hasn't yet attacked her for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the los angeles police department has been den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the white house has decided to quietly withdra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"the time has come to cut off the tongues of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in a shocking, but not unprecedented christmas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thrilling thursday nasdaq &lt;num&gt; is our next su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obama facing environmental lawsuit from chicag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake news is satanic says pope\\n% of readers t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>metalla royalty and streaming increases produc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cleaned  tokenized                                            content\n",
       "0        NaN        NaN  life is an illusion, at least on a quantum lev...\n",
       "1        NaN        NaN  unfortunately, he hasn't yet attacked her for ...\n",
       "2        NaN        NaN  the los angeles police department has been den...\n",
       "3        NaN        NaN  the white house has decided to quietly withdra...\n",
       "4        NaN        NaN  \"the time has come to cut off the tongues of t...\n",
       "..       ...        ...                                                ...\n",
       "995      NaN        NaN  in a shocking, but not unprecedented christmas...\n",
       "996      NaN        NaN  thrilling thursday nasdaq <num> is our next su...\n",
       "997      NaN        NaN  obama facing environmental lawsuit from chicag...\n",
       "998      NaN        NaN  fake news is satanic says pope\\n% of readers t...\n",
       "999      NaN        NaN  metalla royalty and streaming increases produc...\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import  word_tokenize # this tokenizer is arbitrary, but we can use it for now and analyze the results later\n",
    "#nltk.download('punkt')\n",
    "\n",
    "data['processed_content'] = data['cleaned_content'].apply(lambda t : (word_tokenize(t)))\n",
    "word_frq_pre_stopwords_removal = nltk.FreqDist(data['processed_content'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# the following line must be uncommented first time this package is used in the environment\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    # the stopwords from the library are put in a set for faster lookup\n",
    "    words_to_remove = set(stopwords.words('english')) # this line makes the code ~400x faster!\n",
    "    # checking each indiviudal token to see if its in the set of stopwords\n",
    "    return [w for w in s if w not in words_to_remove]\n",
    "# the stepwordless text is stored in a new column 'processed_content' overriding the previous data\n",
    "data['processed_content'] = data['processed_content'].apply(remove_stopwords)\n",
    "word_frq_post_stopwords_removal = nltk.FreqDist(data['processed_content'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size before removing stopwords 16586\n",
      "token number before removing stopwords 200995\n",
      "vocab size after removing stopwords 16454\n",
      "token number after removing stopwords 126763\n",
      "vocab size reduction 0.7958519233088147 %\n",
      "token number reduction 36.93226199656708 %\n"
     ]
    }
   ],
   "source": [
    "print('vocab size before removing stopwords', len(word_frq_pre_stopwords_removal))\n",
    "print('token number before removing stopwords', sum(word_frq_pre_stopwords_removal.values()))\n",
    "print('vocab size after removing stopwords', len(word_frq_post_stopwords_removal))\n",
    "print('token number after removing stopwords', sum(word_frq_post_stopwords_removal.values()))\n",
    "print('vocab size reduction', (len(word_frq_pre_stopwords_removal) - len(word_frq_post_stopwords_removal)) / len(word_frq_pre_stopwords_removal) * 100, '%')\n",
    "print('token number reduction', (sum(word_frq_pre_stopwords_removal.values()) - sum(word_frq_post_stopwords_removal.values())) / sum(word_frq_pre_stopwords_removal.values()) * 100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer # this stemmer is arbitrary, but we can use it for now and analyze the results later\n",
    "# the following line must be uncommented first time this package is used in the environment\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# the stemmer reduces the tokens (words) to their root form\n",
    "def stem_words(l):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(w) for w in l]\n",
    "# the stemmed text is stored in the column 'processed_content' overriding the previous data\n",
    "data['processed_content'] = data['processed_content'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab reduction after stemming 31.445241278716423 %\n"
     ]
    }
   ],
   "source": [
    "word_frq_post_stemming = nltk.FreqDist(data['processed_content'].sum())\n",
    "print('vocab reduction after stemming', (len(word_frq_post_stopwords_removal) - len(word_frq_post_stemming)) / len(word_frq_post_stopwords_removal) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('news_sample_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/church-congregation-brings-gift...</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sometimes the power of christmas will make you...</td>\n",
       "      <td>[sometim, power, christma, make, wild, wonder,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/awakening-start-here/...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awakening of &lt;num&gt; strands of dna \"reconnectin...</td>\n",
       "      <td>[awaken, &lt;, num, &gt;, strand, dna, ``, reconnect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://www.cnnnext.com/video/18526/never-hike-...</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never hike alone: a friday the 13th fan film u...</td>\n",
       "      <td>[never, hike, alon, :, friday, 13th, fan, film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/elusive-alien-of-the-sea-caught...</td>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Elusive ‘Alien Of The Sea ‘ Caught By Scientis...</td>\n",
       "      <td>Alexander Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>when a rare shark was caught, scientists were ...</td>\n",
       "      <td>[rare, shark, caught, ,, scientist, left, blun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/2018/01/21/trumps-...</td>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Trump’s Genius Poll Is Complete &amp; The Results ...</td>\n",
       "      <td>Gloria Christie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>donald trump has the unnerving ability to abil...</td>\n",
       "      <td>[donald, trump, unnerv, abil, abil, creat, rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>39259</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/economy/2017/12/priso...</td>\n",
       "      <td>Prison for Rahm, God’s Work And Many Others\\r\\...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Prison for Rahm, God’s Work And Many Others</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prison for rahm, god's work and many others\\nh...</td>\n",
       "      <td>[prison, rahm, ,, god, 's, work, mani, other, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>39468</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/diy/2017/11/4-useful-...</td>\n",
       "      <td>4 Useful Items for Your Tiny Home\\r\\n\\r\\nHeadl...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>4 Useful Items for Your Tiny Home</td>\n",
       "      <td>Dimitry K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;num&gt; useful items for your tiny home\\nheadlin...</td>\n",
       "      <td>[&lt;, num, &gt;, use, item, tini, home, headlin, :,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>39477</td>\n",
       "      <td>www.newsmax.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.newsmax.com/politics/michael-hayde...</td>\n",
       "      <td>Former CIA Director Michael Hayden said Thursd...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Michael Hayden: We Should Be 'Frightened' by T...</td>\n",
       "      <td>Todd Beamon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['michael hayden', 'sthole countries', 'daca',...</td>\n",
       "      <td>President Donald Trump's reported remarks abou...</td>\n",
       "      <td>Homeland Security, Trump Administration, Immig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>former cia director michael hayden said thursd...</td>\n",
       "      <td>[former, cia, director, michael, hayden, said,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>39550</td>\n",
       "      <td>www.newsmax.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.newsmax.com/newsfront/antonio-saba...</td>\n",
       "      <td>Antonio Sabato Jr. says Hollywood's liberal el...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Antonio Sabato Jr.: It's Oprah or Bust for Hol...</td>\n",
       "      <td>Bill Hoffmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['antonio sabato jr', 'oprah winfrey', 'presid...</td>\n",
       "      <td>Antonio Sabato Jr. says Hollywood's liberal el...</td>\n",
       "      <td>Trump Administration, ISIS/Islamic State, News...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antonio sabato jr. says hollywood's liberal el...</td>\n",
       "      <td>[antonio, sabato, jr., say, hollywood, 's, lib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>39558</td>\n",
       "      <td>www.newsmax.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.newsmax.com/newsfront/bill-clinton...</td>\n",
       "      <td>Former U.S. President Bill Clinton on Monday c...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Bill Clinton Calls for Release of Reuters Jour...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['bill clinton', 'myanmar', 'calls', 'release'...</td>\n",
       "      <td>Former U.S. President Bill Clinton Calls for R...</td>\n",
       "      <td>Donald Trump, Russia, Trump Administration, Gu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>former u.s. president bill clinton on monday c...</td>\n",
       "      <td>[former, u.s., presid, bill, clinton, monday, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                domain        type  \\\n",
       "0      141               awm.com  unreliable   \n",
       "1      256     beforeitsnews.com        fake   \n",
       "2      700           cnnnext.com  unreliable   \n",
       "3      768               awm.com  unreliable   \n",
       "4      791  bipartisanreport.com   clickbait   \n",
       "..     ...                   ...         ...   \n",
       "245  39259     beforeitsnews.com        fake   \n",
       "246  39468     beforeitsnews.com        fake   \n",
       "247  39477       www.newsmax.com         NaN   \n",
       "248  39550       www.newsmax.com         NaN   \n",
       "249  39558       www.newsmax.com         NaN   \n",
       "\n",
       "                                                   url  \\\n",
       "0    http://awm.com/church-congregation-brings-gift...   \n",
       "1    http://beforeitsnews.com/awakening-start-here/...   \n",
       "2    http://www.cnnnext.com/video/18526/never-hike-...   \n",
       "3    http://awm.com/elusive-alien-of-the-sea-caught...   \n",
       "4    http://bipartisanreport.com/2018/01/21/trumps-...   \n",
       "..                                                 ...   \n",
       "245  http://beforeitsnews.com/economy/2017/12/priso...   \n",
       "246  http://beforeitsnews.com/diy/2017/11/4-useful-...   \n",
       "247  https://www.newsmax.com/politics/michael-hayde...   \n",
       "248  https://www.newsmax.com/newsfront/antonio-saba...   \n",
       "249  https://www.newsmax.com/newsfront/bill-clinton...   \n",
       "\n",
       "                                               content  \\\n",
       "0    Sometimes the power of Christmas will make you...   \n",
       "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2    Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3    When a rare shark was caught, scientists were ...   \n",
       "4    Donald Trump has the unnerving ability to abil...   \n",
       "..                                                 ...   \n",
       "245  Prison for Rahm, God’s Work And Many Others\\r\\...   \n",
       "246  4 Useful Items for Your Tiny Home\\r\\n\\r\\nHeadl...   \n",
       "247  Former CIA Director Michael Hayden said Thursd...   \n",
       "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
       "249  Former U.S. President Bill Clinton on Monday c...   \n",
       "\n",
       "                     scraped_at                 inserted_at  \\\n",
       "0    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4    2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "..                          ...                         ...   \n",
       "245  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "246  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "247  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "248  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "249  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                     updated_at  \\\n",
       "0    2018-02-02 01:19:41.756664   \n",
       "1    2018-02-02 01:19:41.756664   \n",
       "2    2018-02-02 01:19:41.756664   \n",
       "3    2018-02-02 01:19:41.756664   \n",
       "4    2018-02-02 01:19:41.756664   \n",
       "..                          ...   \n",
       "245  2018-02-02 01:19:41.756664   \n",
       "246  2018-02-02 01:19:41.756664   \n",
       "247  2018-02-02 01:19:41.756664   \n",
       "248  2018-02-02 01:19:41.756664   \n",
       "249  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                 title          authors  \\\n",
       "0    Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
       "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
       "2    Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
       "3    Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
       "4    Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
       "..                                                 ...              ...   \n",
       "245        Prison for Rahm, God’s Work And Many Others              NaN   \n",
       "246                  4 Useful Items for Your Tiny Home        Dimitry K   \n",
       "247  Michael Hayden: We Should Be 'Frightened' by T...      Todd Beamon   \n",
       "248  Antonio Sabato Jr.: It's Oprah or Bust for Hol...    Bill Hoffmann   \n",
       "249  Bill Clinton Calls for Release of Reuters Jour...              NaN   \n",
       "\n",
       "     keywords                                      meta_keywords  \\\n",
       "0         NaN                                               ['']   \n",
       "1         NaN                                               ['']   \n",
       "2         NaN                                               ['']   \n",
       "3         NaN                                               ['']   \n",
       "4         NaN                                               ['']   \n",
       "..        ...                                                ...   \n",
       "245       NaN                                               ['']   \n",
       "246       NaN                                               ['']   \n",
       "247       NaN  ['michael hayden', 'sthole countries', 'daca',...   \n",
       "248       NaN  ['antonio sabato jr', 'oprah winfrey', 'presid...   \n",
       "249       NaN  ['bill clinton', 'myanmar', 'calls', 'release'...   \n",
       "\n",
       "                                      meta_description  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2    Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "245                                                NaN   \n",
       "246                                                NaN   \n",
       "247  President Donald Trump's reported remarks abou...   \n",
       "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
       "249  Former U.S. President Bill Clinton Calls for R...   \n",
       "\n",
       "                                                  tags  summary  \\\n",
       "0                                                  NaN      NaN   \n",
       "1                                                  NaN      NaN   \n",
       "2                                                  NaN      NaN   \n",
       "3                                                  NaN      NaN   \n",
       "4                                                  NaN      NaN   \n",
       "..                                                 ...      ...   \n",
       "245                                                NaN      NaN   \n",
       "246                                                NaN      NaN   \n",
       "247  Homeland Security, Trump Administration, Immig...      NaN   \n",
       "248  Trump Administration, ISIS/Islamic State, News...      NaN   \n",
       "249  Donald Trump, Russia, Trump Administration, Gu...      NaN   \n",
       "\n",
       "                                       cleaned_content  \\\n",
       "0    sometimes the power of christmas will make you...   \n",
       "1    awakening of <num> strands of dna \"reconnectin...   \n",
       "2    never hike alone: a friday the 13th fan film u...   \n",
       "3    when a rare shark was caught, scientists were ...   \n",
       "4    donald trump has the unnerving ability to abil...   \n",
       "..                                                 ...   \n",
       "245  prison for rahm, god's work and many others\\nh...   \n",
       "246  <num> useful items for your tiny home\\nheadlin...   \n",
       "247  former cia director michael hayden said thursd...   \n",
       "248  antonio sabato jr. says hollywood's liberal el...   \n",
       "249  former u.s. president bill clinton on monday c...   \n",
       "\n",
       "                                     processed_content  \n",
       "0    [sometim, power, christma, make, wild, wonder,...  \n",
       "1    [awaken, <, num, >, strand, dna, ``, reconnect...  \n",
       "2    [never, hike, alon, :, friday, 13th, fan, film...  \n",
       "3    [rare, shark, caught, ,, scientist, left, blun...  \n",
       "4    [donald, trump, unnerv, abil, abil, creat, rea...  \n",
       "..                                                 ...  \n",
       "245  [prison, rahm, ,, god, 's, work, mani, other, ...  \n",
       "246  [<, num, >, use, item, tini, home, headlin, :,...  \n",
       "247  [former, cia, director, michael, hayden, said,...  \n",
       "248  [antonio, sabato, jr., say, hollywood, 's, lib...  \n",
       "249  [former, u.s., presid, bill, clinton, monday, ...  \n",
       "\n",
       "[250 rows x 17 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8d7cf0ce3aaacb75b2914db7b9dafb2d26d63c92bd1cee8dba3ea41598bbe88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

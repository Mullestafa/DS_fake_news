{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text[GPL]\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting ftfy<7.0,>=6.0\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting unidecode<2.0.0,>=1.1.1\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/envs/python3/lib/python3.8/site-packages (from ftfy<7.0,>=6.0->clean-text[GPL]) (0.2.5)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171032 sha256=0da060a50cc8f8b11680f12b43afbc6deed78cd812ae68c4aa3892daa2cfea9e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/33/40/c1/5469c4f03ddc90bb1bd797ef3b6fe7d0d29688155ee2c05529\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, unidecode, ftfy, clean-text\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1 unidecode-1.3.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install clean-text[GPL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets run clean_text on the 'content' column\n",
    "from cleantext import clean\n",
    "def clean_text(s):\n",
    "    return clean(s,lower=True,                     # lowercase text\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,               # replace all numbers with a special token\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_number=\"<NUM>\",\n",
    "        lang=\"en\"                   \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in chunks and run in parallel\n",
    "from joblib import Parallel, delayed\n",
    "from os import cpu_count\n",
    "\n",
    "# run in parallel\n",
    "def run_parallel(df, n_jobs, func):\n",
    "    # call every element in the chunks in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(func)(element) for element in df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def clean_column(df):\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, clean_text)\n",
    "    # replace column with cleaned text\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/envs/python3/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python3/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python3/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2022.10.31\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize the text. run in parallel\n",
    "def tokenize_column(df):\n",
    "    # run the function on the data\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df['content'], n_jobs, word_tokenize)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# removing generic stopwords\n",
    "def remove_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove stopwords from the text\n",
    "    def r(s):\n",
    "        return [w for w in s if not w in stop_words]\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming the text\n",
    "from nltk.stem import PorterStemmer\n",
    "def stem_column(df):\n",
    "    # create a stemmer\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # stem the text\n",
    "    def stem(s):\n",
    "        return [ps.stem(w) for w in s]\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, stem)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctiuation\n",
    "import string\n",
    "def remove_punctuation(df):\n",
    "    # remove punctuation\n",
    "    def remove_punct(s):\n",
    "        return [w for w in s if w not in string.punctuation]\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, remove_punct)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small part of the data for testing\n",
    "df = pd.read_csv('news_sample.csv', nrows=10000, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from os import remove\n",
    "# create a file to be used for storing the data\n",
    "def intialize_file(name):\n",
    "    # check if file exists\n",
    "    if path.exists(name):\n",
    "        # if the file exists, delete it\n",
    "        remove(name)\n",
    "    # create the file\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('') # write an empty string to the file to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append data to csv file\n",
    "def append_to_file(name, data):\n",
    "    with open(name, 'a') as f:\n",
    "        # write the data to the file\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file to store the data\n",
    "tokenized_file = 'tokenized_temp.csv'\n",
    "#intialize_file(tokenized_file)\n",
    "\n",
    "# append header to the file\n",
    "header = df.columns.values\n",
    "append_to_file(tokenized_file, ','.join(header))\n",
    "append_to_file(tokenized_file, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m df:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# process data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/news_cleaned_2018_02_13.csv', chunksize=1000, index_col=0)\n",
    "\n",
    "count = 0\n",
    "for chunk in df:\n",
    "    # process data\n",
    "    chunk['content'] = clean_column(chunk['content'])\n",
    "    chunk['content'] = tokenize_column(chunk)\n",
    "    chunk['content'] = remove_stopwords(chunk['content'])\n",
    "    chunk['content'] = stem_column(chunk['content'])\n",
    "    chunk['content'] = remove_punctuation(chunk['content'])\n",
    "    # append data to file\n",
    "    chunk.to_csv(tokenized_file, mode='a', header=False)\n",
    "\n",
    "    count += 1\n",
    "    print(count, 'chunks processed')\n",
    "    if count == 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from file\n",
    "tokenized_file = 'tokenized_temp.csv'\n",
    "df = pd.read_csv(tokenized_file)\n",
    "# load list from string\n",
    "from ast import literal_eval\n",
    "df['content'] = df['content'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count the tokens\n",
    "def count(s):\n",
    "    return Counter(s)\n",
    "\n",
    "# count token frequency\n",
    "from collections import Counter\n",
    "def count_tokens(df):\n",
    "\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, count)\n",
    "\n",
    "    # total token frequency\n",
    "    total = {}\n",
    "    for list in results:\n",
    "        for k,v in list.items():\n",
    "            if k in total:\n",
    "                total[k] += v\n",
    "            else:\n",
    "                total[k] = v\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq = count_tokens(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the tokens by frequency\n",
    "token_freq = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a set of stopwords from the frequency list\n",
    "stop_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 80% percentile\n",
    "# first finding the total number of tokens\n",
    "total = 0\n",
    "for token in token_freq:\n",
    "    total += token[1]\n",
    "# then finding the 80% percentile\n",
    "percentile = int(total * 0.8)\n",
    "# then finding the token that corresponds to the 80% percentile\n",
    "total = 0\n",
    "for token in token_freq:\n",
    "    total += token[1]\n",
    "    if total > percentile:\n",
    "        index = token_freq.index(token)\n",
    "        break\n",
    "# add all tokens after the 80% percentile to the stopword list\n",
    "for token in token_freq[index:]:\n",
    "    stop_words.add(token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384865"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove custom stopwords from the text\n",
    "def remove_costume_stopwords(df):\n",
    "    # remove stopwords from the text\n",
    "    def r(s):\n",
    "        return [w for w in s if not w in stop_words]\n",
    "\n",
    "    # run the function on the df with multiple threads\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, r)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = [[w for w in s if not w in stop_words] for s in df['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "df.to_csv('ready_data.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ready_data.csv')\n",
    "# load list from string\n",
    "from ast import literal_eval\n",
    "df['content'] = df['content'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, test and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['type'], test_size=0.2, random_state=42)\n",
    "X_test , X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias          0.993285\n",
       "clickbait     0.970310\n",
       "conspiracy    1.050946\n",
       "fake          1.004703\n",
       "hate          1.157447\n",
       "junksci       0.991031\n",
       "political     0.982002\n",
       "reliable      0.548387\n",
       "rumor         0.692308\n",
       "satire        0.621908\n",
       "unknown       1.056604\n",
       "unreliable    1.195062\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()*8 / y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making labels binary\n",
    "y_train = y_train.apply(lambda x: 1 if x in ['fake', 'junksci', 'hate', 'clickbait',] else 0)\n",
    "y_test = y_test.apply(lambda x: 1 if x in ['fake', 'junksci', 'hate', 'clickbait',] else 0)\n",
    "y_val = y_val.apply(lambda x: 1 if x in ['fake', 'junksci', 'hate', 'clickbait',] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# function which pads pandas series\n",
    "def pad_series(s, max_len):\n",
    "    # truncate the series if it is longer than max_len\n",
    "    s = s.apply(lambda x: x[:max_len])\n",
    "\n",
    "    # pad the series\n",
    "    s = s.apply(lambda x: x + ['<pad>'] * (max_len - len(x)))\n",
    "\n",
    "    # convert the series to np array\n",
    "    s = np.array(s.tolist())\n",
    "    return s\n",
    "\n",
    "# pad the series and convert them to np arrays\n",
    "X_test = pad_series(X_test, 750)\n",
    "X_train = pad_series(X_train, 750)\n",
    "X_val = pad_series(X_val, 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the strings to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit the encoder on the training / testing / valdiation data\n",
    "le.fit(np.concatenate((X_train.flatten(), X_test.flatten(), X_val.flatten())))\n",
    "\n",
    "# transform the data\n",
    "X_train = le.transform(X_train.flatten()).reshape(X_train.shape)\n",
    "X_test = le.transform(X_test.flatten()).reshape(X_test.shape)\n",
    "X_val = le.transform(X_val.flatten()).reshape(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a baseline model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "# fit the classifier\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a dummy classifier\n",
    "dummy_clf = LogisticRegression(max_iter=10000, solver='saga')\n",
    "# fit the classifier\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\anaconda3\\envs\\fakeNewsProject\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6611"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a dummy classifier\n",
    "dummy_clf = MLPClassifier(max_iter=10000, hidden_layer_sizes=(750,1000,50))\n",
    "# fit the classifier\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a word embedding\n",
    "import gensim\n",
    "#load model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('glove.6B.50d.txt', no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['<pad>'] = np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which converts a series of tokens to a series of vectors\n",
    "def convert_to_vectors(s):\n",
    "    # function which converts a list of tokens to a list of vectors\n",
    "    def convert(s):\n",
    "        # convert the tokens to vectors if they are in the model\n",
    "        s = [model[w] for w in s if w in model]\n",
    "        # convert the list to np array\n",
    "        s = np.array(s)\n",
    "        # calculate the mean of the vectors\n",
    "        s = np.mean(s, axis=0)\n",
    "        return s\n",
    "\n",
    "    # run the function on the df with multiple threads\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(s, n_jobs, convert)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv', chunksize=10000, index_col=0)\n",
    "\n",
    "# process the data\n",
    "df = next(df)\n",
    "df['content'] = clean_column(df['content'])\n",
    "df['content'] = tokenize_column(df)\n",
    "df['content'] = remove_stopwords(df['content'])\n",
    "df['content'] = remove_punctuation(df['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to vectors\n",
    "df['content'] = [[model[w] for w in s if w in model] for s in df['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0.51491, 0.88806, -0.71906, -0.5748, 0.85655...\n",
       "1       [[0.27404, -0.25123, -0.020682, -0.27062, 0.14...\n",
       "2       [[-0.0097114, 1.0479, -0.15266, 0.95792, -0.64...\n",
       "3       [[-0.68652, 0.80125, -0.6124, -0.1512, 0.997, ...\n",
       "4       [[0.12817, 0.15858, -0.38843, -0.39108, 0.6836...\n",
       "                              ...                        \n",
       "9995    [[0.50801, 0.67231, -0.85555, -0.55372, 0.6621...\n",
       "9996    [[0.50801, 0.67231, -0.85555, -0.55372, 0.6621...\n",
       "9997    [[0.52875, 0.12491, 1.1286, -0.79976, 0.62674,...\n",
       "9998    [[0.088383, 0.64673, 1.1358, -0.41847, 0.24472...\n",
       "9999    [[0.26382, 0.32453, 0.74185, -0.37095, 0.65957...\n",
       "Name: content, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the series and convert them to np arrays\n",
    "X_test = pad_series(X_test, 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('fake_news_output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>575</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/arts/2018/01/chironom...</td>\n",
       "      <td>b'[\"chironomus\",\"crassicaudatus\",\"victor\",\"moz...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Chironomus crassicaudatus by Victor Mozqueda</td>\n",
       "      <td>Baroness Photos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>500</td>\n",
       "      <td>11835</td>\n",
       "      <td>canadafreepress.com</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>http://canadafreepress.com/article/cable-monst...</td>\n",
       "      <td>b'[\"\\'re\",\"leading\",\"audio/video\",\"cable\",\"com...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Cable Monster Diversifies, Upgrades its Offerings</td>\n",
       "      <td>Jim Bray, Because Without America, There Is No...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20500</th>\n",
       "      <td>500</td>\n",
       "      <td>22913</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/blogging-citizen-jour...</td>\n",
       "      <td>b'[\"hillary\",\"clinton\",\"driven\",\"tears\",\"kelly...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Hillary Clinton Driven to Tears By Kellyanne C...</td>\n",
       "      <td>John Ale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30500</th>\n",
       "      <td>500</td>\n",
       "      <td>34297</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/survival/2017/10/tip-...</td>\n",
       "      <td>b'[\"``\",\"tip\",\"iceberg\",\"\\'\\'\",\"judge\",\"jeanin...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>“Tip of the Iceberg”: Judge Jeanine Destroys t...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40500</th>\n",
       "      <td>500</td>\n",
       "      <td>45390</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/alternative/2017/12/c...</td>\n",
       "      <td>b'[\"something\",\"else\",\"going-on\",\"many\",\"us\",\"...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>CP on Twitter – How to Report</td>\n",
       "      <td>Philosophers Stone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50500</th>\n",
       "      <td>500</td>\n",
       "      <td>55804</td>\n",
       "      <td>breitbart.com</td>\n",
       "      <td>political</td>\n",
       "      <td>http://www.breitbart.com/big-government/2015/0...</td>\n",
       "      <td>b'[\"black\",\"activist\",\"defended\",\"confederate\"...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Anthony Hervey, Black Confederate Flag Support...</td>\n",
       "      <td>Lee Stranahan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Anthony Hervey', 'Arlene Barnum', 'car accid...</td>\n",
       "      <td>Anthony Hervey, well known for wearing a Confe...</td>\n",
       "      <td>Anthony Hervey, Mississippi Flag, Confederate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60500</th>\n",
       "      <td>500</td>\n",
       "      <td>66272</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/survival/2013/11/wher...</td>\n",
       "      <td>b'[\"decent\",\"jews\",\"headline\",\"bitcoin\",\"block...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Where Are The Decent Jews?</td>\n",
       "      <td>B Mans Revolt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>Decent Jew Review, myths</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70500</th>\n",
       "      <td>500</td>\n",
       "      <td>77408</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/economy/2015/08/china...</td>\n",
       "      <td>b'[\"china\",\"moves\",\"devalue\",\"yuan\",\"readers\",...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>China Moves to Devalue the Yuan</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80500</th>\n",
       "      <td>500</td>\n",
       "      <td>88342</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/war-and-conflict/2011...</td>\n",
       "      <td>b'[\"aware\",\"headline\",\"bitcoin\",\"blockchain\",\"...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>War and Conflict</td>\n",
       "      <td>Colonel Sixx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90500</th>\n",
       "      <td>500</td>\n",
       "      <td>100084</td>\n",
       "      <td>conservapedia.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://www.conservapedia.com/Four_Modernizations</td>\n",
       "      <td>b'[\"conservapedia\",\"four\",\"modernizations\",\"go...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Four Modernizations</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100500</th>\n",
       "      <td>500</td>\n",
       "      <td>113219</td>\n",
       "      <td>americablog.com</td>\n",
       "      <td>bias</td>\n",
       "      <td>http://americablog.com/2013/07/stephen-colbert...</td>\n",
       "      <td>b'[\"stephen\",\"colbert\",\"usual\",\"great\",\"job\",\"...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Stephen Colbert on Obamacare and GOP opposition</td>\n",
       "      <td>John Aravosis, John'S Article Archive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Stephen Colbert does his usual great job skewe...</td>\n",
       "      <td>Stephen Colbert, Colbert Report, Obamacare, Video</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      id               domain        type  \\\n",
       "500            500     575    beforeitsnews.com        fake   \n",
       "10500          500   11835  canadafreepress.com  conspiracy   \n",
       "20500          500   22913    beforeitsnews.com        fake   \n",
       "30500          500   34297    beforeitsnews.com        fake   \n",
       "40500          500   45390    beforeitsnews.com        fake   \n",
       "50500          500   55804        breitbart.com   political   \n",
       "60500          500   66272    beforeitsnews.com        fake   \n",
       "70500          500   77408    beforeitsnews.com        fake   \n",
       "80500          500   88342    beforeitsnews.com        fake   \n",
       "90500          500  100084    conservapedia.com        bias   \n",
       "100500         500  113219      americablog.com        bias   \n",
       "\n",
       "                                                      url  \\\n",
       "500     http://beforeitsnews.com/arts/2018/01/chironom...   \n",
       "10500   http://canadafreepress.com/article/cable-monst...   \n",
       "20500   http://beforeitsnews.com/blogging-citizen-jour...   \n",
       "30500   http://beforeitsnews.com/survival/2017/10/tip-...   \n",
       "40500   http://beforeitsnews.com/alternative/2017/12/c...   \n",
       "50500   http://www.breitbart.com/big-government/2015/0...   \n",
       "60500   http://beforeitsnews.com/survival/2013/11/wher...   \n",
       "70500   http://beforeitsnews.com/economy/2015/08/china...   \n",
       "80500   http://beforeitsnews.com/war-and-conflict/2011...   \n",
       "90500    http://www.conservapedia.com/Four_Modernizations   \n",
       "100500  http://americablog.com/2013/07/stephen-colbert...   \n",
       "\n",
       "                                                  content  \\\n",
       "500     b'[\"chironomus\",\"crassicaudatus\",\"victor\",\"moz...   \n",
       "10500   b'[\"\\'re\",\"leading\",\"audio/video\",\"cable\",\"com...   \n",
       "20500   b'[\"hillary\",\"clinton\",\"driven\",\"tears\",\"kelly...   \n",
       "30500   b'[\"``\",\"tip\",\"iceberg\",\"\\'\\'\",\"judge\",\"jeanin...   \n",
       "40500   b'[\"something\",\"else\",\"going-on\",\"many\",\"us\",\"...   \n",
       "50500   b'[\"black\",\"activist\",\"defended\",\"confederate\"...   \n",
       "60500   b'[\"decent\",\"jews\",\"headline\",\"bitcoin\",\"block...   \n",
       "70500   b'[\"china\",\"moves\",\"devalue\",\"yuan\",\"readers\",...   \n",
       "80500   b'[\"aware\",\"headline\",\"bitcoin\",\"blockchain\",\"...   \n",
       "90500   b'[\"conservapedia\",\"four\",\"modernizations\",\"go...   \n",
       "100500  b'[\"stephen\",\"colbert\",\"usual\",\"great\",\"job\",\"...   \n",
       "\n",
       "                        scraped_at                 inserted_at  \\\n",
       "500     2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "10500   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "20500   2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "30500   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "40500   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "50500   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "60500   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "70500   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "80500   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "90500   2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "100500  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                        updated_at  \\\n",
       "500     2018-02-02 01:19:41.756664   \n",
       "10500   2018-02-02 01:19:41.756664   \n",
       "20500   2018-02-02 01:19:41.756664   \n",
       "30500   2018-02-02 01:19:41.756664   \n",
       "40500   2018-02-02 01:19:41.756664   \n",
       "50500   2018-02-02 01:19:41.756664   \n",
       "60500   2018-02-02 01:19:41.756664   \n",
       "70500   2018-02-02 01:19:41.756664   \n",
       "80500   2018-02-02 01:19:41.756664   \n",
       "90500   2018-02-02 01:19:41.756664   \n",
       "100500  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                                    title  \\\n",
       "500          Chironomus crassicaudatus by Victor Mozqueda   \n",
       "10500   Cable Monster Diversifies, Upgrades its Offerings   \n",
       "20500   Hillary Clinton Driven to Tears By Kellyanne C...   \n",
       "30500   “Tip of the Iceberg”: Judge Jeanine Destroys t...   \n",
       "40500                       CP on Twitter – How to Report   \n",
       "50500   Anthony Hervey, Black Confederate Flag Support...   \n",
       "60500                          Where Are The Decent Jews?   \n",
       "70500                     China Moves to Devalue the Yuan   \n",
       "80500                                    War and Conflict   \n",
       "90500                                 Four Modernizations   \n",
       "100500    Stephen Colbert on Obamacare and GOP opposition   \n",
       "\n",
       "                                                  authors  keywords  \\\n",
       "500                                       Baroness Photos       NaN   \n",
       "10500   Jim Bray, Because Without America, There Is No...       NaN   \n",
       "20500                                            John Ale       NaN   \n",
       "30500                                                None       NaN   \n",
       "40500                                  Philosophers Stone       NaN   \n",
       "50500                                       Lee Stranahan       NaN   \n",
       "60500                                       B Mans Revolt       NaN   \n",
       "70500                                                None       NaN   \n",
       "80500                                        Colonel Sixx       NaN   \n",
       "90500                                                None       NaN   \n",
       "100500              John Aravosis, John'S Article Archive       NaN   \n",
       "\n",
       "                                            meta_keywords  \\\n",
       "500                                                  ['']   \n",
       "10500                                                ['']   \n",
       "20500                                                ['']   \n",
       "30500                                                ['']   \n",
       "40500                                                ['']   \n",
       "50500   ['Anthony Hervey', 'Arlene Barnum', 'car accid...   \n",
       "60500                                                ['']   \n",
       "70500                                                ['']   \n",
       "80500                                                ['']   \n",
       "90500                                                ['']   \n",
       "100500                                               ['']   \n",
       "\n",
       "                                         meta_description  \\\n",
       "500                                                  None   \n",
       "10500                                                None   \n",
       "20500                                                None   \n",
       "30500                                                None   \n",
       "40500                                                None   \n",
       "50500   Anthony Hervey, well known for wearing a Confe...   \n",
       "60500                                                None   \n",
       "70500                                                None   \n",
       "80500                                                None   \n",
       "90500                                                None   \n",
       "100500  Stephen Colbert does his usual great job skewe...   \n",
       "\n",
       "                                                     tags  summary  source  \n",
       "500                                                  None      NaN     NaN  \n",
       "10500                                                None      NaN     NaN  \n",
       "20500                                                None      NaN     NaN  \n",
       "30500                                                None      NaN     NaN  \n",
       "40500                                                None      NaN     NaN  \n",
       "50500   Anthony Hervey, Mississippi Flag, Confederate ...      NaN     NaN  \n",
       "60500                            Decent Jew Review, myths      NaN     NaN  \n",
       "70500                                                None      NaN     NaN  \n",
       "80500                                                None      NaN     NaN  \n",
       "90500                                                None      NaN     NaN  \n",
       "100500  Stephen Colbert, Colbert Report, Obamacare, Video      NaN     NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all rows with value 500 in the column 'Unnamed: 0'\n",
    "df[df['Unnamed: 0'] == 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_column(df):\n",
    "    df['content'] = df['content'].apply(word_tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mD:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, nrows\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\Programmer\\anaconda_3\\envs\\fakeNewsProject\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv', nrows=100, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "37ef1759c6e89e8d70b9e29662a61b8d9623ea595ee6c595f52ac5c809a12b57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

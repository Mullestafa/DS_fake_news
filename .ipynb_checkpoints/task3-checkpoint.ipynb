{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "# first lets run clean_text on the 'content' column\n",
    "from cleantext import clean\n",
    "def clean_text(s):\n",
    "    return clean(s,lower=True,                     # lowercase text\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_numbers=True,               # replace all numbers with a special token\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_number=\"<NUM>\",\n",
    "        lang=\"en\"                   \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in chunks and run in parallel\n",
    "from joblib import Parallel, delayed\n",
    "from os import cpu_count\n",
    "\n",
    "# run in parallel\n",
    "def run_parallel(df, n_jobs, func):\n",
    "    # call every element in the chunks in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(func)(element) for element in df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def clean_column(df):\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, clean_text)\n",
    "    # replace column with cleaned text\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize the text. run in parallel\n",
    "def tokenize_column(df):\n",
    "    # run the function on the data\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df['content'], n_jobs, word_tokenize)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# removing generic stopwords\n",
    "def remove_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove stopwords from the text\n",
    "    def r(s):\n",
    "        return [w for w in s if not w in stop_words]\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming the text\n",
    "from nltk.stem import PorterStemmer\n",
    "def stem_column(df):\n",
    "    # create a stemmer\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # stem the text\n",
    "    def stem(s):\n",
    "        return [ps.stem(w) for w in s]\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, stem)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctiuation\n",
    "import string\n",
    "def remove_punctuation(df):\n",
    "    # remove punctuation\n",
    "    def remove_punct(s):\n",
    "        return [w for w in s if w not in string.punctuation]\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, remove_punct)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a small part of the data for testing\n",
    "df = pd.read_csv('D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv', nrows=10000, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from os import remove\n",
    "# create a file to be used for storing the data\n",
    "def intialize_file(name):\n",
    "    # check if file exists\n",
    "    if path.exists(name):\n",
    "        # if the file exists, delete it\n",
    "        remove(name)\n",
    "    # create the file\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('') # write an empty string to the file to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append data to csv file\n",
    "def append_to_file(name, data):\n",
    "    with open(name, 'a') as f:\n",
    "        # write the data to the file\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file to store the data\n",
    "tokenized_file = 'tokenized_temp.csv'\n",
    "#intialize_file(tokenized_file)\n",
    "\n",
    "# append header to the file\n",
    "header = df.columns.values\n",
    "append_to_file(tokenized_file, ','.join(header))\n",
    "append_to_file(tokenized_file, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 chunks processed\n",
      "2 chunks processed\n",
      "3 chunks processed\n",
      "4 chunks processed\n",
      "5 chunks processed\n",
      "6 chunks processed\n",
      "7 chunks processed\n",
      "8 chunks processed\n",
      "9 chunks processed\n",
      "10 chunks processed\n",
      "11 chunks processed\n",
      "12 chunks processed\n",
      "13 chunks processed\n",
      "14 chunks processed\n",
      "15 chunks processed\n",
      "16 chunks processed\n",
      "17 chunks processed\n",
      "18 chunks processed\n",
      "19 chunks processed\n",
      "20 chunks processed\n",
      "21 chunks processed\n",
      "22 chunks processed\n",
      "23 chunks processed\n",
      "24 chunks processed\n",
      "25 chunks processed\n",
      "26 chunks processed\n",
      "27 chunks processed\n",
      "28 chunks processed\n",
      "29 chunks processed\n",
      "30 chunks processed\n",
      "31 chunks processed\n",
      "32 chunks processed\n",
      "33 chunks processed\n",
      "34 chunks processed\n",
      "35 chunks processed\n",
      "36 chunks processed\n",
      "37 chunks processed\n",
      "38 chunks processed\n",
      "39 chunks processed\n",
      "40 chunks processed\n",
      "41 chunks processed\n",
      "42 chunks processed\n",
      "43 chunks processed\n",
      "44 chunks processed\n",
      "45 chunks processed\n",
      "46 chunks processed\n",
      "47 chunks processed\n",
      "48 chunks processed\n",
      "49 chunks processed\n",
      "50 chunks processed\n",
      "51 chunks processed\n",
      "52 chunks processed\n",
      "53 chunks processed\n",
      "54 chunks processed\n",
      "55 chunks processed\n",
      "56 chunks processed\n",
      "57 chunks processed\n",
      "58 chunks processed\n",
      "59 chunks processed\n",
      "60 chunks processed\n",
      "61 chunks processed\n",
      "62 chunks processed\n",
      "63 chunks processed\n",
      "64 chunks processed\n",
      "65 chunks processed\n",
      "66 chunks processed\n",
      "67 chunks processed\n",
      "68 chunks processed\n",
      "69 chunks processed\n",
      "70 chunks processed\n",
      "71 chunks processed\n",
      "72 chunks processed\n",
      "73 chunks processed\n",
      "74 chunks processed\n",
      "75 chunks processed\n",
      "76 chunks processed\n",
      "77 chunks processed\n",
      "78 chunks processed\n",
      "79 chunks processed\n",
      "80 chunks processed\n",
      "81 chunks processed\n",
      "82 chunks processed\n",
      "83 chunks processed\n",
      "84 chunks processed\n",
      "85 chunks processed\n",
      "86 chunks processed\n",
      "87 chunks processed\n",
      "88 chunks processed\n",
      "89 chunks processed\n",
      "90 chunks processed\n",
      "91 chunks processed\n",
      "92 chunks processed\n",
      "93 chunks processed\n",
      "94 chunks processed\n",
      "95 chunks processed\n",
      "96 chunks processed\n",
      "97 chunks processed\n",
      "98 chunks processed\n",
      "99 chunks processed\n",
      "100 chunks processed\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv', chunksize=1000, index_col=0)\n",
    "\n",
    "count = 0\n",
    "for chunk in df:\n",
    "    # process data\n",
    "    chunk['content'] = clean_column(chunk['content'])\n",
    "    chunk['content'] = tokenize_column(chunk)\n",
    "    chunk['content'] = remove_stopwords(chunk['content'])\n",
    "    chunk['content'] = stem_column(chunk['content'])\n",
    "    chunk['content'] = remove_punctuation(chunk['content'])\n",
    "    # append data to file\n",
    "    chunk.to_csv(tokenized_file, mode='a', header=False)\n",
    "\n",
    "    count += 1\n",
    "    print(count, 'chunks processed')\n",
    "    if count == 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from file\n",
    "tokenized_file = 'tokenized_temp.csv'\n",
    "df = pd.read_csv(tokenized_file)\n",
    "# load list from string\n",
    "from ast import literal_eval\n",
    "df['content'] = df['content'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count the tokens\n",
    "def count(s):\n",
    "    return Counter(s)\n",
    "\n",
    "# count token frequency\n",
    "from collections import Counter\n",
    "def count_tokens(df):\n",
    "\n",
    "\n",
    "    # run the function on the df\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, count)\n",
    "\n",
    "    # total token frequency\n",
    "    total = {}\n",
    "    for list in results:\n",
    "        for k,v in list.items():\n",
    "            if k in total:\n",
    "                total[k] += v\n",
    "            else:\n",
    "                total[k] = v\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq = count_tokens(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the tokens by frequency\n",
    "token_freq = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a set of stopwords from the frequency list\n",
    "stop_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 80% percentile\n",
    "# first finding the total number of tokens\n",
    "total = 0\n",
    "for token in token_freq:\n",
    "    total += token[1]\n",
    "# then finding the 80% percentile\n",
    "percentile = int(total * 0.8)\n",
    "# then finding the token that corresponds to the 80% percentile\n",
    "total = 0\n",
    "for token in token_freq:\n",
    "    total += token[1]\n",
    "    if total > percentile:\n",
    "        index = token_freq.index(token)\n",
    "        break\n",
    "# add all tokens after the 80% percentile to the stopword list\n",
    "for token in token_freq[index:]:\n",
    "    stop_words.add(token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384865"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove custom stopwords from the text\n",
    "def remove_costume_stopwords(df):\n",
    "    # remove stopwords from the text\n",
    "    def r(s):\n",
    "        return [w for w in s if not w in stop_words]\n",
    "\n",
    "    # run the function on the df with multiple threads\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(df, n_jobs, r)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = [[w for w in s if not w in stop_words] for s in df['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "df.to_csv('ready_data.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ready_data.csv')\n",
    "# load list from string\n",
    "from ast import literal_eval\n",
    "df['content'] = df['content'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, test and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['type'], test_size=0.2, random_state=42)\n",
    "X_test , X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias          0.993285\n",
       "clickbait     0.970310\n",
       "conspiracy    1.050946\n",
       "fake          1.004703\n",
       "hate          1.157447\n",
       "junksci       0.991031\n",
       "political     0.982002\n",
       "reliable      0.548387\n",
       "rumor         0.692308\n",
       "satire        0.621908\n",
       "unknown       1.056604\n",
       "unreliable    1.195062\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()*8 / y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making labels binary\n",
    "y_train = y_train.apply(lambda x: 1 if x in ['fake', 'junksci', 'hate', 'clickbait',] else 0)\n",
    "y_test = y_test.apply(lambda x: 1 if x in ['fake', 'junksci', 'hate', 'clickbait',] else 0)\n",
    "y_val = y_val.apply(lambda x: 1 if x in ['fake', 'junksci', 'hate', 'clickbait',] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# function which pads pandas series\n",
    "def pad_series(s, max_len):\n",
    "    # truncate the series if it is longer than max_len\n",
    "    s = s.apply(lambda x: x[:max_len])\n",
    "\n",
    "    # pad the series\n",
    "    s = s.apply(lambda x: x + ['<pad>'] * (max_len - len(x)))\n",
    "\n",
    "    # convert the series to np array\n",
    "    s = np.array(s.tolist())\n",
    "    return s\n",
    "\n",
    "# pad the series and convert them to np arrays\n",
    "X_test = pad_series(X_test, 750)\n",
    "X_train = pad_series(X_train, 750)\n",
    "X_val = pad_series(X_val, 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the strings to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit the encoder on the training / testing / valdiation data\n",
    "le.fit(np.concatenate((X_train.flatten(), X_test.flatten(), X_val.flatten())))\n",
    "\n",
    "# transform the data\n",
    "X_train = le.transform(X_train.flatten()).reshape(X_train.shape)\n",
    "X_test = le.transform(X_test.flatten()).reshape(X_test.shape)\n",
    "X_val = le.transform(X_val.flatten()).reshape(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a baseline model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "# fit the classifier\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a dummy classifier\n",
    "dummy_clf = LogisticRegression(max_iter=10000, solver='saga')\n",
    "# fit the classifier\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\anaconda3\\envs\\fakeNewsProject\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6611"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a dummy classifier\n",
    "dummy_clf = MLPClassifier(max_iter=10000, hidden_layer_sizes=(750,1000,50))\n",
    "# fit the classifier\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a word embedding\n",
    "import gensim\n",
    "#load model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('glove.6B.50d.txt', no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['<pad>'] = np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which converts a series of tokens to a series of vectors\n",
    "def convert_to_vectors(s):\n",
    "    # function which converts a list of tokens to a list of vectors\n",
    "    def convert(s):\n",
    "        # convert the tokens to vectors if they are in the model\n",
    "        s = [model[w] for w in s if w in model]\n",
    "        # convert the list to np array\n",
    "        s = np.array(s)\n",
    "        # calculate the mean of the vectors\n",
    "        s = np.mean(s, axis=0)\n",
    "        return s\n",
    "\n",
    "    # run the function on the df with multiple threads\n",
    "    n_jobs = cpu_count()\n",
    "    results = run_parallel(s, n_jobs, convert)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/FakeNews_data/news.csv/news_cleaned_2018_02_13.csv', chunksize=10000, index_col=0)\n",
    "\n",
    "# process the data\n",
    "df = next(df)\n",
    "df['content'] = clean_column(df['content'])\n",
    "df['content'] = tokenize_column(df)\n",
    "df['content'] = remove_stopwords(df['content'])\n",
    "df['content'] = remove_punctuation(df['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to vectors\n",
    "df['content'] = [[model[w] for w in s if w in model] for s in df['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0.51491, 0.88806, -0.71906, -0.5748, 0.85655...\n",
       "1       [[0.27404, -0.25123, -0.020682, -0.27062, 0.14...\n",
       "2       [[-0.0097114, 1.0479, -0.15266, 0.95792, -0.64...\n",
       "3       [[-0.68652, 0.80125, -0.6124, -0.1512, 0.997, ...\n",
       "4       [[0.12817, 0.15858, -0.38843, -0.39108, 0.6836...\n",
       "                              ...                        \n",
       "9995    [[0.50801, 0.67231, -0.85555, -0.55372, 0.6621...\n",
       "9996    [[0.50801, 0.67231, -0.85555, -0.55372, 0.6621...\n",
       "9997    [[0.52875, 0.12491, 1.1286, -0.79976, 0.62674,...\n",
       "9998    [[0.088383, 0.64673, 1.1358, -0.41847, 0.24472...\n",
       "9999    [[0.26382, 0.32453, 0.74185, -0.37095, 0.65957...\n",
       "Name: content, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the series and convert them to np arrays\n",
    "X_test = pad_series(X_test, 750)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeNewsProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37ef1759c6e89e8d70b9e29662a61b8d9623ea595ee6c595f52ac5c809a12b57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
